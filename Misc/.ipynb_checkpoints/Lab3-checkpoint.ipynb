{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.gpu.set_per_process_memory_fraction(0.3)\n",
    "tf.config.gpu.set_per_process_memory_growth(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##image loader\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "def path_loader(fold1, fold2, data_path):\n",
    "    #Creating data path\n",
    "    image_data_path = os.path.join(data_path, fold1)   \n",
    "    mask_data_path = os.path.join(data_path, fold2)\n",
    "    images = []\n",
    "    masks = []\n",
    "    #Listing all file names in the path\n",
    "    for root, dirs, files in os.walk(image_data_path):\n",
    "        for name in files:\n",
    "            images.append(os.path.join(image_data_path,name))\n",
    "    for root2, dirs2, files2 in os.walk(mask_data_path):\n",
    "        for name2 in files2:\n",
    "            masks.append(os.path.join(mask_data_path,name2))\n",
    "    return images, masks\n",
    "    \n",
    "\n",
    "# reading and resizing the training images with their corresponding labels\n",
    "def get_train_data_shuffled(images, masks, p):\n",
    "    \n",
    "    c = list(zip(images, masks))\n",
    "\n",
    "    shuffle(c)\n",
    "\n",
    "    images, masks = zip(*c)\n",
    "    \n",
    "    train_x, test_x, train_y, test_y = train_test_split(images,masks,test_size = p)\n",
    "\n",
    "    return train_x, test_x, train_y, test_y \n",
    "\n",
    "def data_loader(fold1, fold2, data_path, p,img_h, img_w):\n",
    "    \n",
    "    images, masks = path_loader(fold1, fold2, data_path)\n",
    "    train_x, test_x, train_y, test_y = get_train_data_shuffled(images, masks, p)\n",
    "    \n",
    "    train_img = []\n",
    "    train_mask = []\n",
    "    test_img = []\n",
    "    test_mask = []\n",
    "    len(train_x)\n",
    "    for i in range(len(train_x)):\n",
    "        image_name = train_x[i]\n",
    "        img = imread(image_name, as_grey=True)\n",
    "        img = resize(img, (img_h, img_w), anti_aliasing = True).astype('float32')\n",
    "        train_img.append([np.array(img)]) \n",
    "\n",
    "        if i % 50 == 0:\n",
    "             print('Reading: {0}/{1}  of train images'.format(i, len(train_x)))\n",
    "    for j in range(len(train_y)):\n",
    "        mask_name = train_y[j]\n",
    "        mask = imread(mask_name, as_grey=True)\n",
    "        mask = resize(mask, (img_h, img_w), anti_aliasing = True).astype('float32')\n",
    "        train_mask.append([np.array(mask)])\n",
    "        \n",
    "    for i in range(len(test_x)):\n",
    "        image_name = test_x[i]\n",
    "        img = imread(image_name, as_grey=True)\n",
    "        img = resize(img, (img_h, img_w), anti_aliasing = True).astype('float32')\n",
    "        test_img.append([np.array(img)]) \n",
    "\n",
    "        if i % 50 == 0:\n",
    "             print('Reading: {0}/{1}  of test images'.format(i, len(test_x)))\n",
    "                \n",
    "    for j in range(len(test_y)):\n",
    "        mask_name = test_y[j]\n",
    "        mask = imread(mask_name, as_grey=True)\n",
    "        mask = resize(mask, (img_h, img_w), anti_aliasing = True).astype('float32')\n",
    "        test_mask.append([np.array(mask)])        \n",
    "        \n",
    " \n",
    "    return train_img, train_mask, test_img, test_mask\n",
    "\n",
    "# Instantiating images and labels for the model.\n",
    "def get_train_test_data(fold1, fold2, data_path, p,img_h, img_w):\n",
    "    \n",
    "    train_img, train_mask, test_img, test_mask = data_loader(fold1, fold2, data_path, p,img_h, img_w)\n",
    "\n",
    "    Train_Img = np.zeros((len(train_img), img_h, img_w), dtype = np.float32)\n",
    "    Test_Img = np.zeros((len(test_img), img_h, img_w), dtype = np.float32)\n",
    "\n",
    "    Train_Label = np.zeros((len(train_mask),img_h, img_w), dtype = np.int32)\n",
    "    Test_Label = np.zeros((len(test_mask),img_h, img_w), dtype = np.int32)\n",
    "\n",
    "    for i in range(len(train_img)):\n",
    "        Train_Img[i] = train_img[i][0]\n",
    "        Train_Label[i] = train_mask[i][0]\n",
    "\n",
    "    Train_Img = np.expand_dims(Train_Img, axis = 3)  \n",
    "    Train_Label = np.expand_dims(Train_Label, axis = 3) \n",
    "\n",
    "    for j in range(len(test_img)):\n",
    "        Test_Img[j] = test_img[j][0]\n",
    "        Test_Label[j] = test_mask[j][0]\n",
    "\n",
    "    Test_Img = np.expand_dims(Test_Img, axis = 3)\n",
    "    Test_Label = np.expand_dims(Test_Label, axis = 3)\n",
    "    print(Train_Img.shape)\n",
    "    print(Test_Img.shape)\n",
    "\n",
    "    return Train_Img, Train_Label, Test_Img, Test_Label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Data Augmentation\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img\n",
    "\n",
    "def DataAugmentation(rotation_range,width_shift,height_shift_range,rescale,horizontal_flip):\n",
    "    \n",
    "    #Train data\n",
    "    train_datagen = ImageDataGenerator(rotation_range = rotation_range, width_shift_range = width_shift, height_shift_range=height_shift_range,\n",
    "                                       horizontal_flip = horizontal_flip, rescale = rescale)\n",
    "   \n",
    "    #Val data\n",
    "    val_datagen = ImageDataGenerator(rescale = rescale)\n",
    " \n",
    "    \n",
    "    \n",
    "    return train_datagen, val_datagen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##similarity metrics\n",
    "from sklearn.metrics import recall_score, precision_score\n",
    "from scipy.spatial.distance import dice\n",
    "\n",
    "\n",
    "def similarity(metric,img1,img2):\n",
    "\n",
    "    if metric == 'recall':\n",
    "        m = recall_score(img1,img2)\n",
    "    elif metric == 'precision':\n",
    "        m = precision_score(img1,img2)\n",
    "        \n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "smooth = 1.\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1.-dice_coef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##model\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, Input, concatenate, Conv2DTranspose\n",
    "from tensorflow.keras.layers import Dropout, BatchNormalization, SpatialDropout2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "\n",
    "def u_net(Base,img_height, img_width, img_ch, batchNormalization, SDRate, spatial_dropout):\n",
    "    inputs = Input((img_height, img_width, img_ch))\n",
    "    #model = Sequential(img_ch,img_width,img_height, batchNormalization, spatial_dropout, SDRate, dropout, dropoutRate)\n",
    "    ## Contraction\n",
    "    # Conv Block 1\n",
    "    \n",
    "    c1 = Conv2D(filters=Base,\n",
    "                     kernel_size=(3,3), strides=(1,1), padding='same')(inputs)\n",
    "    \n",
    "     #Add batch Normalization\n",
    "    if batchNormalization:\n",
    "        c1 = BatchNormalization(axis=-1)(c1)\n",
    "    \n",
    "    a1 = Activation('relu')(c1)\n",
    "    \n",
    "    #Add spatial Dropout\n",
    "    if spatial_dropout:\n",
    "        a1 = SpatialDropout2D(SDRate)(a1)\n",
    "        \n",
    "    c2 = Conv2D(filters=Base,\n",
    "                     kernel_size=(3,3), strides=(1,1), padding='same')(a1)\n",
    "    \n",
    "     #Add batch Normalization\n",
    "    if batchNormalization:\n",
    "        c2 = BatchNormalization(axis=-1)(c2)\n",
    "    \n",
    "    a2 = Activation('relu')(c2)\n",
    "    \n",
    "    #Add spatial Dropout\n",
    "    if spatial_dropout:\n",
    "        a2 = SpatialDropout2D(SDRate)(a2)\n",
    "        \n",
    "    m1 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(a2)\n",
    "        \n",
    "    # Conv Block 2\n",
    "    c3 = Conv2D(filters=Base*2,\n",
    "                     kernel_size=(3,3), strides=(1,1), padding='same')(m1)\n",
    "    \n",
    "     #Add batch Normalization\n",
    "    if batchNormalization:\n",
    "        c3 = BatchNormalization(axis=-1)(c3)\n",
    "    \n",
    "    a3 = Activation('relu')(c3)\n",
    "    \n",
    "    #Add spatial Dropout\n",
    "    if spatial_dropout:\n",
    "        a3 = SpatialDropout2D(SDRate)(a3)\n",
    "    \n",
    "    c4 = Conv2D(filters=Base*2,\n",
    "                     kernel_size=(3,3), strides=(1,1), padding='same')(a3)\n",
    "    \n",
    "     #Add batch Normalization\n",
    "    if batchNormalization:\n",
    "        c4 = BatchNormalization(axis=-1)(c4)\n",
    "    \n",
    "    a4 = Activation('relu')(c4)\n",
    "    \n",
    "    #Add spatial Dropout\n",
    "    if spatial_dropout:\n",
    "        a4 = SpatialDropout2D(SDRate)(a4)\n",
    "    \n",
    "    m2 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(a4)\n",
    "    \n",
    "    # Conv Block 3\n",
    "    c5 = Conv2D(filters=Base*4, \n",
    "                     kernel_size=(3,3), strides=(1,1), padding='same')(m2)\n",
    "    \n",
    "     #Add batch Normalization\n",
    "    if batchNormalization:\n",
    "        c5 = BatchNormalization(axis=-1)(c5)\n",
    "    \n",
    "    a5 = Activation('relu')(c5)\n",
    "    \n",
    "    #Add spatial Dropout\n",
    "    if spatial_dropout:\n",
    "        a5 = SpatialDropout2D(SDRate)(a5)\n",
    "        \n",
    "    c6 = Conv2D(filters=Base*4,\n",
    "                     kernel_size=(3,3), strides=(1,1), padding='same')(a5)\n",
    "    \n",
    "     #Add batch Normalization\n",
    "    if batchNormalization:\n",
    "          c6 = BatchNormalization(axis=-1)(c6)\n",
    "    \n",
    "    a6 = Activation('relu')(c6)\n",
    "    \n",
    "    #Add spatial Dropout\n",
    "    if spatial_dropout:\n",
    "        a6 = SpatialDropout2D(SDRate)(a6)\n",
    "        \n",
    "    m3 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(a6)\n",
    "    \n",
    "    # Conv Block 4\n",
    "    c7 = Conv2D(filters=Base*8, \n",
    "                     kernel_size=(3,3), strides=(1,1), padding='same')(m3)\n",
    "    \n",
    "     #Add batch Normalization\n",
    "    if batchNormalization:\n",
    "        c7 = BatchNormalization(axis=-1)(c7)\n",
    "    \n",
    "    a7 = Activation('relu')(c7)\n",
    "    \n",
    "    #Add spatial Dropout\n",
    "    if spatial_dropout:\n",
    "        a7 = SpatialDropout2D(SDRate)(a7)\n",
    "        \n",
    "    c8 = Conv2D(filters=Base*8,\n",
    "                     kernel_size=(3,3), strides=(1,1), padding='same')(a7)\n",
    "    \n",
    "     #Add batch Normalization\n",
    "    if batchNormalization:\n",
    "        c8 = BatchNormalization(axis=-1)(c8)\n",
    "    \n",
    "    a8 = Activation('relu')(c8)\n",
    "    \n",
    "    #Add spatial Dropout\n",
    "    if spatial_dropout:\n",
    "        a8 = SpatialDropout2D(SDRate)(a8)\n",
    "        \n",
    "    m4 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(a8)\n",
    "    \n",
    "    ##Bottleneck\n",
    "    # Conv Layer\n",
    "    c9 = Conv2D(filters=Base*16, \n",
    "                     kernel_size=(3,3), strides=(1,1), padding='same')(m4)\n",
    "    \n",
    "     #Add batch Normalization\n",
    "    if batchNormalization:\n",
    "        c9 = BatchNormalization(axis=-1)(c9)\n",
    "    \n",
    "    a9 = Activation('relu')(c9)\n",
    "    \n",
    "    #Add spatial Dropout\n",
    "    if spatial_dropout:\n",
    "        a9 = SpatialDropout2D(SDRate)(a9)\n",
    "        \n",
    "    ##Expansion\n",
    "    #Conv Block 1\n",
    "    c10 = Conv2DTranspose(filters=Base*8,\n",
    "                     kernel_size=(2,2), strides=(2,2), padding='same')(a9)\n",
    "    c10 = concatenate([a8,c10])\n",
    "    \n",
    "    c11 = Conv2D(filters=Base*8,\n",
    "                     kernel_size=(3,3), strides=(1,1), padding='same')(c10)\n",
    "    \n",
    "     #Add batch Normalization\n",
    "    if batchNormalization:\n",
    "        c11 = BatchNormalization(axis=-1)(c11)\n",
    "    \n",
    "    a10 = Activation('relu')(c11)\n",
    "    \n",
    "    #Add spatial Dropout\n",
    "    if spatial_dropout:\n",
    "        a10 = SpatialDropout2D(SDRate)(a10)\n",
    "        \n",
    "    c12 = Conv2D(filters=Base*8, \n",
    "                     kernel_size=(3,3), strides=(1,1), padding='same')(a10)\n",
    "    \n",
    "     #Add batch Normalization\n",
    "    if batchNormalization:\n",
    "        c12 = BatchNormalization(axis=-1)(c12)\n",
    "    \n",
    "    a11 = Activation('relu')(c12)\n",
    "    \n",
    "    #Add spatial Dropout\n",
    "    if spatial_dropout:\n",
    "        a11 = SpatialDropout2D(SDRate)(a11)\n",
    "        \n",
    "    \n",
    "    #Conv Block 2\n",
    "    c13 = Conv2DTranspose(filters=Base*4,\n",
    "                     kernel_size=(2,2), strides=(2,2), padding='same')(a11)\n",
    "    c13 = concatenate([a6,c13])\n",
    "    \n",
    "    c14 = Conv2D(filters=Base*4,\n",
    "                     kernel_size=(3,3), strides=(1,1), padding='same')(c13)\n",
    "    \n",
    "     #Add batch Normalization\n",
    "    if batchNormalization:\n",
    "        c14 = BatchNormalization(axis=-1)(c14)\n",
    "    \n",
    "    a12 = Activation('relu')(c14)\n",
    "    \n",
    "    #Add spatial Dropout\n",
    "    if spatial_dropout:\n",
    "        a12 = SpatialDropout2D(SDRate)(a12)\n",
    "        \n",
    "    c15 = Conv2D(filters=Base*4, \n",
    "                     kernel_size=(3,3), strides=(1,1), padding='same')(a12)\n",
    "    \n",
    "     #Add batch Normalization\n",
    "    if batchNormalization:\n",
    "        c15 = BatchNormalization(axis=-1)(c15)\n",
    "    \n",
    "    a13 = Activation('relu')(c15)\n",
    "    \n",
    "    #Add spatial Dropout\n",
    "    if spatial_dropout:\n",
    "        a13 = SpatialDropout2D(SDRate)(a13)\n",
    "        \n",
    "    \n",
    "    #Conv Block 3\n",
    "    c16 = Conv2DTranspose(filters=Base*2,\n",
    "                     kernel_size=(2,2), strides=(2,2), padding='same')(a13)\n",
    "    c16 = concatenate([a4,c16])\n",
    "    \n",
    "    c17 = Conv2D(filters=Base*2, \n",
    "                     kernel_size=(3,3), strides=(1,1), padding='same')(c16)\n",
    "    \n",
    "     #Add batch Normalization\n",
    "    if batchNormalization:\n",
    "        c17 = BatchNormalization(axis=-1)(c17)\n",
    "    \n",
    "    a14 = Activation('relu')(c17)\n",
    "    \n",
    "    #Add spatial Dropout\n",
    "    if spatial_dropout:\n",
    "        a14 = SpatialDropout2D(SDRate)(a14)\n",
    "        \n",
    "    c18 = Conv2D(filters=Base*2,\n",
    "                     kernel_size=(3,3), strides=(1,1), padding='same')(a14)\n",
    "    \n",
    "     #Add batch Normalization\n",
    "    if batchNormalization:\n",
    "        c18 = BatchNormalization(axis=-1)(c18)\n",
    "    \n",
    "    a15 = Activation('relu')(c18)\n",
    "    \n",
    "    #Add spatial Dropout\n",
    "    if spatial_dropout:\n",
    "        a15 = SpatialDropout2D(SDRate)(a15)\n",
    "        \n",
    "    \n",
    "    #Conv Block 4\n",
    "    c19 = Conv2DTranspose(filters=Base,\n",
    "                     kernel_size=(2,2), strides=(2,2), padding='same')(a15)\n",
    "    c19 = concatenate([a2,c19])\n",
    "    \n",
    "    c20 = Conv2D(filters=Base,\n",
    "                     kernel_size=(3,3), strides=(1,1), padding='same')(c19)\n",
    "    \n",
    "     #Add batch Normalization\n",
    "    if batchNormalization:\n",
    "        c20 = BatchNormalization(axis=-1)(c20)\n",
    "    \n",
    "    a16 = Activation('relu')(c20)\n",
    "    \n",
    "    #Add spatial Dropout\n",
    "    if spatial_dropout:\n",
    "        a16 = SpatialDropout2D(SDRate)(a16)\n",
    "        \n",
    "    c21 = Conv2D(filters=Base,\n",
    "                     kernel_size=(3,3), strides=(1,1), padding='same')(a16)\n",
    "    \n",
    "     #Add batch Normalization\n",
    "    if batchNormalization:\n",
    "        c21 = BatchNormalization(axis=-1)(c21)\n",
    "    \n",
    "    a17 = Activation('relu')(c21)\n",
    "    \n",
    "    #final layer\n",
    "    c22 = Conv2D(1, kernel_size=(3,3), strides=(1,1), padding='same')(a17)\n",
    "    a18 = Activation('sigmoid')(c22)\n",
    "    \n",
    "    model = Model(inputs,a18)\n",
    "    \n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##model\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, Input, concatenate, Conv2DTranspose\n",
    "from tensorflow.keras.layers import Dropout, BatchNormalization, SpatialDropout2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "\n",
    "def u_net_3labels(Base,img_height, img_width, img_ch, batchNormalization, SDRate, spatial_dropout):\n",
    "    print(\"building model\")\n",
    "    inputs = Input((img_height, img_width, img_ch))\n",
    "    #model = Sequential(img_ch,img_width,img_height, batchNormalization, spatial_dropout, SDRate, dropout, dropoutRate)\n",
    "    ## Contraction\n",
    "    # Conv Block 1\n",
    "    \n",
    "    c1 = Conv2D(filters=Base,\n",
    "                     kernel_size=(3,3), strides=(1,1), padding='same')(inputs)\n",
    "    \n",
    "     #Add batch Normalization\n",
    "    if batchNormalization:\n",
    "        c1 = BatchNormalization(axis=-1)(c1)\n",
    "    \n",
    "    a1 = Activation('relu')(c1)\n",
    "    \n",
    "    #Add spatial Dropout\n",
    "    if spatial_dropout:\n",
    "        a1 = SpatialDropout2D(SDRate)(a1)\n",
    "        \n",
    "    c2 = Conv2D(filters=Base,\n",
    "                     kernel_size=(3,3), strides=(1,1), padding='same')(a1)\n",
    "    \n",
    "     #Add batch Normalization\n",
    "    if batchNormalization:\n",
    "        c2 = BatchNormalization(axis=-1)(c2)\n",
    "    \n",
    "    a2 = Activation('relu')(c2)\n",
    "    \n",
    "    #Add spatial Dropout\n",
    "    if spatial_dropout:\n",
    "        a2 = SpatialDropout2D(SDRate)(a2)\n",
    "        \n",
    "    m1 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(a2)\n",
    "        \n",
    "    # Conv Block 2\n",
    "    c3 = Conv2D(filters=Base*2,\n",
    "                     kernel_size=(3,3), strides=(1,1), padding='same')(m1)\n",
    "    \n",
    "     #Add batch Normalization\n",
    "    if batchNormalization:\n",
    "        c3 = BatchNormalization(axis=-1)(c3)\n",
    "    \n",
    "    a3 = Activation('relu')(c3)\n",
    "    \n",
    "    #Add spatial Dropout\n",
    "    if spatial_dropout:\n",
    "        a3 = SpatialDropout2D(SDRate)(a3)\n",
    "    \n",
    "    c4 = Conv2D(filters=Base*2,\n",
    "                     kernel_size=(3,3), strides=(1,1), padding='same')(a3)\n",
    "    \n",
    "     #Add batch Normalization\n",
    "    if batchNormalization:\n",
    "        c4 = BatchNormalization(axis=-1)(c4)\n",
    "    \n",
    "    a4 = Activation('relu')(c4)\n",
    "    \n",
    "    #Add spatial Dropout\n",
    "    if spatial_dropout:\n",
    "        a4 = SpatialDropout2D(SDRate)(a4)\n",
    "    \n",
    "    m2 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(a4)\n",
    "    \n",
    "    # Conv Block 3\n",
    "    c5 = Conv2D(filters=Base*4, \n",
    "                     kernel_size=(3,3), strides=(1,1), padding='same')(m2)\n",
    "    \n",
    "     #Add batch Normalization\n",
    "    if batchNormalization:\n",
    "        c5 = BatchNormalization(axis=-1)(c5)\n",
    "    \n",
    "    a5 = Activation('relu')(c5)\n",
    "    \n",
    "    #Add spatial Dropout\n",
    "    if spatial_dropout:\n",
    "        a5 = SpatialDropout2D(SDRate)(a5)\n",
    "        \n",
    "    c6 = Conv2D(filters=Base*4,\n",
    "                     kernel_size=(3,3), strides=(1,1), padding='same')(a5)\n",
    "    \n",
    "     #Add batch Normalization\n",
    "    if batchNormalization:\n",
    "          c6 = BatchNormalization(axis=-1)(c6)\n",
    "    \n",
    "    a6 = Activation('relu')(c6)\n",
    "    \n",
    "    #Add spatial Dropout\n",
    "    if spatial_dropout:\n",
    "        a6 = SpatialDropout2D(SDRate)(a6)\n",
    "        \n",
    "    m3 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(a6)\n",
    "    \n",
    "    # Conv Block 4\n",
    "    c7 = Conv2D(filters=Base*8, \n",
    "                     kernel_size=(3,3), strides=(1,1), padding='same')(m3)\n",
    "    \n",
    "     #Add batch Normalization\n",
    "    if batchNormalization:\n",
    "        c7 = BatchNormalization(axis=-1)(c7)\n",
    "    \n",
    "    a7 = Activation('relu')(c7)\n",
    "    \n",
    "    #Add spatial Dropout\n",
    "    if spatial_dropout:\n",
    "        a7 = SpatialDropout2D(SDRate)(a7)\n",
    "        \n",
    "    c8 = Conv2D(filters=Base*8,\n",
    "                     kernel_size=(3,3), strides=(1,1), padding='same')(a7)\n",
    "    \n",
    "     #Add batch Normalization\n",
    "    if batchNormalization:\n",
    "        c8 = BatchNormalization(axis=-1)(c8)\n",
    "    \n",
    "    a8 = Activation('relu')(c8)\n",
    "    \n",
    "    #Add spatial Dropout\n",
    "    if spatial_dropout:\n",
    "        a8 = SpatialDropout2D(SDRate)(a8)\n",
    "        \n",
    "    m4 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(a8)\n",
    "    \n",
    "    ##Bottleneck\n",
    "    # Conv Layer\n",
    "    c9 = Conv2D(filters=Base*16, \n",
    "                     kernel_size=(3,3), strides=(1,1), padding='same')(m4)\n",
    "    \n",
    "     #Add batch Normalization\n",
    "    if batchNormalization:\n",
    "        c9 = BatchNormalization(axis=-1)(c9)\n",
    "    \n",
    "    a9 = Activation('relu')(c9)\n",
    "    \n",
    "    #Add spatial Dropout\n",
    "    if spatial_dropout:\n",
    "        a9 = SpatialDropout2D(SDRate)(a9)\n",
    "        \n",
    "    ##Expansion\n",
    "    #Conv Block 1\n",
    "    c10 = Conv2DTranspose(filters=Base*8,\n",
    "                     kernel_size=(2,2), strides=(2,2), padding='same')(a9)\n",
    "    c10 = concatenate([a8,c10])\n",
    "    \n",
    "    c11 = Conv2D(filters=Base*8,\n",
    "                     kernel_size=(3,3), strides=(1,1), padding='same')(c10)\n",
    "    \n",
    "     #Add batch Normalization\n",
    "    if batchNormalization:\n",
    "        c11 = BatchNormalization(axis=-1)(c11)\n",
    "    \n",
    "    a10 = Activation('relu')(c11)\n",
    "    \n",
    "    #Add spatial Dropout\n",
    "    if spatial_dropout:\n",
    "        a10 = SpatialDropout2D(SDRate)(a10)\n",
    "        \n",
    "    c12 = Conv2D(filters=Base*8, \n",
    "                     kernel_size=(3,3), strides=(1,1), padding='same')(a10)\n",
    "    \n",
    "     #Add batch Normalization\n",
    "    if batchNormalization:\n",
    "        c12 = BatchNormalization(axis=-1)(c12)\n",
    "    \n",
    "    a11 = Activation('relu')(c12)\n",
    "    \n",
    "    #Add spatial Dropout\n",
    "    if spatial_dropout:\n",
    "        a11 = SpatialDropout2D(SDRate)(a11)\n",
    "        \n",
    "    \n",
    "    #Conv Block 2\n",
    "    c13 = Conv2DTranspose(filters=Base*4,\n",
    "                     kernel_size=(2,2), strides=(2,2), padding='same')(a11)\n",
    "    c13 = concatenate([a6,c13])\n",
    "    \n",
    "    c14 = Conv2D(filters=Base*4,\n",
    "                     kernel_size=(3,3), strides=(1,1), padding='same')(c13)\n",
    "    \n",
    "     #Add batch Normalization\n",
    "    if batchNormalization:\n",
    "        c14 = BatchNormalization(axis=-1)(c14)\n",
    "    \n",
    "    a12 = Activation('relu')(c14)\n",
    "    \n",
    "    #Add spatial Dropout\n",
    "    if spatial_dropout:\n",
    "        a12 = SpatialDropout2D(SDRate)(a12)\n",
    "        \n",
    "    c15 = Conv2D(filters=Base*4, \n",
    "                     kernel_size=(3,3), strides=(1,1), padding='same')(a12)\n",
    "    \n",
    "     #Add batch Normalization\n",
    "    if batchNormalization:\n",
    "        c15 = BatchNormalization(axis=-1)(c15)\n",
    "    \n",
    "    a13 = Activation('relu')(c15)\n",
    "    \n",
    "    #Add spatial Dropout\n",
    "    if spatial_dropout:\n",
    "        a13 = SpatialDropout2D(SDRate)(a13)\n",
    "        \n",
    "    \n",
    "    #Conv Block 3\n",
    "    c16 = Conv2DTranspose(filters=Base*2,\n",
    "                     kernel_size=(2,2), strides=(2,2), padding='same')(a13)\n",
    "    c16 = concatenate([a4,c16])\n",
    "    \n",
    "    c17 = Conv2D(filters=Base*2, \n",
    "                     kernel_size=(3,3), strides=(1,1), padding='same')(c16)\n",
    "    \n",
    "     #Add batch Normalization\n",
    "    if batchNormalization:\n",
    "        c17 = BatchNormalization(axis=-1)(c17)\n",
    "    \n",
    "    a14 = Activation('relu')(c17)\n",
    "    \n",
    "    #Add spatial Dropout\n",
    "    if spatial_dropout:\n",
    "        a14 = SpatialDropout2D(SDRate)(a14)\n",
    "        \n",
    "    c18 = Conv2D(filters=Base*2,\n",
    "                     kernel_size=(3,3), strides=(1,1), padding='same')(a14)\n",
    "    \n",
    "     #Add batch Normalization\n",
    "    if batchNormalization:\n",
    "        c18 = BatchNormalization(axis=-1)(c18)\n",
    "    \n",
    "    a15 = Activation('relu')(c18)\n",
    "    \n",
    "    #Add spatial Dropout\n",
    "    if spatial_dropout:\n",
    "        a15 = SpatialDropout2D(SDRate)(a15)\n",
    "        \n",
    "    \n",
    "    #Conv Block 4\n",
    "    c19 = Conv2DTranspose(filters=Base,\n",
    "                     kernel_size=(2,2), strides=(2,2), padding='same')(a15)\n",
    "    c19 = concatenate([a2,c19])\n",
    "    \n",
    "    c20 = Conv2D(filters=Base,\n",
    "                     kernel_size=(3,3), strides=(1,1), padding='same')(c19)\n",
    "    \n",
    "     #Add batch Normalization\n",
    "    if batchNormalization:\n",
    "        c20 = BatchNormalization(axis=-1)(c20)\n",
    "    \n",
    "    a16 = Activation('relu')(c20)\n",
    "    \n",
    "    #Add spatial Dropout\n",
    "    if spatial_dropout:\n",
    "        a16 = SpatialDropout2D(SDRate)(a16)\n",
    "        \n",
    "    c21 = Conv2D(filters=Base,\n",
    "                     kernel_size=(3,3), strides=(1,1), padding='same')(a16)\n",
    "    \n",
    "     #Add batch Normalization\n",
    "    if batchNormalization:\n",
    "        c21 = BatchNormalization(axis=-1)(c21)\n",
    "    \n",
    "    a17 = Activation('relu')(c21)\n",
    "    \n",
    "    #final layer\n",
    "    c22 = Conv2D(3, kernel_size=(3,3), strides=(1,1), padding='same')(a17)\n",
    "    a18 = Activation('softmax')(c22)\n",
    "    \n",
    "    model = Model(inputs,a18)\n",
    "    \n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##plotter\n",
    "import matplotlib.pyplot as plt\n",
    "def plotter(History):\n",
    "    #Training vs Validation Learning loss \n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.title(\"Learning curve\")\n",
    "    plt.plot(History.history[\"loss\"], label=\"loss\")\n",
    "    plt.plot(History.history[\"val_loss\"], label=\"val_loss\")\n",
    "    plt.plot( np.argmin(History.history[\"val_loss\"]),\n",
    "             np.min(History.history[\"val_loss\"]),\n",
    "             marker=\"x\", color=\"r\", label=\"best model\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss Value\")\n",
    "    plt.legend(); \n",
    "    \n",
    "    #Train and test accuracy plot\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.title(\"Dice Score Curve\")\n",
    "    plt.plot(History.history[\"dice_coef\"], label=\"dice_coef\")\n",
    "    plt.plot(History.history[\"val_dice_coef\"], label=\"val_dice_coef\")\n",
    "    #plt.plot(History.history[\"categorical_accuracy\"], label=\"categorical_accuracy\")\n",
    "    #plt.plot(History.history[\"val_categorical_accuracy\"], label=\"val_categorical_accuracy\")\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Dice Coef')\n",
    "    plt.legend(); \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1a/b & 2 & 3 & 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: 0/187  of train images\n",
      "Reading: 50/187  of train images\n",
      "Reading: 100/187  of train images\n",
      "Reading: 150/187  of train images\n",
      "Reading: 0/47  of test images\n",
      "(187, 256, 256, 1)\n",
      "(47, 256, 256, 1)\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 256, 256, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 256, 256, 32) 320         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 256, 256, 32) 0           conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_16 (SpatialDr (None, 256, 256, 32) 0           activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 256, 256, 32) 9248        spatial_dropout2d_16[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 256, 256, 32) 0           conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_17 (SpatialDr (None, 256, 256, 32) 0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 128, 128, 32) 0           spatial_dropout2d_17[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 128, 128, 64) 18496       max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 128, 128, 64) 0           conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_18 (SpatialDr (None, 128, 128, 64) 0           activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 128, 128, 64) 36928       spatial_dropout2d_18[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 128, 128, 64) 0           conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_19 (SpatialDr (None, 128, 128, 64) 0           activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 64, 64, 64)   0           spatial_dropout2d_19[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 64, 64, 128)  73856       max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 64, 64, 128)  0           conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_20 (SpatialDr (None, 64, 64, 128)  0           activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 64, 64, 128)  147584      spatial_dropout2d_20[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 64, 64, 128)  0           conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_21 (SpatialDr (None, 64, 64, 128)  0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 32, 32, 128)  0           spatial_dropout2d_21[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 32, 32, 256)  295168      max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 32, 32, 256)  0           conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_22 (SpatialDr (None, 32, 32, 256)  0           activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 32, 32, 256)  590080      spatial_dropout2d_22[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 32, 32, 256)  0           conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_23 (SpatialDr (None, 32, 32, 256)  0           activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 16, 16, 256)  0           spatial_dropout2d_23[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 16, 16, 512)  1180160     max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 16, 16, 512)  0           conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_24 (SpatialDr (None, 16, 16, 512)  0           activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTrans (None, 32, 32, 256)  524544      spatial_dropout2d_24[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 32, 32, 512)  0           spatial_dropout2d_23[0][0]       \n",
      "                                                                 conv2d_transpose_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 32, 32, 256)  1179904     concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 32, 32, 256)  0           conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_25 (SpatialDr (None, 32, 32, 256)  0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 32, 32, 256)  590080      spatial_dropout2d_25[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 32, 32, 256)  0           conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_26 (SpatialDr (None, 32, 32, 256)  0           activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTrans (None, 64, 64, 128)  131200      spatial_dropout2d_26[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 64, 64, 256)  0           spatial_dropout2d_21[0][0]       \n",
      "                                                                 conv2d_transpose_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 64, 64, 128)  295040      concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 64, 64, 128)  0           conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_27 (SpatialDr (None, 64, 64, 128)  0           activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 64, 64, 128)  147584      spatial_dropout2d_27[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 64, 64, 128)  0           conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_28 (SpatialDr (None, 64, 64, 128)  0           activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_6 (Conv2DTrans (None, 128, 128, 64) 32832       spatial_dropout2d_28[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 128, 128, 128 0           spatial_dropout2d_19[0][0]       \n",
      "                                                                 conv2d_transpose_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 128, 128, 64) 73792       concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 128, 128, 64) 0           conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_29 (SpatialDr (None, 128, 128, 64) 0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 128, 128, 64) 36928       spatial_dropout2d_29[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 128, 128, 64) 0           conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_30 (SpatialDr (None, 128, 128, 64) 0           activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_7 (Conv2DTrans (None, 256, 256, 32) 8224        spatial_dropout2d_30[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 256, 256, 64) 0           spatial_dropout2d_17[0][0]       \n",
      "                                                                 conv2d_transpose_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 256, 256, 32) 18464       concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 256, 256, 32) 0           conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_31 (SpatialDr (None, 256, 256, 32) 0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 256, 256, 32) 9248        spatial_dropout2d_31[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 256, 256, 32) 0           conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 256, 256, 1)  289         activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 256, 256, 1)  0           conv2d_35[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 5,399,969\n",
      "Trainable params: 5,399,969\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 11s 463ms/step - loss: 0.6557 - dice_coef: 0.3443 - val_loss: 0.6625 - val_dice_coef: 0.3375\n",
      "Epoch 2/150\n",
      "24/24 [==============================] - 2s 90ms/step - loss: 0.6360 - dice_coef: 0.3641 - val_loss: 0.5875 - val_dice_coef: 0.4125\n",
      "Epoch 3/150\n",
      "24/24 [==============================] - 2s 89ms/step - loss: 0.5862 - dice_coef: 0.4138 - val_loss: 0.6029 - val_dice_coef: 0.3971\n",
      "Epoch 4/150\n",
      "24/24 [==============================] - 2s 86ms/step - loss: 0.5878 - dice_coef: 0.4121 - val_loss: 0.6070 - val_dice_coef: 0.3930\n",
      "Epoch 5/150\n",
      "24/24 [==============================] - 2s 86ms/step - loss: 0.5862 - dice_coef: 0.4138 - val_loss: 0.6024 - val_dice_coef: 0.3976\n",
      "Epoch 6/150\n",
      "24/24 [==============================] - 2s 89ms/step - loss: 0.5803 - dice_coef: 0.4198 - val_loss: 0.5837 - val_dice_coef: 0.4163\n",
      "Epoch 7/150\n",
      "24/24 [==============================] - 2s 86ms/step - loss: 0.5724 - dice_coef: 0.4276 - val_loss: 0.5586 - val_dice_coef: 0.4414\n",
      "Epoch 8/150\n",
      "24/24 [==============================] - 2s 89ms/step - loss: 0.5567 - dice_coef: 0.4433 - val_loss: 0.5414 - val_dice_coef: 0.4586\n",
      "Epoch 9/150\n",
      "24/24 [==============================] - 2s 86ms/step - loss: 0.5506 - dice_coef: 0.4494 - val_loss: 0.5343 - val_dice_coef: 0.4657\n",
      "Epoch 10/150\n",
      "24/24 [==============================] - 2s 89ms/step - loss: 0.5455 - dice_coef: 0.4545 - val_loss: 0.5378 - val_dice_coef: 0.4622\n",
      "Epoch 11/150\n",
      "24/24 [==============================] - 2s 89ms/step - loss: 0.5376 - dice_coef: 0.4625 - val_loss: 0.5309 - val_dice_coef: 0.4691\n",
      "Epoch 12/150\n",
      "24/24 [==============================] - 2s 90ms/step - loss: 0.5300 - dice_coef: 0.4701 - val_loss: 0.5037 - val_dice_coef: 0.4963\n",
      "Epoch 13/150\n",
      "24/24 [==============================] - 2s 91ms/step - loss: 0.5018 - dice_coef: 0.4982 - val_loss: 0.3537 - val_dice_coef: 0.6463\n",
      "Epoch 14/150\n",
      "24/24 [==============================] - 2s 89ms/step - loss: 0.3876 - dice_coef: 0.6123 - val_loss: 0.3410 - val_dice_coef: 0.6590\n",
      "Epoch 15/150\n",
      "24/24 [==============================] - 2s 89ms/step - loss: 0.3529 - dice_coef: 0.6471 - val_loss: 0.2696 - val_dice_coef: 0.7304\n",
      "Epoch 16/150\n",
      "24/24 [==============================] - 2s 89ms/step - loss: 0.3504 - dice_coef: 0.6496 - val_loss: 0.2538 - val_dice_coef: 0.7462\n",
      "Epoch 17/150\n",
      "24/24 [==============================] - 2s 89ms/step - loss: 0.3337 - dice_coef: 0.6663 - val_loss: 0.3030 - val_dice_coef: 0.6970\n",
      "Epoch 18/150\n",
      "24/24 [==============================] - 2s 90ms/step - loss: 0.3365 - dice_coef: 0.6634 - val_loss: 0.2997 - val_dice_coef: 0.7003\n",
      "Epoch 19/150\n",
      "24/24 [==============================] - 2s 90ms/step - loss: 0.3165 - dice_coef: 0.6836 - val_loss: 0.2360 - val_dice_coef: 0.7640\n",
      "Epoch 20/150\n",
      "24/24 [==============================] - 2s 90ms/step - loss: 0.3127 - dice_coef: 0.6874 - val_loss: 0.2506 - val_dice_coef: 0.7494\n",
      "Epoch 21/150\n",
      "24/24 [==============================] - 2s 86ms/step - loss: 0.3078 - dice_coef: 0.6922 - val_loss: 0.2395 - val_dice_coef: 0.7605\n",
      "Epoch 22/150\n",
      "24/24 [==============================] - 2s 90ms/step - loss: 0.3068 - dice_coef: 0.6932 - val_loss: 0.2721 - val_dice_coef: 0.7279\n",
      "Epoch 23/150\n",
      "24/24 [==============================] - 2s 90ms/step - loss: 0.3082 - dice_coef: 0.6919 - val_loss: 0.2495 - val_dice_coef: 0.7505\n",
      "Epoch 24/150\n",
      "24/24 [==============================] - 2s 90ms/step - loss: 0.2980 - dice_coef: 0.7021 - val_loss: 0.2300 - val_dice_coef: 0.7700\n",
      "Epoch 25/150\n",
      "24/24 [==============================] - 2s 90ms/step - loss: 0.2984 - dice_coef: 0.7017 - val_loss: 0.2271 - val_dice_coef: 0.7729\n",
      "Epoch 26/150\n",
      "24/24 [==============================] - 2s 90ms/step - loss: 0.3031 - dice_coef: 0.6969 - val_loss: 0.2417 - val_dice_coef: 0.7583\n",
      "Epoch 27/150\n",
      "24/24 [==============================] - 2s 91ms/step - loss: 0.2978 - dice_coef: 0.7023 - val_loss: 0.2268 - val_dice_coef: 0.7732\n",
      "Epoch 28/150\n",
      "24/24 [==============================] - 2s 90ms/step - loss: 0.2985 - dice_coef: 0.7015 - val_loss: 0.2342 - val_dice_coef: 0.7658\n",
      "Epoch 29/150\n",
      "24/24 [==============================] - 2s 90ms/step - loss: 0.3043 - dice_coef: 0.6956 - val_loss: 0.2419 - val_dice_coef: 0.7581\n",
      "Epoch 30/150\n",
      "24/24 [==============================] - 2s 91ms/step - loss: 0.2948 - dice_coef: 0.7052 - val_loss: 0.2442 - val_dice_coef: 0.7558\n",
      "Epoch 31/150\n",
      "24/24 [==============================] - 2s 92ms/step - loss: 0.2965 - dice_coef: 0.7035 - val_loss: 0.2536 - val_dice_coef: 0.7464\n",
      "Epoch 32/150\n",
      "24/24 [==============================] - 2s 90ms/step - loss: 0.2981 - dice_coef: 0.7019 - val_loss: 0.2375 - val_dice_coef: 0.7625\n",
      "Epoch 33/150\n",
      "24/24 [==============================] - 2s 91ms/step - loss: 0.2904 - dice_coef: 0.7096 - val_loss: 0.2267 - val_dice_coef: 0.7733\n",
      "Epoch 34/150\n",
      "24/24 [==============================] - 2s 90ms/step - loss: 0.2942 - dice_coef: 0.7057 - val_loss: 0.2368 - val_dice_coef: 0.7632\n",
      "Epoch 35/150\n",
      "24/24 [==============================] - 2s 90ms/step - loss: 0.2915 - dice_coef: 0.7085 - val_loss: 0.2273 - val_dice_coef: 0.7727\n",
      "Epoch 36/150\n",
      "24/24 [==============================] - 2s 92ms/step - loss: 0.2904 - dice_coef: 0.7096 - val_loss: 0.2276 - val_dice_coef: 0.7724\n",
      "Epoch 37/150\n",
      "24/24 [==============================] - 2s 91ms/step - loss: 0.2892 - dice_coef: 0.7108 - val_loss: 0.2130 - val_dice_coef: 0.7870\n",
      "Epoch 38/150\n",
      "24/24 [==============================] - 2s 91ms/step - loss: 0.2926 - dice_coef: 0.7074 - val_loss: 0.2206 - val_dice_coef: 0.7794\n",
      "Epoch 39/150\n",
      "24/24 [==============================] - 2s 91ms/step - loss: 0.2912 - dice_coef: 0.7088 - val_loss: 0.2288 - val_dice_coef: 0.7712\n",
      "Epoch 40/150\n",
      "24/24 [==============================] - 2s 92ms/step - loss: 0.2952 - dice_coef: 0.7048 - val_loss: 0.2092 - val_dice_coef: 0.7908\n",
      "Epoch 41/150\n",
      "24/24 [==============================] - 2s 92ms/step - loss: 0.2879 - dice_coef: 0.7121 - val_loss: 0.2046 - val_dice_coef: 0.7954\n",
      "Epoch 42/150\n",
      "24/24 [==============================] - 2s 93ms/step - loss: 0.2965 - dice_coef: 0.7035 - val_loss: 0.2330 - val_dice_coef: 0.7670\n",
      "Epoch 43/150\n",
      "24/24 [==============================] - 2s 93ms/step - loss: 0.2842 - dice_coef: 0.7157 - val_loss: 0.2186 - val_dice_coef: 0.7814\n",
      "Epoch 44/150\n",
      "24/24 [==============================] - 2s 93ms/step - loss: 0.2909 - dice_coef: 0.7091 - val_loss: 0.2090 - val_dice_coef: 0.7910\n",
      "Epoch 45/150\n",
      "24/24 [==============================] - 2s 93ms/step - loss: 0.2935 - dice_coef: 0.7065 - val_loss: 0.2025 - val_dice_coef: 0.7975\n",
      "Epoch 46/150\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.2903 - dice_coef: 0.7097 - val_loss: 0.2120 - val_dice_coef: 0.7880\n",
      "Epoch 47/150\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.2883 - dice_coef: 0.7117 - val_loss: 0.2178 - val_dice_coef: 0.7822\n",
      "Epoch 48/150\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.2880 - dice_coef: 0.7120 - val_loss: 0.2282 - val_dice_coef: 0.7718\n",
      "Epoch 49/150\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.2866 - dice_coef: 0.7133 - val_loss: 0.2229 - val_dice_coef: 0.7771\n",
      "Epoch 50/150\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.2829 - dice_coef: 0.7171 - val_loss: 0.2158 - val_dice_coef: 0.7842\n",
      "Epoch 51/150\n",
      "24/24 [==============================] - 2s 93ms/step - loss: 0.2840 - dice_coef: 0.7160 - val_loss: 0.2135 - val_dice_coef: 0.7865\n",
      "Epoch 52/150\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.2847 - dice_coef: 0.7153 - val_loss: 0.2067 - val_dice_coef: 0.7933\n",
      "Epoch 53/150\n",
      "24/24 [==============================] - 2s 96ms/step - loss: 0.2893 - dice_coef: 0.7107 - val_loss: 0.2296 - val_dice_coef: 0.7704\n",
      "Epoch 54/150\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.2802 - dice_coef: 0.7198 - val_loss: 0.2249 - val_dice_coef: 0.7751\n",
      "Epoch 55/150\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.2810 - dice_coef: 0.7191 - val_loss: 0.2137 - val_dice_coef: 0.7863\n",
      "Epoch 56/150\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.2755 - dice_coef: 0.7246 - val_loss: 0.2276 - val_dice_coef: 0.7724\n",
      "Epoch 57/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 2s 94ms/step - loss: 0.2873 - dice_coef: 0.7127 - val_loss: 0.2332 - val_dice_coef: 0.7668\n",
      "Epoch 58/150\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.2845 - dice_coef: 0.7156 - val_loss: 0.2197 - val_dice_coef: 0.7803\n",
      "Epoch 59/150\n",
      "24/24 [==============================] - 2s 93ms/step - loss: 0.2884 - dice_coef: 0.7116 - val_loss: 0.2205 - val_dice_coef: 0.7795\n",
      "Epoch 60/150\n",
      "24/24 [==============================] - 2s 95ms/step - loss: 0.2788 - dice_coef: 0.7212 - val_loss: 0.2231 - val_dice_coef: 0.7769\n",
      "Epoch 61/150\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.2850 - dice_coef: 0.7150 - val_loss: 0.2162 - val_dice_coef: 0.7838\n",
      "Epoch 62/150\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.2847 - dice_coef: 0.7153 - val_loss: 0.1948 - val_dice_coef: 0.8052\n",
      "Epoch 63/150\n",
      "24/24 [==============================] - 2s 93ms/step - loss: 0.2833 - dice_coef: 0.7167 - val_loss: 0.2150 - val_dice_coef: 0.7850\n",
      "Epoch 64/150\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.2750 - dice_coef: 0.7248 - val_loss: 0.2219 - val_dice_coef: 0.7781\n",
      "Epoch 65/150\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.2843 - dice_coef: 0.7157 - val_loss: 0.2426 - val_dice_coef: 0.7574\n",
      "Epoch 66/150\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.2797 - dice_coef: 0.7204 - val_loss: 0.2029 - val_dice_coef: 0.7971\n",
      "Epoch 67/150\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.2868 - dice_coef: 0.7131 - val_loss: 0.2072 - val_dice_coef: 0.7928\n",
      "Epoch 68/150\n",
      "24/24 [==============================] - 2s 95ms/step - loss: 0.2783 - dice_coef: 0.7217 - val_loss: 0.2215 - val_dice_coef: 0.7785\n",
      "Epoch 69/150\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.2807 - dice_coef: 0.7193 - val_loss: 0.2269 - val_dice_coef: 0.7731\n",
      "Epoch 70/150\n",
      "24/24 [==============================] - 2s 95ms/step - loss: 0.2728 - dice_coef: 0.7272 - val_loss: 0.2056 - val_dice_coef: 0.7944\n",
      "Epoch 71/150\n",
      "24/24 [==============================] - 2s 95ms/step - loss: 0.2771 - dice_coef: 0.7229 - val_loss: 0.2053 - val_dice_coef: 0.7947\n",
      "Epoch 72/150\n",
      "24/24 [==============================] - 2s 95ms/step - loss: 0.2804 - dice_coef: 0.7196 - val_loss: 0.1925 - val_dice_coef: 0.8075\n",
      "Epoch 73/150\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.2775 - dice_coef: 0.7225 - val_loss: 0.2085 - val_dice_coef: 0.7915\n",
      "Epoch 74/150\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.2852 - dice_coef: 0.7148 - val_loss: 0.2062 - val_dice_coef: 0.7938\n",
      "Epoch 75/150\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.2806 - dice_coef: 0.7193 - val_loss: 0.2153 - val_dice_coef: 0.7847\n",
      "Epoch 76/150\n",
      "24/24 [==============================] - 2s 95ms/step - loss: 0.2716 - dice_coef: 0.7284 - val_loss: 0.1856 - val_dice_coef: 0.8144\n",
      "Epoch 77/150\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.2756 - dice_coef: 0.7244 - val_loss: 0.2232 - val_dice_coef: 0.7768\n",
      "Epoch 78/150\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.2741 - dice_coef: 0.7260 - val_loss: 0.2009 - val_dice_coef: 0.7991\n",
      "Epoch 79/150\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.2682 - dice_coef: 0.7318 - val_loss: 0.1935 - val_dice_coef: 0.8065\n",
      "Epoch 80/150\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.2736 - dice_coef: 0.7264 - val_loss: 0.1903 - val_dice_coef: 0.8097\n",
      "Epoch 81/150\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.2810 - dice_coef: 0.7190 - val_loss: 0.1908 - val_dice_coef: 0.8092\n",
      "Epoch 82/150\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.2732 - dice_coef: 0.7268 - val_loss: 0.1936 - val_dice_coef: 0.8064\n",
      "Epoch 83/150\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.2734 - dice_coef: 0.7265 - val_loss: 0.2141 - val_dice_coef: 0.7859\n",
      "Epoch 84/150\n",
      "24/24 [==============================] - 2s 93ms/step - loss: 0.2739 - dice_coef: 0.7261 - val_loss: 0.1836 - val_dice_coef: 0.8164\n",
      "Epoch 85/150\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.2761 - dice_coef: 0.7239 - val_loss: 0.2143 - val_dice_coef: 0.7857\n",
      "Epoch 86/150\n",
      "24/24 [==============================] - 2s 93ms/step - loss: 0.2769 - dice_coef: 0.7231 - val_loss: 0.1982 - val_dice_coef: 0.8018\n",
      "Epoch 87/150\n",
      "24/24 [==============================] - 2s 95ms/step - loss: 0.2747 - dice_coef: 0.7254 - val_loss: 0.2084 - val_dice_coef: 0.7916\n",
      "Epoch 88/150\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.2760 - dice_coef: 0.7240 - val_loss: 0.2071 - val_dice_coef: 0.7929\n",
      "Epoch 89/150\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.2721 - dice_coef: 0.7279 - val_loss: 0.1942 - val_dice_coef: 0.8058\n",
      "Epoch 90/150\n",
      "24/24 [==============================] - 2s 95ms/step - loss: 0.2814 - dice_coef: 0.7186 - val_loss: 0.2010 - val_dice_coef: 0.7990\n",
      "Epoch 91/150\n",
      "24/24 [==============================] - 2s 95ms/step - loss: 0.2715 - dice_coef: 0.7285 - val_loss: 0.1957 - val_dice_coef: 0.8043\n",
      "Epoch 92/150\n",
      "24/24 [==============================] - 2s 95ms/step - loss: 0.2713 - dice_coef: 0.7288 - val_loss: 0.1945 - val_dice_coef: 0.8055\n",
      "Epoch 93/150\n",
      "24/24 [==============================] - 2s 96ms/step - loss: 0.2762 - dice_coef: 0.7238 - val_loss: 0.1989 - val_dice_coef: 0.8011\n",
      "Epoch 94/150\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.2761 - dice_coef: 0.7239 - val_loss: 0.1966 - val_dice_coef: 0.8034\n",
      "Epoch 95/150\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.2775 - dice_coef: 0.7226 - val_loss: 0.1985 - val_dice_coef: 0.8015\n",
      "Epoch 96/150\n",
      "24/24 [==============================] - 2s 95ms/step - loss: 0.2771 - dice_coef: 0.7229 - val_loss: 0.2040 - val_dice_coef: 0.7960\n",
      "Epoch 97/150\n",
      "24/24 [==============================] - 2s 95ms/step - loss: 0.2723 - dice_coef: 0.7276 - val_loss: 0.1892 - val_dice_coef: 0.8108\n",
      "Epoch 98/150\n",
      "24/24 [==============================] - 2s 95ms/step - loss: 0.2793 - dice_coef: 0.7207 - val_loss: 0.2090 - val_dice_coef: 0.7910\n",
      "Epoch 99/150\n",
      "24/24 [==============================] - 2s 95ms/step - loss: 0.2738 - dice_coef: 0.7262 - val_loss: 0.2097 - val_dice_coef: 0.7903\n",
      "Epoch 100/150\n",
      "24/24 [==============================] - 2s 95ms/step - loss: 0.2775 - dice_coef: 0.7226 - val_loss: 0.2118 - val_dice_coef: 0.7882\n",
      "Epoch 101/150\n",
      "24/24 [==============================] - 2s 95ms/step - loss: 0.2719 - dice_coef: 0.7281 - val_loss: 0.1868 - val_dice_coef: 0.8132\n",
      "Epoch 102/150\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.2659 - dice_coef: 0.7341 - val_loss: 0.1841 - val_dice_coef: 0.8159\n",
      "Epoch 103/150\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.2731 - dice_coef: 0.7269 - val_loss: 0.1977 - val_dice_coef: 0.8023\n",
      "Epoch 104/150\n",
      "24/24 [==============================] - 2s 95ms/step - loss: 0.2785 - dice_coef: 0.7215 - val_loss: 0.1928 - val_dice_coef: 0.8072\n",
      "Epoch 105/150\n",
      "24/24 [==============================] - 2s 95ms/step - loss: 0.2779 - dice_coef: 0.7220 - val_loss: 0.1931 - val_dice_coef: 0.8069\n",
      "Epoch 106/150\n",
      "24/24 [==============================] - 2s 95ms/step - loss: 0.2751 - dice_coef: 0.7249 - val_loss: 0.2007 - val_dice_coef: 0.7993\n",
      "Epoch 107/150\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.2681 - dice_coef: 0.7319 - val_loss: 0.1896 - val_dice_coef: 0.8104\n",
      "Epoch 108/150\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.2720 - dice_coef: 0.7279 - val_loss: 0.1863 - val_dice_coef: 0.8137\n",
      "Epoch 109/150\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.2674 - dice_coef: 0.7327 - val_loss: 0.1801 - val_dice_coef: 0.8199\n",
      "Epoch 110/150\n",
      "24/24 [==============================] - 2s 93ms/step - loss: 0.2716 - dice_coef: 0.7284 - val_loss: 0.1931 - val_dice_coef: 0.8069\n",
      "Epoch 111/150\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.2687 - dice_coef: 0.7313 - val_loss: 0.1915 - val_dice_coef: 0.8085\n",
      "Epoch 112/150\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.2752 - dice_coef: 0.7248 - val_loss: 0.1961 - val_dice_coef: 0.8039\n",
      "Epoch 113/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 2s 93ms/step - loss: 0.2695 - dice_coef: 0.7305 - val_loss: 0.2052 - val_dice_coef: 0.7948\n",
      "Epoch 114/150\n",
      "24/24 [==============================] - 2s 95ms/step - loss: 0.2644 - dice_coef: 0.7356 - val_loss: 0.1839 - val_dice_coef: 0.8161\n",
      "Epoch 115/150\n",
      "24/24 [==============================] - 2s 93ms/step - loss: 0.2693 - dice_coef: 0.7307 - val_loss: 0.1882 - val_dice_coef: 0.8118\n",
      "Epoch 116/150\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.2704 - dice_coef: 0.7295 - val_loss: 0.2023 - val_dice_coef: 0.7977\n",
      "Epoch 117/150\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.2628 - dice_coef: 0.7372 - val_loss: 0.1891 - val_dice_coef: 0.8109\n",
      "Epoch 118/150\n",
      "24/24 [==============================] - 2s 95ms/step - loss: 0.2692 - dice_coef: 0.7308 - val_loss: 0.1944 - val_dice_coef: 0.8056\n",
      "Epoch 119/150\n",
      "24/24 [==============================] - 2s 95ms/step - loss: 0.2726 - dice_coef: 0.7274 - val_loss: 0.1888 - val_dice_coef: 0.8112\n",
      "Epoch 120/150\n",
      "24/24 [==============================] - 2s 95ms/step - loss: 0.2726 - dice_coef: 0.7274 - val_loss: 0.1883 - val_dice_coef: 0.8117\n",
      "Epoch 121/150\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.2734 - dice_coef: 0.7266 - val_loss: 0.1926 - val_dice_coef: 0.8074\n",
      "Epoch 122/150\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.2648 - dice_coef: 0.7353 - val_loss: 0.1898 - val_dice_coef: 0.8102\n",
      "Epoch 123/150\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.2619 - dice_coef: 0.7382 - val_loss: 0.1895 - val_dice_coef: 0.8105\n",
      "Epoch 124/150\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.2748 - dice_coef: 0.7253 - val_loss: 0.1964 - val_dice_coef: 0.8036\n",
      "Epoch 125/150\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.2682 - dice_coef: 0.7318 - val_loss: 0.1939 - val_dice_coef: 0.8061\n",
      "Epoch 126/150\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.2651 - dice_coef: 0.7350 - val_loss: 0.1810 - val_dice_coef: 0.8190\n",
      "Epoch 127/150\n",
      "24/24 [==============================] - 2s 95ms/step - loss: 0.2742 - dice_coef: 0.7258 - val_loss: 0.1969 - val_dice_coef: 0.8031\n",
      "Epoch 128/150\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.2766 - dice_coef: 0.7233 - val_loss: 0.2112 - val_dice_coef: 0.7888\n",
      "Epoch 129/150\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.2631 - dice_coef: 0.7369 - val_loss: 0.1660 - val_dice_coef: 0.8340\n",
      "Epoch 130/150\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.2605 - dice_coef: 0.7395 - val_loss: 0.1757 - val_dice_coef: 0.8243\n",
      "Epoch 131/150\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.2632 - dice_coef: 0.7368 - val_loss: 0.1887 - val_dice_coef: 0.8113\n",
      "Epoch 132/150\n",
      "24/24 [==============================] - 2s 92ms/step - loss: 0.2668 - dice_coef: 0.7331 - val_loss: 0.1787 - val_dice_coef: 0.8213\n",
      "Epoch 133/150\n",
      "24/24 [==============================] - 2s 93ms/step - loss: 0.2596 - dice_coef: 0.7405 - val_loss: 0.1859 - val_dice_coef: 0.8141\n",
      "Epoch 134/150\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.2732 - dice_coef: 0.7268 - val_loss: 0.1870 - val_dice_coef: 0.8130\n",
      "Epoch 135/150\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.2636 - dice_coef: 0.7364 - val_loss: 0.1687 - val_dice_coef: 0.8313\n",
      "Epoch 136/150\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.2726 - dice_coef: 0.7274 - val_loss: 0.1903 - val_dice_coef: 0.8097\n",
      "Epoch 137/150\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.2674 - dice_coef: 0.7326 - val_loss: 0.1870 - val_dice_coef: 0.8130\n",
      "Epoch 138/150\n",
      "24/24 [==============================] - 2s 95ms/step - loss: 0.2619 - dice_coef: 0.7381 - val_loss: 0.1870 - val_dice_coef: 0.8130\n",
      "Epoch 139/150\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.2723 - dice_coef: 0.7276 - val_loss: 0.1894 - val_dice_coef: 0.8106\n",
      "Epoch 140/150\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.2645 - dice_coef: 0.7355 - val_loss: 0.1873 - val_dice_coef: 0.8127\n",
      "Epoch 141/150\n",
      "24/24 [==============================] - 2s 93ms/step - loss: 0.2614 - dice_coef: 0.7387 - val_loss: 0.1907 - val_dice_coef: 0.8093\n",
      "Epoch 142/150\n",
      "24/24 [==============================] - 2s 93ms/step - loss: 0.2667 - dice_coef: 0.7332 - val_loss: 0.2051 - val_dice_coef: 0.7949\n",
      "Epoch 143/150\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.2608 - dice_coef: 0.7392 - val_loss: 0.1881 - val_dice_coef: 0.8119\n",
      "Epoch 144/150\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.2583 - dice_coef: 0.7417 - val_loss: 0.1837 - val_dice_coef: 0.8163\n",
      "Epoch 145/150\n",
      "24/24 [==============================] - 2s 93ms/step - loss: 0.2723 - dice_coef: 0.7277 - val_loss: 0.1934 - val_dice_coef: 0.8066\n",
      "Epoch 146/150\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.2710 - dice_coef: 0.7289 - val_loss: 0.1968 - val_dice_coef: 0.8032\n",
      "Epoch 147/150\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.2649 - dice_coef: 0.7350 - val_loss: 0.1876 - val_dice_coef: 0.8124\n",
      "Epoch 148/150\n",
      "24/24 [==============================] - 2s 94ms/step - loss: 0.2724 - dice_coef: 0.7275 - val_loss: 0.1833 - val_dice_coef: 0.8167\n",
      "Epoch 149/150\n",
      "24/24 [==============================] - 2s 93ms/step - loss: 0.2682 - dice_coef: 0.7319 - val_loss: 0.1868 - val_dice_coef: 0.8132\n",
      "Epoch 150/150\n",
      "24/24 [==============================] - 2s 93ms/step - loss: 0.2611 - dice_coef: 0.7389 - val_loss: 0.1835 - val_dice_coef: 0.8165\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from scipy.spatial.distance import dice\n",
    "image_size = 256\n",
    "img_ch = 1\n",
    "batch_size = 8\n",
    "LR = 0.0001\n",
    "SDRate = 0.5\n",
    "spatial_dropout = True\n",
    "epochs = 150\n",
    "p = 0.2 #percentage of training and test data\n",
    "path = '/Lab1/Lab3/X_ray/'\n",
    "fold1 = 'Image'\n",
    "fold2 = 'Mask'\n",
    "task = '4' \n",
    "\n",
    "if task == '4':\n",
    "    \n",
    "    base  = 32\n",
    "    batch_normalization = False\n",
    "    rotation_range = 10\n",
    "    width_shift = 0.1\n",
    "    height_shift = 0.1\n",
    "    rescale = 0.2\n",
    "    horizontal_flip = True\n",
    "    train_datagen, val_datagen = DataAugmentation(rotation_range,width_shift,height_shift,rescale,horizontal_flip)\n",
    "    train_img, train_mask, test_img, test_mask = get_train_test_data(fold1, fold2, path, p,image_size, image_size)\n",
    "    model = u_net(base,image_size, image_size, img_ch, batch_normalization, SDRate, spatial_dropout)\n",
    "    model.compile(optimizer = Adam(lr=LR), loss = dice_coef_loss, metrics =[dice_coef])\n",
    "    #Fit the data into the model\n",
    "    History = model.fit_generator(train_datagen.flow(train_img, train_mask,batch_size = batch_size), validation_data = val_datagen.flow(test_img, test_mask), epochs = epochs, verbose = 1)        \n",
    "\n",
    "else:\n",
    "    if task == '1a':\n",
    "        base = 16\n",
    "        batch_normalization = True\n",
    "        train_img, train_mask, test_img, test_mask = get_train_test_data(fold1, fold2, path, p, image_size, image_size)\n",
    "        model = u_net(base,image_size, image_size, img_ch, batch_normalization, SDRate, spatial_dropout)\n",
    "        model.compile(optimizer = Adam(lr=LR), loss = 'binary_crossentropy', metrics =[dice_coef])\n",
    "\n",
    "    elif task == '1b':\n",
    "        base = 16\n",
    "        batch_normalization = True\n",
    "        train_img, train_mask, test_img, test_mask = get_train_test_data(fold1, fold2, path, p,image_size, image_size)\n",
    "        model = u_net(base,image_size, image_size, img_ch, batch_normalization, SDRate, spatial_dropout)\n",
    "        model.compile(optimizer = Adam(lr=LR), loss = dice_coef_loss, metrics =[dice_coef])\n",
    "\n",
    "    elif task == '2a':\n",
    "        base = 16\n",
    "        batch_normalization = False\n",
    "        train_img, train_mask, test_img, test_mask = get_train_test_data(fold1, fold2, path, p,image_size, image_size)\n",
    "        model = u_net(base,image_size, image_size, img_ch, batch_normalization, SDRate, spatial_dropout)\n",
    "        model.compile(optimizer = Adam(lr=LR), loss = 'binary_crossentropy', metrics =[dice_coef])\n",
    "\n",
    "    elif task == '2b':\n",
    "        base = 16\n",
    "        batch_normalization = False\n",
    "        train_img, train_mask, test_img, test_mask = get_train_test_data(fold1, fold2, path, p,image_size, image_size)\n",
    "        model = u_net(base,image_size, image_size, img_ch, batch_normalization, SDRate, spatial_dropout)\n",
    "        model.compile(optimizer = Adam(lr=LR), loss = dice_coef_loss, metrics =[dice_coef])\n",
    "\n",
    "    elif task == '3':\n",
    "        base = 32\n",
    "        batch_normalization = False\n",
    "        train_img, train_mask, test_img, test_mask = get_train_test_data(fold1, fold2, path, p,image_size, image_size)\n",
    "        model = u_net(base,image_size, image_size, img_ch, batch_normalization, SDRate, spatial_dropout)\n",
    "        model.compile(optimizer = Adam(lr=LR), loss = dice_coef_loss, metrics =[dice_coef])\n",
    "    \n",
    "    History = model.fit(train_img, train_mask, epochs = epochs, batch_size = batch_size, verbose = 1,\n",
    "                        validation_data = (test_img,test_mask))\n",
    "    \n",
    "\n",
    "plotter(History)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls '/Lab1/Lab3/CT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7ff2024e48d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = plt.imread('/Lab1/Lab3/CT/Mask/Im127_128_Mask.png')\n",
    "plt.figure()\n",
    "plt.imshow(image)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: 0/6735  of train images\n",
      "Reading: 50/6735  of train images\n",
      "Reading: 100/6735  of train images\n",
      "Reading: 150/6735  of train images\n",
      "Reading: 200/6735  of train images\n",
      "Reading: 250/6735  of train images\n",
      "Reading: 300/6735  of train images\n",
      "Reading: 350/6735  of train images\n",
      "Reading: 400/6735  of train images\n",
      "Reading: 450/6735  of train images\n",
      "Reading: 500/6735  of train images\n",
      "Reading: 550/6735  of train images\n",
      "Reading: 600/6735  of train images\n",
      "Reading: 650/6735  of train images\n",
      "Reading: 700/6735  of train images\n",
      "Reading: 750/6735  of train images\n",
      "Reading: 800/6735  of train images\n",
      "Reading: 850/6735  of train images\n",
      "Reading: 900/6735  of train images\n",
      "Reading: 950/6735  of train images\n",
      "Reading: 1000/6735  of train images\n",
      "Reading: 1050/6735  of train images\n",
      "Reading: 1100/6735  of train images\n",
      "Reading: 1150/6735  of train images\n",
      "Reading: 1200/6735  of train images\n",
      "Reading: 1250/6735  of train images\n",
      "Reading: 1300/6735  of train images\n",
      "Reading: 1350/6735  of train images\n",
      "Reading: 1400/6735  of train images\n",
      "Reading: 1450/6735  of train images\n",
      "Reading: 1500/6735  of train images\n",
      "Reading: 1550/6735  of train images\n",
      "Reading: 1600/6735  of train images\n",
      "Reading: 1650/6735  of train images\n",
      "Reading: 1700/6735  of train images\n",
      "Reading: 1750/6735  of train images\n",
      "Reading: 1800/6735  of train images\n",
      "Reading: 1850/6735  of train images\n",
      "Reading: 1900/6735  of train images\n",
      "Reading: 1950/6735  of train images\n",
      "Reading: 2000/6735  of train images\n",
      "Reading: 2050/6735  of train images\n",
      "Reading: 2100/6735  of train images\n",
      "Reading: 2150/6735  of train images\n",
      "Reading: 2200/6735  of train images\n",
      "Reading: 2250/6735  of train images\n",
      "Reading: 2300/6735  of train images\n",
      "Reading: 2350/6735  of train images\n",
      "Reading: 2400/6735  of train images\n",
      "Reading: 2450/6735  of train images\n",
      "Reading: 2500/6735  of train images\n",
      "Reading: 2550/6735  of train images\n",
      "Reading: 2600/6735  of train images\n",
      "Reading: 2650/6735  of train images\n",
      "Reading: 2700/6735  of train images\n",
      "Reading: 2750/6735  of train images\n",
      "Reading: 2800/6735  of train images\n",
      "Reading: 2850/6735  of train images\n",
      "Reading: 2900/6735  of train images\n",
      "Reading: 2950/6735  of train images\n",
      "Reading: 3000/6735  of train images\n",
      "Reading: 3050/6735  of train images\n",
      "Reading: 3100/6735  of train images\n",
      "Reading: 3150/6735  of train images\n",
      "Reading: 3200/6735  of train images\n",
      "Reading: 3250/6735  of train images\n",
      "Reading: 3300/6735  of train images\n",
      "Reading: 3350/6735  of train images\n",
      "Reading: 3400/6735  of train images\n",
      "Reading: 3450/6735  of train images\n",
      "Reading: 3500/6735  of train images\n",
      "Reading: 3550/6735  of train images\n",
      "Reading: 3600/6735  of train images\n",
      "Reading: 3650/6735  of train images\n",
      "Reading: 3700/6735  of train images\n",
      "Reading: 3750/6735  of train images\n",
      "Reading: 3800/6735  of train images\n",
      "Reading: 3850/6735  of train images\n",
      "Reading: 3900/6735  of train images\n",
      "Reading: 3950/6735  of train images\n",
      "Reading: 4000/6735  of train images\n",
      "Reading: 4050/6735  of train images\n",
      "Reading: 4100/6735  of train images\n",
      "Reading: 4150/6735  of train images\n",
      "Reading: 4200/6735  of train images\n",
      "Reading: 4250/6735  of train images\n",
      "Reading: 4300/6735  of train images\n",
      "Reading: 4350/6735  of train images\n",
      "Reading: 4400/6735  of train images\n",
      "Reading: 4450/6735  of train images\n",
      "Reading: 4500/6735  of train images\n",
      "Reading: 4550/6735  of train images\n",
      "Reading: 4600/6735  of train images\n",
      "Reading: 4650/6735  of train images\n",
      "Reading: 4700/6735  of train images\n",
      "Reading: 4750/6735  of train images\n",
      "Reading: 4800/6735  of train images\n",
      "Reading: 4850/6735  of train images\n",
      "Reading: 4900/6735  of train images\n",
      "Reading: 4950/6735  of train images\n",
      "Reading: 5000/6735  of train images\n",
      "Reading: 5050/6735  of train images\n",
      "Reading: 5100/6735  of train images\n",
      "Reading: 5150/6735  of train images\n",
      "Reading: 5200/6735  of train images\n",
      "Reading: 5250/6735  of train images\n",
      "Reading: 5300/6735  of train images\n",
      "Reading: 5350/6735  of train images\n",
      "Reading: 5400/6735  of train images\n",
      "Reading: 5450/6735  of train images\n",
      "Reading: 5500/6735  of train images\n",
      "Reading: 5550/6735  of train images\n",
      "Reading: 5600/6735  of train images\n",
      "Reading: 5650/6735  of train images\n",
      "Reading: 5700/6735  of train images\n",
      "Reading: 5750/6735  of train images\n",
      "Reading: 5800/6735  of train images\n",
      "Reading: 5850/6735  of train images\n",
      "Reading: 5900/6735  of train images\n",
      "Reading: 5950/6735  of train images\n",
      "Reading: 6000/6735  of train images\n",
      "Reading: 6050/6735  of train images\n",
      "Reading: 6100/6735  of train images\n",
      "Reading: 6150/6735  of train images\n",
      "Reading: 6200/6735  of train images\n",
      "Reading: 6250/6735  of train images\n",
      "Reading: 6300/6735  of train images\n",
      "Reading: 6350/6735  of train images\n",
      "Reading: 6400/6735  of train images\n",
      "Reading: 6450/6735  of train images\n",
      "Reading: 6500/6735  of train images\n",
      "Reading: 6550/6735  of train images\n",
      "Reading: 6600/6735  of train images\n",
      "Reading: 6650/6735  of train images\n",
      "Reading: 6700/6735  of train images\n",
      "Reading: 0/1684  of test images\n",
      "Reading: 50/1684  of test images\n",
      "Reading: 100/1684  of test images\n",
      "Reading: 150/1684  of test images\n",
      "Reading: 200/1684  of test images\n",
      "Reading: 250/1684  of test images\n",
      "Reading: 300/1684  of test images\n",
      "Reading: 350/1684  of test images\n",
      "Reading: 400/1684  of test images\n",
      "Reading: 450/1684  of test images\n",
      "Reading: 500/1684  of test images\n",
      "Reading: 550/1684  of test images\n",
      "Reading: 600/1684  of test images\n",
      "Reading: 650/1684  of test images\n",
      "Reading: 700/1684  of test images\n",
      "Reading: 750/1684  of test images\n",
      "Reading: 800/1684  of test images\n",
      "Reading: 850/1684  of test images\n",
      "Reading: 900/1684  of test images\n",
      "Reading: 950/1684  of test images\n",
      "Reading: 1000/1684  of test images\n",
      "Reading: 1050/1684  of test images\n",
      "Reading: 1100/1684  of test images\n",
      "Reading: 1150/1684  of test images\n",
      "Reading: 1200/1684  of test images\n",
      "Reading: 1250/1684  of test images\n",
      "Reading: 1300/1684  of test images\n",
      "Reading: 1350/1684  of test images\n",
      "Reading: 1400/1684  of test images\n",
      "Reading: 1450/1684  of test images\n",
      "Reading: 1500/1684  of test images\n",
      "Reading: 1550/1684  of test images\n",
      "Reading: 1600/1684  of test images\n",
      "Reading: 1650/1684  of test images\n",
      "(6735, 256, 256, 1)\n",
      "(1684, 256, 256, 1)\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 256, 256, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 256, 256, 16) 160         input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_17 (Batc (None, 256, 256, 16) 64          conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 256, 256, 16) 0           batch_normalization_v2_17[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_32 (SpatialDr (None, 256, 256, 16) 0           activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 256, 256, 16) 2320        spatial_dropout2d_32[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_18 (Batc (None, 256, 256, 16) 64          conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 256, 256, 16) 0           batch_normalization_v2_18[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_33 (SpatialDr (None, 256, 256, 16) 0           activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 128, 128, 16) 0           spatial_dropout2d_33[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 128, 128, 32) 4640        max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_19 (Batc (None, 128, 128, 32) 128         conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 128, 128, 32) 0           batch_normalization_v2_19[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_34 (SpatialDr (None, 128, 128, 32) 0           activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 128, 128, 32) 9248        spatial_dropout2d_34[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_20 (Batc (None, 128, 128, 32) 128         conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 128, 128, 32) 0           batch_normalization_v2_20[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_35 (SpatialDr (None, 128, 128, 32) 0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2D)  (None, 64, 64, 32)   0           spatial_dropout2d_35[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 64, 64, 64)   18496       max_pooling2d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_21 (Batc (None, 64, 64, 64)   256         conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 64, 64, 64)   0           batch_normalization_v2_21[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_36 (SpatialDr (None, 64, 64, 64)   0           activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 64, 64, 64)   36928       spatial_dropout2d_36[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_22 (Batc (None, 64, 64, 64)   256         conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 64, 64, 64)   0           batch_normalization_v2_22[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_37 (SpatialDr (None, 64, 64, 64)   0           activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling2D) (None, 32, 32, 64)   0           spatial_dropout2d_37[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 32, 32, 128)  73856       max_pooling2d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_23 (Batc (None, 32, 32, 128)  512         conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 32, 32, 128)  0           batch_normalization_v2_23[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_38 (SpatialDr (None, 32, 32, 128)  0           activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 32, 32, 128)  147584      spatial_dropout2d_38[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_24 (Batc (None, 32, 32, 128)  512         conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 32, 32, 128)  0           batch_normalization_v2_24[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_39 (SpatialDr (None, 32, 32, 128)  0           activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling2D) (None, 16, 16, 128)  0           spatial_dropout2d_39[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 16, 16, 256)  295168      max_pooling2d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_25 (Batc (None, 16, 16, 256)  1024        conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 16, 16, 256)  0           batch_normalization_v2_25[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_40 (SpatialDr (None, 16, 16, 256)  0           activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_8 (Conv2DTrans (None, 32, 32, 128)  131200      spatial_dropout2d_40[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 32, 32, 256)  0           spatial_dropout2d_39[0][0]       \n",
      "                                                                 conv2d_transpose_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 32, 32, 128)  295040      concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_26 (Batc (None, 32, 32, 128)  512         conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 32, 32, 128)  0           batch_normalization_v2_26[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_41 (SpatialDr (None, 32, 32, 128)  0           activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 32, 32, 128)  147584      spatial_dropout2d_41[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_27 (Batc (None, 32, 32, 128)  512         conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 32, 32, 128)  0           batch_normalization_v2_27[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_42 (SpatialDr (None, 32, 32, 128)  0           activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_9 (Conv2DTrans (None, 64, 64, 64)   32832       spatial_dropout2d_42[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 64, 64, 128)  0           spatial_dropout2d_37[0][0]       \n",
      "                                                                 conv2d_transpose_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 64, 64, 64)   73792       concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_28 (Batc (None, 64, 64, 64)   256         conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 64, 64, 64)   0           batch_normalization_v2_28[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_43 (SpatialDr (None, 64, 64, 64)   0           activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 64, 64, 64)   36928       spatial_dropout2d_43[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_29 (Batc (None, 64, 64, 64)   256         conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 64, 64, 64)   0           batch_normalization_v2_29[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_44 (SpatialDr (None, 64, 64, 64)   0           activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_10 (Conv2DTran (None, 128, 128, 32) 8224        spatial_dropout2d_44[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 128, 128, 64) 0           spatial_dropout2d_35[0][0]       \n",
      "                                                                 conv2d_transpose_10[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 128, 128, 32) 18464       concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_30 (Batc (None, 128, 128, 32) 128         conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 128, 128, 32) 0           batch_normalization_v2_30[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_45 (SpatialDr (None, 128, 128, 32) 0           activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 128, 128, 32) 9248        spatial_dropout2d_45[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_31 (Batc (None, 128, 128, 32) 128         conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 128, 128, 32) 0           batch_normalization_v2_31[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_46 (SpatialDr (None, 128, 128, 32) 0           activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_11 (Conv2DTran (None, 256, 256, 16) 2064        spatial_dropout2d_46[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 256, 256, 32) 0           spatial_dropout2d_33[0][0]       \n",
      "                                                                 conv2d_transpose_11[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 256, 256, 16) 4624        concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_32 (Batc (None, 256, 256, 16) 64          conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 256, 256, 16) 0           batch_normalization_v2_32[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_47 (SpatialDr (None, 256, 256, 16) 0           activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 256, 256, 16) 2320        spatial_dropout2d_47[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_33 (Batc (None, 256, 256, 16) 64          conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 256, 256, 16) 0           batch_normalization_v2_33[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 256, 256, 1)  145         activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 256, 256, 1)  0           conv2d_53[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,355,729\n",
      "Trainable params: 1,353,297\n",
      "Non-trainable params: 2,432\n",
      "__________________________________________________________________________________________________\n",
      "Train on 6735 samples, validate on 1684 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "6735/6735 [==============================] - 55s 8ms/sample - loss: 0.8619 - dice_coef: 0.1382 - val_loss: 0.9320 - val_dice_coef: 0.0681\n",
      "Epoch 2/150\n",
      "6735/6735 [==============================] - 51s 8ms/sample - loss: 0.0225 - dice_coef: 0.9775 - val_loss: 0.4796 - val_dice_coef: 0.5207\n",
      "Epoch 3/150\n",
      "6735/6735 [==============================] - 53s 8ms/sample - loss: 0.0040 - dice_coef: 0.9960 - val_loss: 0.3497 - val_dice_coef: 0.6507\n",
      "Epoch 4/150\n",
      "6735/6735 [==============================] - 53s 8ms/sample - loss: 0.0016 - dice_coef: 0.9984 - val_loss: 0.2530 - val_dice_coef: 0.7473\n",
      "Epoch 5/150\n",
      "6735/6735 [==============================] - 53s 8ms/sample - loss: 8.3456e-04 - dice_coef: 0.9992 - val_loss: 0.1978 - val_dice_coef: 0.8024\n",
      "Epoch 6/150\n",
      "6735/6735 [==============================] - 53s 8ms/sample - loss: 4.5402e-04 - dice_coef: 0.9995 - val_loss: 0.1576 - val_dice_coef: 0.8426\n",
      "Epoch 7/150\n",
      "6735/6735 [==============================] - 53s 8ms/sample - loss: 2.5356e-04 - dice_coef: 0.9997 - val_loss: 0.1121 - val_dice_coef: 0.8881\n",
      "Epoch 8/150\n",
      "6735/6735 [==============================] - 53s 8ms/sample - loss: 1.6345e-04 - dice_coef: 0.9998 - val_loss: 0.1041 - val_dice_coef: 0.8960\n",
      "Epoch 9/150\n",
      "6735/6735 [==============================] - 53s 8ms/sample - loss: 1.0093e-04 - dice_coef: 0.9999 - val_loss: 0.0844 - val_dice_coef: 0.9157\n",
      "Epoch 10/150\n",
      "6735/6735 [==============================] - 53s 8ms/sample - loss: 5.6686e-05 - dice_coef: 0.9999 - val_loss: 0.0655 - val_dice_coef: 0.9346\n",
      "Epoch 11/150\n",
      "6735/6735 [==============================] - 52s 8ms/sample - loss: 4.4468e-05 - dice_coef: 1.0000 - val_loss: 0.0568 - val_dice_coef: 0.9432\n",
      "Epoch 12/150\n",
      "6735/6735 [==============================] - 52s 8ms/sample - loss: 1.9358e-05 - dice_coef: 1.0000 - val_loss: 0.0462 - val_dice_coef: 0.9538\n",
      "Epoch 13/150\n",
      "6735/6735 [==============================] - 52s 8ms/sample - loss: 1.5765e-05 - dice_coef: 1.0000 - val_loss: 0.0363 - val_dice_coef: 0.9637\n",
      "Epoch 14/150\n",
      "6735/6735 [==============================] - 52s 8ms/sample - loss: 9.3161e-06 - dice_coef: 1.0000 - val_loss: 0.0310 - val_dice_coef: 0.9690\n",
      "Epoch 15/150\n",
      "6735/6735 [==============================] - 52s 8ms/sample - loss: 6.7557e-06 - dice_coef: 1.0000 - val_loss: 0.0290 - val_dice_coef: 0.9711\n",
      "Epoch 16/150\n",
      "6735/6735 [==============================] - 52s 8ms/sample - loss: 3.0486e-06 - dice_coef: 1.0000 - val_loss: 0.0220 - val_dice_coef: 0.9780\n",
      "Epoch 17/150\n",
      "6735/6735 [==============================] - 52s 8ms/sample - loss: 2.5515e-06 - dice_coef: 1.0000 - val_loss: 0.0181 - val_dice_coef: 0.9819\n",
      "Epoch 18/150\n",
      "6735/6735 [==============================] - 52s 8ms/sample - loss: 1.3735e-06 - dice_coef: 1.0000 - val_loss: 0.0157 - val_dice_coef: 0.9843\n",
      "Epoch 19/150\n",
      "6735/6735 [==============================] - 52s 8ms/sample - loss: 1.2977e-06 - dice_coef: 1.0000 - val_loss: 0.0134 - val_dice_coef: 0.9866\n",
      "Epoch 20/150\n",
      "6735/6735 [==============================] - 52s 8ms/sample - loss: 9.0666e-07 - dice_coef: 1.0000 - val_loss: 0.0141 - val_dice_coef: 0.9859\n",
      "Epoch 21/150\n",
      "6735/6735 [==============================] - 52s 8ms/sample - loss: 7.2329e-07 - dice_coef: 1.0000 - val_loss: 0.0091 - val_dice_coef: 0.9909\n",
      "Epoch 22/150\n",
      "6735/6735 [==============================] - 52s 8ms/sample - loss: 2.1721e-07 - dice_coef: 1.0000 - val_loss: 0.0082 - val_dice_coef: 0.9918\n",
      "Epoch 23/150\n",
      "6735/6735 [==============================] - 52s 8ms/sample - loss: 1.5052e-07 - dice_coef: 1.0000 - val_loss: 0.0034 - val_dice_coef: 0.9966\n",
      "Epoch 24/150\n",
      "6735/6735 [==============================] - 52s 8ms/sample - loss: 9.1473e-08 - dice_coef: 1.0000 - val_loss: 0.0039 - val_dice_coef: 0.9961\n",
      "Epoch 25/150\n",
      "6735/6735 [==============================] - 52s 8ms/sample - loss: 7.7172e-08 - dice_coef: 1.0000 - val_loss: 0.0024 - val_dice_coef: 0.9976\n",
      "Epoch 26/150\n",
      "6735/6735 [==============================] - 52s 8ms/sample - loss: 7.1508e-08 - dice_coef: 1.0000 - val_loss: 0.0020 - val_dice_coef: 0.9980\n",
      "Epoch 27/150\n",
      "6735/6735 [==============================] - 52s 8ms/sample - loss: 2.7895e-07 - dice_coef: 1.0000 - val_loss: 7.2590e-04 - val_dice_coef: 0.9993\n",
      "Epoch 28/150\n",
      "6735/6735 [==============================] - 52s 8ms/sample - loss: 2.4922e-08 - dice_coef: 1.0000 - val_loss: 4.3285e-04 - val_dice_coef: 0.9996\n",
      "Epoch 29/150\n",
      "6735/6735 [==============================] - 52s 8ms/sample - loss: 1.3027e-08 - dice_coef: 1.0000 - val_loss: 3.0941e-04 - val_dice_coef: 0.9997\n",
      "Epoch 30/150\n",
      "6735/6735 [==============================] - 52s 8ms/sample - loss: 1.2178e-08 - dice_coef: 1.0000 - val_loss: 2.3960e-04 - val_dice_coef: 0.9998\n",
      "Epoch 31/150\n",
      "6735/6735 [==============================] - 52s 8ms/sample - loss: 1.2036e-08 - dice_coef: 1.0000 - val_loss: 1.3270e-04 - val_dice_coef: 0.9999\n",
      "Epoch 32/150\n",
      "6735/6735 [==============================] - 52s 8ms/sample - loss: 5.9472e-09 - dice_coef: 1.0000 - val_loss: 1.8684e-04 - val_dice_coef: 0.9998\n",
      "Epoch 33/150\n",
      "6735/6735 [==============================] - 52s 8ms/sample - loss: 3.6816e-09 - dice_coef: 1.0000 - val_loss: 1.3978e-04 - val_dice_coef: 0.9999\n",
      "Epoch 34/150\n",
      "6735/6735 [==============================] - 52s 8ms/sample - loss: 4.3896e-09 - dice_coef: 1.0000 - val_loss: 4.9487e-05 - val_dice_coef: 1.0000\n",
      "Epoch 35/150\n",
      "6735/6735 [==============================] - 52s 8ms/sample - loss: 3.6816e-09 - dice_coef: 1.0000 - val_loss: 3.4680e-05 - val_dice_coef: 1.0000\n",
      "Epoch 36/150\n",
      "6735/6735 [==============================] - 52s 8ms/sample - loss: 1.1328e-09 - dice_coef: 1.0000 - val_loss: 3.4972e-05 - val_dice_coef: 1.0000\n",
      "Epoch 37/150\n",
      "6735/6735 [==============================] - 52s 8ms/sample - loss: 1.6992e-09 - dice_coef: 1.0000 - val_loss: 7.8443e-06 - val_dice_coef: 1.0000\n",
      "Epoch 38/150\n",
      "6735/6735 [==============================] - 52s 8ms/sample - loss: 1.6992e-09 - dice_coef: 1.0000 - val_loss: 1.7115e-05 - val_dice_coef: 1.0000\n",
      "Epoch 39/150\n",
      "6735/6735 [==============================] - 52s 8ms/sample - loss: 5.3666e-08 - dice_coef: 1.0000 - val_loss: 1.2855e-06 - val_dice_coef: 1.0000\n",
      "Epoch 40/150\n",
      "6735/6735 [==============================] - 52s 8ms/sample - loss: 2.6904e-09 - dice_coef: 1.0000 - val_loss: 7.7642e-07 - val_dice_coef: 1.0000\n",
      "Epoch 41/150\n",
      "6735/6735 [==============================] - 52s 8ms/sample - loss: 1.1328e-09 - dice_coef: 1.0000 - val_loss: 3.3129e-07 - val_dice_coef: 1.0000\n",
      "Epoch 42/150\n",
      "6735/6735 [==============================] - 51s 8ms/sample - loss: 4.2480e-10 - dice_coef: 1.0000 - val_loss: 1.9566e-07 - val_dice_coef: 1.0000\n",
      "Epoch 43/150\n",
      "6735/6735 [==============================] - 52s 8ms/sample - loss: 0.0000e+00 - dice_coef: 1.0000 - val_loss: 3.3186e-07 - val_dice_coef: 1.0000\n",
      "Epoch 44/150\n",
      "6735/6735 [==============================] - 52s 8ms/sample - loss: 2.9736e-09 - dice_coef: 1.0000 - val_loss: 7.9850e-08 - val_dice_coef: 1.0000\n",
      "Epoch 45/150\n",
      "6735/6735 [==============================] - 52s 8ms/sample - loss: 8.4960e-10 - dice_coef: 1.0000 - val_loss: 1.3025e-07 - val_dice_coef: 1.0000\n",
      "Epoch 46/150\n",
      "6735/6735 [==============================] - 52s 8ms/sample - loss: 1.8408e-09 - dice_coef: 1.0000 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "Epoch 47/150\n",
      "6735/6735 [==============================] - 52s 8ms/sample - loss: 1.6992e-09 - dice_coef: 1.0000 - val_loss: 5.9463e-08 - val_dice_coef: 1.0000\n",
      "Epoch 48/150\n",
      "6735/6735 [==============================] - 52s 8ms/sample - loss: 4.2480e-10 - dice_coef: 1.0000 - val_loss: 6.7958e-09 - val_dice_coef: 1.0000\n",
      "Epoch 49/150\n",
      "6735/6735 [==============================] - 322s 48ms/sample - loss: 8.4960e-10 - dice_coef: 1.0000 - val_loss: 7.3621e-09 - val_dice_coef: 1.0000\n",
      "Epoch 50/150\n",
      "6735/6735 [==============================] - 50s 7ms/sample - loss: 2.8320e-10 - dice_coef: 1.0000 - val_loss: 1.6989e-09 - val_dice_coef: 1.0000\n",
      "Epoch 51/150\n",
      "6735/6735 [==============================] - 50s 7ms/sample - loss: 1.4160e-10 - dice_coef: 1.0000 - val_loss: 5.0968e-09 - val_dice_coef: 1.0000\n",
      "Epoch 52/150\n",
      "6735/6735 [==============================] - 52s 8ms/sample - loss: 8.4960e-10 - dice_coef: 1.0000 - val_loss: 5.6631e-10 - val_dice_coef: 1.0000\n",
      "Epoch 53/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6735/6735 [==============================] - 52s 8ms/sample - loss: 2.8320e-10 - dice_coef: 1.0000 - val_loss: 8.4947e-09 - val_dice_coef: 1.0000\n",
      "Epoch 54/150\n",
      "6735/6735 [==============================] - 52s 8ms/sample - loss: 1.4160e-10 - dice_coef: 1.0000 - val_loss: 4.5305e-09 - val_dice_coef: 1.0000\n",
      "Epoch 55/150\n",
      "6735/6735 [==============================] - 52s 8ms/sample - loss: 2.8320e-10 - dice_coef: 1.0000 - val_loss: 2.8316e-09 - val_dice_coef: 1.0000\n",
      "Epoch 56/150\n",
      "6735/6735 [==============================] - 52s 8ms/sample - loss: 0.0000e+00 - dice_coef: 1.0000 - val_loss: 2.2653e-09 - val_dice_coef: 1.0000\n",
      "Epoch 57/150\n",
      "6735/6735 [==============================] - 52s 8ms/sample - loss: 2.8320e-10 - dice_coef: 1.0000 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "Epoch 60/150\n",
      "6735/6735 [==============================] - 51s 8ms/sample - loss: 0.0000e+00 - dice_coef: 1.0000 - val_loss: 7.9284e-09 - val_dice_coef: 1.0000\n",
      "Epoch 61/150\n",
      "6735/6735 [==============================] - 52s 8ms/sample - loss: 2.8320e-10 - dice_coef: 1.0000 - val_loss: 1.6989e-09 - val_dice_coef: 1.0000\n",
      "Epoch 62/150\n",
      "6735/6735 [==============================] - 51s 8ms/sample - loss: 1.4160e-10 - dice_coef: 1.0000 - val_loss: 3.3979e-09 - val_dice_coef: 1.0000\n",
      "Epoch 63/150\n",
      "2672/6735 [==========>...................] - ETA: 29s - loss: 3.5691e-10 - dice_coef: 1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6735/6735 [==============================] - 52s 8ms/sample - loss: 1.4160e-10 - dice_coef: 1.0000 - val_loss: 5.6631e-10 - val_dice_coef: 1.0000\n",
      "Epoch 71/150\n",
      "6735/6735 [==============================] - 52s 8ms/sample - loss: 1.4160e-10 - dice_coef: 1.0000 - val_loss: 2.8316e-09 - val_dice_coef: 1.0000\n",
      "Epoch 72/150\n",
      "6735/6735 [==============================] - 51s 8ms/sample - loss: 4.2480e-10 - dice_coef: 1.0000 - val_loss: 1.1326e-09 - val_dice_coef: 1.0000\n",
      "Epoch 73/150\n",
      "6480/6735 [===========================>..] - ETA: 1s - loss: 0.0000e+00 - dice_coef: 1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6735/6735 [==============================] - 51s 8ms/sample - loss: 0.0000e+00 - dice_coef: 1.0000 - val_loss: 3.3979e-09 - val_dice_coef: 1.0000\n",
      "Epoch 81/150\n",
      "6735/6735 [==============================] - 51s 8ms/sample - loss: 0.0000e+00 - dice_coef: 1.0000 - val_loss: 1.6989e-09 - val_dice_coef: 1.0000\n",
      "Epoch 82/150\n",
      "6735/6735 [==============================] - 51s 8ms/sample - loss: 1.4160e-10 - dice_coef: 1.0000 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "Epoch 83/150\n",
      "6735/6735 [==============================] - 52s 8ms/sample - loss: 1.4160e-10 - dice_coef: 1.0000 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "Epoch 84/150\n",
      "3384/6735 [==============>...............] - ETA: 24s - loss: 0.0000e+00 - dice_coef: 1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6735/6735 [==============================] - 51s 8ms/sample - loss: 0.0000e+00 - dice_coef: 1.0000 - val_loss: 5.6631e-10 - val_dice_coef: 1.0000\n",
      "Epoch 92/150\n",
      "6735/6735 [==============================] - 51s 8ms/sample - loss: 0.0000e+00 - dice_coef: 1.0000 - val_loss: 1.1326e-09 - val_dice_coef: 1.0000\n",
      "Epoch 93/150\n",
      "6735/6735 [==============================] - 51s 8ms/sample - loss: 2.8320e-10 - dice_coef: 1.0000 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "Epoch 94/150\n",
      "6735/6735 [==============================] - 51s 8ms/sample - loss: 0.0000e+00 - dice_coef: 1.0000 - val_loss: 5.6631e-10 - val_dice_coef: 1.0000\n",
      "Epoch 95/150\n",
      "2040/6735 [========>.....................] - ETA: 33s - loss: 0.0000e+00 - dice_coef: 1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6735/6735 [==============================] - 51s 8ms/sample - loss: 1.4160e-10 - dice_coef: 1.0000 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "Epoch 103/150\n",
      "6735/6735 [==============================] - 51s 8ms/sample - loss: 1.4160e-10 - dice_coef: 1.0000 - val_loss: 5.6631e-10 - val_dice_coef: 1.0000\n",
      "Epoch 104/150\n",
      "6735/6735 [==============================] - 51s 8ms/sample - loss: 1.4160e-10 - dice_coef: 1.0000 - val_loss: 1.1326e-09 - val_dice_coef: 1.0000\n",
      "Epoch 105/150\n",
      "5408/6735 [=======================>......] - ETA: 9s - loss: 0.0000e+00 - dice_coef: 1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6735/6735 [==============================] - 51s 8ms/sample - loss: 0.0000e+00 - dice_coef: 1.0000 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "Epoch 114/150\n",
      "6735/6735 [==============================] - 51s 8ms/sample - loss: 0.0000e+00 - dice_coef: 1.0000 - val_loss: 5.6631e-10 - val_dice_coef: 1.0000\n",
      "Epoch 115/150\n",
      "6735/6735 [==============================] - 51s 8ms/sample - loss: 0.0000e+00 - dice_coef: 1.0000 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "Epoch 116/150\n",
      "4256/6735 [=================>............] - ETA: 17s - loss: 0.0000e+00 - dice_coef: 1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6735/6735 [==============================] - 51s 8ms/sample - loss: 0.0000e+00 - dice_coef: 1.0000 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "Epoch 124/150\n",
      "6735/6735 [==============================] - 51s 8ms/sample - loss: 0.0000e+00 - dice_coef: 1.0000 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "Epoch 125/150\n",
      "6735/6735 [==============================] - 51s 8ms/sample - loss: 0.0000e+00 - dice_coef: 1.0000 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "Epoch 126/150\n",
      "5344/6735 [======================>.......] - ETA: 9s - loss: 0.0000e+00 - dice_coef: 1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6735/6735 [==============================] - 50s 7ms/sample - loss: 1.4160e-10 - dice_coef: 1.0000 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "Epoch 135/150\n",
      "6735/6735 [==============================] - 50s 7ms/sample - loss: 0.0000e+00 - dice_coef: 1.0000 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "Epoch 136/150\n",
      "6735/6735 [==============================] - 50s 7ms/sample - loss: 0.0000e+00 - dice_coef: 1.0000 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "Epoch 137/150\n",
      "3920/6735 [================>.............] - ETA: 19s - loss: 0.0000e+00 - dice_coef: 1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6735/6735 [==============================] - 50s 7ms/sample - loss: 0.0000e+00 - dice_coef: 1.0000 - val_loss: 1.1326e-09 - val_dice_coef: 1.0000\n",
      "Epoch 145/150\n",
      "6735/6735 [==============================] - 51s 8ms/sample - loss: 1.4160e-10 - dice_coef: 1.0000 - val_loss: 1.1326e-09 - val_dice_coef: 1.0000\n",
      "Epoch 146/150\n",
      "6735/6735 [==============================] - 51s 8ms/sample - loss: 0.0000e+00 - dice_coef: 1.0000 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000\n",
      "Epoch 147/150\n",
      "5336/6735 [======================>.......] - ETA: 9s - loss: 0.0000e+00 - dice_coef: 1.0000 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "base = 16\n",
    "image_size = 256\n",
    "img_ch = 1\n",
    "batch_size =8\n",
    "LR = 0.0001\n",
    "SDRate = 0.5\n",
    "batch_normalization = True\n",
    "spatial_dropout = True\n",
    "metric = 'dice'\n",
    "epochs = 150\n",
    "p = 0.2\n",
    "path = '/Lab1/Lab3/CT/'\n",
    "fold1 = 'Image'\n",
    "fold2 = 'Mask'\n",
    "\n",
    "train_img, train_mask, test_img, test_mask = get_train_test_data(fold1, fold2, path, p,image_size, image_size)\n",
    "model = u_net(base,image_size, image_size, img_ch, batch_normalization, SDRate, spatial_dropout)\n",
    "\n",
    "model.compile(optimizer = Adam(lr=LR), loss = dice_coef_loss, metrics =[dice_coef] )\n",
    "History = model.fit(train_img, train_mask, epochs = epochs, batch_size = batch_size, verbose = 1,\n",
    "                    validation_data = (test_img,test_mask))\n",
    "\n",
    "plotter(History)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.metrics import Recall, Precision\n",
    "\n",
    "#Model parameters\n",
    "base = 16\n",
    "image_size = 256\n",
    "img_ch = 1\n",
    "batch_size =8\n",
    "LR = 0.0001\n",
    "SDRate = 0.5\n",
    "batch_normalization = True\n",
    "spatial_dropout = True\n",
    "metric = 'dice'\n",
    "epochs = 150\n",
    "\n",
    "#Data loader parameters\n",
    "p = 0.2\n",
    "path = '/Lab1/Lab3/CT/'\n",
    "fold1 = 'Image'\n",
    "fold2 = 'Mask'\n",
    "\n",
    "#Data augmentation parameters\n",
    "rotation_range = 10\n",
    "width_shift = 0.1\n",
    "height_shift_range = 0.1,\n",
    "rescale = 1./255\n",
    "horizontal_flip = True\n",
    "\n",
    "#Load the data\n",
    "train_img, train_mask, test_img, test_mask = get_train_test_data(fold1, fold2, path, p,image_size, image_size)\n",
    "\n",
    "#Data augmentation\n",
    "train_datagen, val_datagen = DataAugmentation(rotation_range,width_shift,height_shift_range,rescale,horizontal_flip)\n",
    "\n",
    "\n",
    "#Build the model\n",
    "model = u_net(base,image_size, image_size, img_ch, batch_normalization, SDRate, spatial_dropout)\n",
    "\n",
    "#Compile the model\n",
    "model.compile(optimizer = Adam(lr=LR), loss = dice_coef_loss, metrics =[dice_coef, Recall(), Precision()] )\n",
    "\n",
    "#Fit the data into the model\n",
    "History = model.fit_generator(train_datagen.flow(train_img, train_mask,batch_size = batch_size), validation_data = val_datagen.flow(test_img, test_mask), epochs = epochs, verbose = 1)        \n",
    "\n",
    "#Plot results\n",
    "#Training vs Validation Learning loss \n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.title(\"Learning curve\")\n",
    "plt.plot(History.history[\"loss\"], label=\"loss\")\n",
    "plt.plot(History.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot( np.argmin(History.history[\"val_loss\"]),\n",
    "         np.min(History.history[\"val_loss\"]),\n",
    "         marker=\"x\", color=\"r\", label=\"best model\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss Value\")\n",
    "plt.legend(); \n",
    "\n",
    "#Train and test accuracy plot\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.title(\"Dice Score Curve\")\n",
    "plt.plot(History.history[\"dice_coef\"], label=\"dice_coef\")\n",
    "plt.plot(History.history[\"val_dice_coef\"], label=\"val_dice_coef\")\n",
    "plt.plot(History.history[\"val_recall\"], label=\"val_recall\")\n",
    "plt.plot(History.history[\"val_precision\"], label=\"val_precision\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Dice Coef')\n",
    "plt.legend(); \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: 0/6735  of train images\n",
      "Reading: 50/6735  of train images\n",
      "Reading: 100/6735  of train images\n",
      "Reading: 150/6735  of train images\n",
      "Reading: 200/6735  of train images\n",
      "Reading: 250/6735  of train images\n",
      "Reading: 300/6735  of train images\n",
      "Reading: 350/6735  of train images\n",
      "Reading: 400/6735  of train images\n",
      "Reading: 450/6735  of train images\n",
      "Reading: 500/6735  of train images\n",
      "Reading: 550/6735  of train images\n",
      "Reading: 600/6735  of train images\n",
      "Reading: 650/6735  of train images\n",
      "Reading: 700/6735  of train images\n",
      "Reading: 750/6735  of train images\n",
      "Reading: 800/6735  of train images\n",
      "Reading: 850/6735  of train images\n",
      "Reading: 900/6735  of train images\n",
      "Reading: 950/6735  of train images\n",
      "Reading: 1000/6735  of train images\n",
      "Reading: 1050/6735  of train images\n",
      "Reading: 1100/6735  of train images\n",
      "Reading: 1150/6735  of train images\n",
      "Reading: 1200/6735  of train images\n",
      "Reading: 1250/6735  of train images\n",
      "Reading: 1300/6735  of train images\n",
      "Reading: 1350/6735  of train images\n",
      "Reading: 1400/6735  of train images\n",
      "Reading: 1450/6735  of train images\n",
      "Reading: 1500/6735  of train images\n",
      "Reading: 1550/6735  of train images\n",
      "Reading: 1600/6735  of train images\n",
      "Reading: 1650/6735  of train images\n",
      "Reading: 1700/6735  of train images\n",
      "Reading: 1750/6735  of train images\n",
      "Reading: 1800/6735  of train images\n",
      "Reading: 1850/6735  of train images\n",
      "Reading: 1900/6735  of train images\n",
      "Reading: 1950/6735  of train images\n",
      "Reading: 2000/6735  of train images\n",
      "Reading: 2050/6735  of train images\n",
      "Reading: 2100/6735  of train images\n",
      "Reading: 2150/6735  of train images\n",
      "Reading: 2200/6735  of train images\n",
      "Reading: 2250/6735  of train images\n",
      "Reading: 2300/6735  of train images\n",
      "Reading: 2350/6735  of train images\n",
      "Reading: 2400/6735  of train images\n",
      "Reading: 2450/6735  of train images\n",
      "Reading: 2500/6735  of train images\n",
      "Reading: 2550/6735  of train images\n",
      "Reading: 2600/6735  of train images\n",
      "Reading: 2650/6735  of train images\n",
      "Reading: 2700/6735  of train images\n",
      "Reading: 2750/6735  of train images\n",
      "Reading: 2800/6735  of train images\n",
      "Reading: 2850/6735  of train images\n",
      "Reading: 2900/6735  of train images\n",
      "Reading: 2950/6735  of train images\n",
      "Reading: 3000/6735  of train images\n",
      "Reading: 3050/6735  of train images\n",
      "Reading: 3100/6735  of train images\n",
      "Reading: 3150/6735  of train images\n",
      "Reading: 3200/6735  of train images\n",
      "Reading: 3250/6735  of train images\n",
      "Reading: 3300/6735  of train images\n",
      "Reading: 3350/6735  of train images\n",
      "Reading: 3400/6735  of train images\n",
      "Reading: 3450/6735  of train images\n",
      "Reading: 3500/6735  of train images\n",
      "Reading: 3550/6735  of train images\n",
      "Reading: 3600/6735  of train images\n",
      "Reading: 3650/6735  of train images\n",
      "Reading: 3700/6735  of train images\n",
      "Reading: 3750/6735  of train images\n",
      "Reading: 3800/6735  of train images\n",
      "Reading: 3850/6735  of train images\n",
      "Reading: 3900/6735  of train images\n",
      "Reading: 3950/6735  of train images\n",
      "Reading: 4000/6735  of train images\n",
      "Reading: 4050/6735  of train images\n",
      "Reading: 4100/6735  of train images\n",
      "Reading: 4150/6735  of train images\n",
      "Reading: 4200/6735  of train images\n",
      "Reading: 4250/6735  of train images\n",
      "Reading: 4300/6735  of train images\n",
      "Reading: 4350/6735  of train images\n",
      "Reading: 4400/6735  of train images\n",
      "Reading: 4450/6735  of train images\n",
      "Reading: 4500/6735  of train images\n",
      "Reading: 4550/6735  of train images\n",
      "Reading: 4600/6735  of train images\n",
      "Reading: 4650/6735  of train images\n",
      "Reading: 4700/6735  of train images\n",
      "Reading: 4750/6735  of train images\n",
      "Reading: 4800/6735  of train images\n",
      "Reading: 4850/6735  of train images\n",
      "Reading: 4900/6735  of train images\n",
      "Reading: 4950/6735  of train images\n",
      "Reading: 5000/6735  of train images\n",
      "Reading: 5050/6735  of train images\n",
      "Reading: 5100/6735  of train images\n",
      "Reading: 5150/6735  of train images\n",
      "Reading: 5200/6735  of train images\n",
      "Reading: 5250/6735  of train images\n",
      "Reading: 5300/6735  of train images\n",
      "Reading: 5350/6735  of train images\n",
      "Reading: 5400/6735  of train images\n",
      "Reading: 5450/6735  of train images\n",
      "Reading: 5500/6735  of train images\n",
      "Reading: 5550/6735  of train images\n",
      "Reading: 5600/6735  of train images\n",
      "Reading: 5650/6735  of train images\n",
      "Reading: 5700/6735  of train images\n",
      "Reading: 5750/6735  of train images\n",
      "Reading: 5800/6735  of train images\n",
      "Reading: 5850/6735  of train images\n",
      "Reading: 5900/6735  of train images\n",
      "Reading: 5950/6735  of train images\n",
      "Reading: 6000/6735  of train images\n",
      "Reading: 6050/6735  of train images\n",
      "Reading: 6100/6735  of train images\n",
      "Reading: 6150/6735  of train images\n",
      "Reading: 6200/6735  of train images\n",
      "Reading: 6250/6735  of train images\n",
      "Reading: 6300/6735  of train images\n",
      "Reading: 6350/6735  of train images\n",
      "Reading: 6400/6735  of train images\n",
      "Reading: 6450/6735  of train images\n",
      "Reading: 6500/6735  of train images\n",
      "Reading: 6550/6735  of train images\n",
      "Reading: 6600/6735  of train images\n",
      "Reading: 6650/6735  of train images\n",
      "Reading: 6700/6735  of train images\n",
      "Reading: 0/1684  of test images\n",
      "Reading: 50/1684  of test images\n",
      "Reading: 100/1684  of test images\n",
      "Reading: 150/1684  of test images\n",
      "Reading: 200/1684  of test images\n",
      "Reading: 250/1684  of test images\n",
      "Reading: 300/1684  of test images\n",
      "Reading: 350/1684  of test images\n",
      "Reading: 400/1684  of test images\n",
      "Reading: 450/1684  of test images\n",
      "Reading: 500/1684  of test images\n",
      "Reading: 550/1684  of test images\n",
      "Reading: 600/1684  of test images\n",
      "Reading: 650/1684  of test images\n",
      "Reading: 700/1684  of test images\n",
      "Reading: 750/1684  of test images\n",
      "Reading: 800/1684  of test images\n",
      "Reading: 850/1684  of test images\n",
      "Reading: 900/1684  of test images\n",
      "Reading: 950/1684  of test images\n",
      "Reading: 1000/1684  of test images\n",
      "Reading: 1050/1684  of test images\n",
      "Reading: 1100/1684  of test images\n",
      "Reading: 1150/1684  of test images\n",
      "Reading: 1200/1684  of test images\n",
      "Reading: 1250/1684  of test images\n",
      "Reading: 1300/1684  of test images\n",
      "Reading: 1350/1684  of test images\n",
      "Reading: 1400/1684  of test images\n",
      "Reading: 1450/1684  of test images\n",
      "Reading: 1500/1684  of test images\n",
      "Reading: 1550/1684  of test images\n",
      "Reading: 1600/1684  of test images\n",
      "Reading: 1650/1684  of test images\n",
      "(6735, 256, 256, 1)\n",
      "(1684, 256, 256, 1)\n",
      "building model\n",
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            [(None, 256, 256, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_108 (Conv2D)             (None, 256, 256, 16) 160         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_85 (Batc (None, 256, 256, 16) 64          conv2d_108[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_108 (Activation)     (None, 256, 256, 16) 0           batch_normalization_v2_85[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_96 (SpatialDr (None, 256, 256, 16) 0           activation_108[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_109 (Conv2D)             (None, 256, 256, 16) 2320        spatial_dropout2d_96[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_86 (Batc (None, 256, 256, 16) 64          conv2d_109[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_109 (Activation)     (None, 256, 256, 16) 0           batch_normalization_v2_86[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_97 (SpatialDr (None, 256, 256, 16) 0           activation_109[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_24 (MaxPooling2D) (None, 128, 128, 16) 0           spatial_dropout2d_97[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_110 (Conv2D)             (None, 128, 128, 32) 4640        max_pooling2d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_87 (Batc (None, 128, 128, 32) 128         conv2d_110[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_110 (Activation)     (None, 128, 128, 32) 0           batch_normalization_v2_87[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_98 (SpatialDr (None, 128, 128, 32) 0           activation_110[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_111 (Conv2D)             (None, 128, 128, 32) 9248        spatial_dropout2d_98[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_88 (Batc (None, 128, 128, 32) 128         conv2d_111[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_111 (Activation)     (None, 128, 128, 32) 0           batch_normalization_v2_88[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_99 (SpatialDr (None, 128, 128, 32) 0           activation_111[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_25 (MaxPooling2D) (None, 64, 64, 32)   0           spatial_dropout2d_99[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_112 (Conv2D)             (None, 64, 64, 64)   18496       max_pooling2d_25[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_89 (Batc (None, 64, 64, 64)   256         conv2d_112[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_112 (Activation)     (None, 64, 64, 64)   0           batch_normalization_v2_89[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_100 (SpatialD (None, 64, 64, 64)   0           activation_112[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_113 (Conv2D)             (None, 64, 64, 64)   36928       spatial_dropout2d_100[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_90 (Batc (None, 64, 64, 64)   256         conv2d_113[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_113 (Activation)     (None, 64, 64, 64)   0           batch_normalization_v2_90[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_101 (SpatialD (None, 64, 64, 64)   0           activation_113[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_26 (MaxPooling2D) (None, 32, 32, 64)   0           spatial_dropout2d_101[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_114 (Conv2D)             (None, 32, 32, 128)  73856       max_pooling2d_26[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_91 (Batc (None, 32, 32, 128)  512         conv2d_114[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_114 (Activation)     (None, 32, 32, 128)  0           batch_normalization_v2_91[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_102 (SpatialD (None, 32, 32, 128)  0           activation_114[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_115 (Conv2D)             (None, 32, 32, 128)  147584      spatial_dropout2d_102[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_92 (Batc (None, 32, 32, 128)  512         conv2d_115[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_115 (Activation)     (None, 32, 32, 128)  0           batch_normalization_v2_92[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_103 (SpatialD (None, 32, 32, 128)  0           activation_115[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_27 (MaxPooling2D) (None, 16, 16, 128)  0           spatial_dropout2d_103[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_116 (Conv2D)             (None, 16, 16, 256)  295168      max_pooling2d_27[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_93 (Batc (None, 16, 16, 256)  1024        conv2d_116[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_116 (Activation)     (None, 16, 16, 256)  0           batch_normalization_v2_93[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_104 (SpatialD (None, 16, 16, 256)  0           activation_116[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_24 (Conv2DTran (None, 32, 32, 128)  131200      spatial_dropout2d_104[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_24 (Concatenate)    (None, 32, 32, 256)  0           spatial_dropout2d_103[0][0]      \n",
      "                                                                 conv2d_transpose_24[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_117 (Conv2D)             (None, 32, 32, 128)  295040      concatenate_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_94 (Batc (None, 32, 32, 128)  512         conv2d_117[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_117 (Activation)     (None, 32, 32, 128)  0           batch_normalization_v2_94[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_105 (SpatialD (None, 32, 32, 128)  0           activation_117[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_118 (Conv2D)             (None, 32, 32, 128)  147584      spatial_dropout2d_105[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_95 (Batc (None, 32, 32, 128)  512         conv2d_118[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_118 (Activation)     (None, 32, 32, 128)  0           batch_normalization_v2_95[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_106 (SpatialD (None, 32, 32, 128)  0           activation_118[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_25 (Conv2DTran (None, 64, 64, 64)   32832       spatial_dropout2d_106[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_25 (Concatenate)    (None, 64, 64, 128)  0           spatial_dropout2d_101[0][0]      \n",
      "                                                                 conv2d_transpose_25[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_119 (Conv2D)             (None, 64, 64, 64)   73792       concatenate_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_96 (Batc (None, 64, 64, 64)   256         conv2d_119[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_119 (Activation)     (None, 64, 64, 64)   0           batch_normalization_v2_96[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_107 (SpatialD (None, 64, 64, 64)   0           activation_119[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_120 (Conv2D)             (None, 64, 64, 64)   36928       spatial_dropout2d_107[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_97 (Batc (None, 64, 64, 64)   256         conv2d_120[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_120 (Activation)     (None, 64, 64, 64)   0           batch_normalization_v2_97[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_108 (SpatialD (None, 64, 64, 64)   0           activation_120[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_26 (Conv2DTran (None, 128, 128, 32) 8224        spatial_dropout2d_108[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_26 (Concatenate)    (None, 128, 128, 64) 0           spatial_dropout2d_99[0][0]       \n",
      "                                                                 conv2d_transpose_26[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_121 (Conv2D)             (None, 128, 128, 32) 18464       concatenate_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_98 (Batc (None, 128, 128, 32) 128         conv2d_121[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_121 (Activation)     (None, 128, 128, 32) 0           batch_normalization_v2_98[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_109 (SpatialD (None, 128, 128, 32) 0           activation_121[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_122 (Conv2D)             (None, 128, 128, 32) 9248        spatial_dropout2d_109[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_99 (Batc (None, 128, 128, 32) 128         conv2d_122[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_122 (Activation)     (None, 128, 128, 32) 0           batch_normalization_v2_99[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_110 (SpatialD (None, 128, 128, 32) 0           activation_122[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_27 (Conv2DTran (None, 256, 256, 16) 2064        spatial_dropout2d_110[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_27 (Concatenate)    (None, 256, 256, 32) 0           spatial_dropout2d_97[0][0]       \n",
      "                                                                 conv2d_transpose_27[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_123 (Conv2D)             (None, 256, 256, 16) 4624        concatenate_27[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_100 (Bat (None, 256, 256, 16) 64          conv2d_123[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_123 (Activation)     (None, 256, 256, 16) 0           batch_normalization_v2_100[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_111 (SpatialD (None, 256, 256, 16) 0           activation_123[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_124 (Conv2D)             (None, 256, 256, 16) 2320        spatial_dropout2d_111[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_101 (Bat (None, 256, 256, 16) 64          conv2d_124[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_124 (Activation)     (None, 256, 256, 16) 0           batch_normalization_v2_101[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_125 (Conv2D)             (None, 256, 256, 3)  435         activation_124[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_125 (Activation)     (None, 256, 256, 3)  0           conv2d_125[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 1,356,019\n",
      "Trainable params: 1,353,587\n",
      "Non-trainable params: 2,432\n",
      "__________________________________________________________________________________________________\n",
      "here\n",
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "842/842 [==============================] - 106s 126ms/step - loss: -0.8692 - dice_coef: 0.8692 - recall_1: 0.9280 - precision_1: 0.9743 - val_loss: -0.9629 - val_dice_coef: 0.9629 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 2/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9927 - dice_coef: 0.9927 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -0.9874 - val_dice_coef: 0.9874 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 3/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9975 - dice_coef: 0.9975 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -0.9933 - val_dice_coef: 0.9933 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 4/150\n",
      "842/842 [==============================] - 56s 67ms/step - loss: -0.9987 - dice_coef: 0.9987 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -0.9963 - val_dice_coef: 0.9963 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 5/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9993 - dice_coef: 0.9993 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -0.9977 - val_dice_coef: 0.9977 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 6/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9995 - dice_coef: 0.9995 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -0.9984 - val_dice_coef: 0.9984 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 7/150\n",
      "842/842 [==============================] - 56s 67ms/step - loss: -0.9996 - dice_coef: 0.9996 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -0.9990 - val_dice_coef: 0.9990 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 8/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9997 - dice_coef: 0.9997 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -0.9993 - val_dice_coef: 0.9993 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 9/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -0.9995 - val_dice_coef: 0.9995 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 10/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -0.9997 - val_dice_coef: 0.9997 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 11/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -0.9998 - val_dice_coef: 0.9998 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 12/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -0.9999 - val_dice_coef: 0.9999 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 13/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -0.9999 - val_dice_coef: 0.9999 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 14/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -0.9999 - val_dice_coef: 0.9999 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 15/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 16/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 17/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 18/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 19/150\n",
      "842/842 [==============================] - 55s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 20/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 21/150\n",
      "842/842 [==============================] - 55s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 22/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 23/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 24/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 25/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 26/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 27/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 28/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 29/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 30/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 31/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 32/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 33/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 34/150\n",
      "842/842 [==============================] - 56s 67ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 35/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 36/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 37/150\n",
      "842/842 [==============================] - 56s 67ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 38/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 39/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 40/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 41/150\n",
      "842/842 [==============================] - 55s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 42/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 43/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 44/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 45/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 46/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 47/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 48/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 49/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 50/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 51/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 52/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 53/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 54/150\n",
      "842/842 [==============================] - 56s 67ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 55/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 56/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 57/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 58/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 59/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 60/150\n",
      "842/842 [==============================] - 56s 67ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 61/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 62/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 63/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 64/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 65/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 66/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 67/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 68/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 69/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 70/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 71/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 72/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 73/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 74/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 75/150\n",
      "842/842 [==============================] - 56s 67ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 76/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 77/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 78/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 79/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 80/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 81/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 82/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 83/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 84/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 85/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 86/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 87/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 88/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 89/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 90/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 91/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 92/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 93/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 94/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 95/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 96/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 97/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 98/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 99/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 100/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 101/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 102/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 103/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 104/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 105/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 106/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 107/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 108/150\n",
      "842/842 [==============================] - 56s 67ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 109/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 110/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 111/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 112/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 113/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 114/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 115/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 116/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 117/150\n",
      "842/842 [==============================] - 55s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 118/150\n",
      "842/842 [==============================] - 55s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 119/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 120/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 121/150\n",
      "842/842 [==============================] - 55s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 122/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 123/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 124/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 125/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 126/150\n",
      "842/842 [==============================] - 55s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 127/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 128/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 129/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 130/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 131/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 132/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 133/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 134/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 135/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 136/150\n",
      "842/842 [==============================] - 55s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 137/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 138/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 139/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 140/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 141/150\n",
      "842/842 [==============================] - 56s 67ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 142/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 143/150\n",
      "842/842 [==============================] - 55s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 144/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 145/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 146/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 147/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 148/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 149/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 150/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.metrics import Recall, Precision\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "#Model parameters\n",
    "base = 16\n",
    "image_size = 256\n",
    "img_ch = 1\n",
    "batch_size =8\n",
    "LR = 0.0001\n",
    "SDRate = 0.5\n",
    "batch_normalization = True\n",
    "spatial_dropout = True\n",
    "epochs = 150\n",
    "\n",
    "#Data loader parameters\n",
    "p = 0.2\n",
    "path = '/Lab1/Lab3/CT/'\n",
    "fold1 = 'Image'\n",
    "fold2 = 'Mask'\n",
    "\n",
    "#Data augmentation parameters\n",
    "rotation_range = 10\n",
    "width_shift = 0.1\n",
    "height_shift_range = 0.1,\n",
    "rescale = 1./255\n",
    "horizontal_flip = True\n",
    "\n",
    "#Load the data\n",
    "train_img, train_mask, test_img, test_mask = get_train_test_data(fold1, fold2, path, p,image_size, image_size)\n",
    "\n",
    "#To one-hot-encoding\n",
    "train_mask = to_categorical(train_mask, num_classes=3)\n",
    "test_mask = to_categorical(test_mask, num_classes=3)\n",
    "\n",
    "#Data augmentation\n",
    "train_datagen, val_datagen = DataAugmentation(rotation_range,width_shift,height_shift_range,rescale,horizontal_flip)\n",
    "\n",
    "#Build the multi-classification model\n",
    "model = u_net_3labels(base,image_size, image_size, img_ch, batch_normalization, SDRate, spatial_dropout)\n",
    "\n",
    "#Compile the model\n",
    "model.compile(optimizer = Adam(lr=LR), loss = dice_coef_loss, metrics =[dice_coef, Recall(), Precision()] )\n",
    "\n",
    "#Fit the data into the model\n",
    "History = model.fit_generator(train_datagen.flow(train_img, train_mask,batch_size = batch_size), validation_data = val_datagen.flow(test_img, test_mask), epochs = epochs, verbose = 1)          \n",
    "\n",
    "#Plot results\n",
    "dice = True\n",
    "recall = True\n",
    "precision = True\n",
    "plotter(History)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: 0/7522  of train images\n",
      "Reading: 50/7522  of train images\n",
      "Reading: 100/7522  of train images\n",
      "Reading: 150/7522  of train images\n",
      "Reading: 200/7522  of train images\n",
      "Reading: 250/7522  of train images\n",
      "Reading: 300/7522  of train images\n",
      "Reading: 350/7522  of train images\n",
      "Reading: 400/7522  of train images\n",
      "Reading: 450/7522  of train images\n",
      "Reading: 500/7522  of train images\n",
      "Reading: 550/7522  of train images\n",
      "Reading: 600/7522  of train images\n",
      "Reading: 650/7522  of train images\n",
      "Reading: 700/7522  of train images\n",
      "Reading: 750/7522  of train images\n",
      "Reading: 800/7522  of train images\n",
      "Reading: 850/7522  of train images\n",
      "Reading: 900/7522  of train images\n",
      "Reading: 950/7522  of train images\n",
      "Reading: 1000/7522  of train images\n",
      "Reading: 1050/7522  of train images\n",
      "Reading: 1100/7522  of train images\n",
      "Reading: 1150/7522  of train images\n",
      "Reading: 1200/7522  of train images\n",
      "Reading: 1250/7522  of train images\n",
      "Reading: 1300/7522  of train images\n",
      "Reading: 1350/7522  of train images\n",
      "Reading: 1400/7522  of train images\n",
      "Reading: 1450/7522  of train images\n",
      "Reading: 1500/7522  of train images\n",
      "Reading: 1550/7522  of train images\n",
      "Reading: 1600/7522  of train images\n",
      "Reading: 1650/7522  of train images\n",
      "Reading: 1700/7522  of train images\n",
      "Reading: 1750/7522  of train images\n",
      "Reading: 1800/7522  of train images\n",
      "Reading: 1850/7522  of train images\n",
      "Reading: 1900/7522  of train images\n",
      "Reading: 1950/7522  of train images\n",
      "Reading: 2000/7522  of train images\n",
      "Reading: 2050/7522  of train images\n",
      "Reading: 2100/7522  of train images\n",
      "Reading: 2150/7522  of train images\n",
      "Reading: 2200/7522  of train images\n",
      "Reading: 2250/7522  of train images\n",
      "Reading: 2300/7522  of train images\n",
      "Reading: 2350/7522  of train images\n",
      "Reading: 2400/7522  of train images\n",
      "Reading: 2450/7522  of train images\n",
      "Reading: 2500/7522  of train images\n",
      "Reading: 2550/7522  of train images\n",
      "Reading: 2600/7522  of train images\n",
      "Reading: 2650/7522  of train images\n",
      "Reading: 2700/7522  of train images\n",
      "Reading: 2750/7522  of train images\n",
      "Reading: 2800/7522  of train images\n",
      "Reading: 2850/7522  of train images\n",
      "Reading: 2900/7522  of train images\n",
      "Reading: 2950/7522  of train images\n",
      "Reading: 3000/7522  of train images\n",
      "Reading: 3050/7522  of train images\n",
      "Reading: 3100/7522  of train images\n",
      "Reading: 3150/7522  of train images\n",
      "Reading: 3200/7522  of train images\n",
      "Reading: 3250/7522  of train images\n",
      "Reading: 3300/7522  of train images\n",
      "Reading: 3350/7522  of train images\n",
      "Reading: 3400/7522  of train images\n",
      "Reading: 3450/7522  of train images\n",
      "Reading: 3500/7522  of train images\n",
      "Reading: 3550/7522  of train images\n",
      "Reading: 3600/7522  of train images\n",
      "Reading: 3650/7522  of train images\n",
      "Reading: 3700/7522  of train images\n",
      "Reading: 3750/7522  of train images\n",
      "Reading: 3800/7522  of train images\n",
      "Reading: 3850/7522  of train images\n",
      "Reading: 3900/7522  of train images\n",
      "Reading: 3950/7522  of train images\n",
      "Reading: 4000/7522  of train images\n",
      "Reading: 4050/7522  of train images\n",
      "Reading: 4100/7522  of train images\n",
      "Reading: 4150/7522  of train images\n",
      "Reading: 4200/7522  of train images\n",
      "Reading: 4250/7522  of train images\n",
      "Reading: 4300/7522  of train images\n",
      "Reading: 4350/7522  of train images\n",
      "Reading: 4400/7522  of train images\n",
      "Reading: 4450/7522  of train images\n",
      "Reading: 4500/7522  of train images\n",
      "Reading: 4550/7522  of train images\n",
      "Reading: 4600/7522  of train images\n",
      "Reading: 4650/7522  of train images\n",
      "Reading: 4700/7522  of train images\n",
      "Reading: 4750/7522  of train images\n",
      "Reading: 4800/7522  of train images\n",
      "Reading: 4850/7522  of train images\n",
      "Reading: 4900/7522  of train images\n",
      "Reading: 4950/7522  of train images\n",
      "Reading: 5000/7522  of train images\n",
      "Reading: 5050/7522  of train images\n",
      "Reading: 5100/7522  of train images\n",
      "Reading: 5150/7522  of train images\n",
      "Reading: 5200/7522  of train images\n",
      "Reading: 5250/7522  of train images\n",
      "Reading: 5300/7522  of train images\n",
      "Reading: 5350/7522  of train images\n",
      "Reading: 5400/7522  of train images\n",
      "Reading: 5450/7522  of train images\n",
      "Reading: 5500/7522  of train images\n",
      "Reading: 5550/7522  of train images\n",
      "Reading: 5600/7522  of train images\n",
      "Reading: 5650/7522  of train images\n",
      "Reading: 5700/7522  of train images\n",
      "Reading: 5750/7522  of train images\n",
      "Reading: 5800/7522  of train images\n",
      "Reading: 5850/7522  of train images\n",
      "Reading: 5900/7522  of train images\n",
      "Reading: 5950/7522  of train images\n",
      "Reading: 6000/7522  of train images\n",
      "Reading: 6050/7522  of train images\n",
      "Reading: 6100/7522  of train images\n",
      "Reading: 6150/7522  of train images\n",
      "Reading: 6200/7522  of train images\n",
      "Reading: 6250/7522  of train images\n",
      "Reading: 6300/7522  of train images\n",
      "Reading: 6350/7522  of train images\n",
      "Reading: 6400/7522  of train images\n",
      "Reading: 6450/7522  of train images\n",
      "Reading: 6500/7522  of train images\n",
      "Reading: 6550/7522  of train images\n",
      "Reading: 6600/7522  of train images\n",
      "Reading: 6650/7522  of train images\n",
      "Reading: 6700/7522  of train images\n",
      "Reading: 6750/7522  of train images\n",
      "Reading: 6800/7522  of train images\n",
      "Reading: 6850/7522  of train images\n",
      "Reading: 6900/7522  of train images\n",
      "Reading: 6950/7522  of train images\n",
      "Reading: 7000/7522  of train images\n",
      "Reading: 7050/7522  of train images\n",
      "Reading: 7100/7522  of train images\n",
      "Reading: 7150/7522  of train images\n",
      "Reading: 7200/7522  of train images\n",
      "Reading: 7250/7522  of train images\n",
      "Reading: 7300/7522  of train images\n",
      "Reading: 7350/7522  of train images\n",
      "Reading: 7400/7522  of train images\n",
      "Reading: 7450/7522  of train images\n",
      "Reading: 7500/7522  of train images\n",
      "Reading: 0/1881  of test images\n",
      "Reading: 50/1881  of test images\n",
      "Reading: 100/1881  of test images\n",
      "Reading: 150/1881  of test images\n",
      "Reading: 200/1881  of test images\n",
      "Reading: 250/1881  of test images\n",
      "Reading: 300/1881  of test images\n",
      "Reading: 350/1881  of test images\n",
      "Reading: 400/1881  of test images\n",
      "Reading: 450/1881  of test images\n",
      "Reading: 500/1881  of test images\n",
      "Reading: 550/1881  of test images\n",
      "Reading: 600/1881  of test images\n",
      "Reading: 650/1881  of test images\n",
      "Reading: 700/1881  of test images\n",
      "Reading: 750/1881  of test images\n",
      "Reading: 800/1881  of test images\n",
      "Reading: 850/1881  of test images\n",
      "Reading: 900/1881  of test images\n",
      "Reading: 950/1881  of test images\n",
      "Reading: 1000/1881  of test images\n",
      "Reading: 1050/1881  of test images\n",
      "Reading: 1100/1881  of test images\n",
      "Reading: 1150/1881  of test images\n",
      "Reading: 1200/1881  of test images\n",
      "Reading: 1250/1881  of test images\n",
      "Reading: 1300/1881  of test images\n",
      "Reading: 1350/1881  of test images\n",
      "Reading: 1400/1881  of test images\n",
      "Reading: 1450/1881  of test images\n",
      "Reading: 1500/1881  of test images\n",
      "Reading: 1550/1881  of test images\n",
      "Reading: 1600/1881  of test images\n",
      "Reading: 1650/1881  of test images\n",
      "Reading: 1700/1881  of test images\n",
      "Reading: 1750/1881  of test images\n",
      "Reading: 1800/1881  of test images\n",
      "Reading: 1850/1881  of test images\n",
      "(7522, 96, 96, 1)\n",
      "(1881, 96, 96, 1)\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 96, 96, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 96, 96, 16)   160         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_17 (Batc (None, 96, 96, 16)   64          conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 96, 96, 16)   0           batch_normalization_v2_17[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_16 (SpatialDr (None, 96, 96, 16)   0           activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 96, 96, 16)   2320        spatial_dropout2d_16[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_18 (Batc (None, 96, 96, 16)   64          conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 96, 96, 16)   0           batch_normalization_v2_18[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_17 (SpatialDr (None, 96, 96, 16)   0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 48, 48, 16)   0           spatial_dropout2d_17[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 48, 48, 32)   4640        max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_19 (Batc (None, 48, 48, 32)   128         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 48, 48, 32)   0           batch_normalization_v2_19[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_18 (SpatialDr (None, 48, 48, 32)   0           activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 48, 48, 32)   9248        spatial_dropout2d_18[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_20 (Batc (None, 48, 48, 32)   128         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 48, 48, 32)   0           batch_normalization_v2_20[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_19 (SpatialDr (None, 48, 48, 32)   0           activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 24, 24, 32)   0           spatial_dropout2d_19[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 24, 24, 64)   18496       max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_21 (Batc (None, 24, 24, 64)   256         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 24, 24, 64)   0           batch_normalization_v2_21[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_20 (SpatialDr (None, 24, 24, 64)   0           activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 24, 24, 64)   36928       spatial_dropout2d_20[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_22 (Batc (None, 24, 24, 64)   256         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 24, 24, 64)   0           batch_normalization_v2_22[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_21 (SpatialDr (None, 24, 24, 64)   0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 12, 12, 64)   0           spatial_dropout2d_21[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 12, 12, 128)  73856       max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_23 (Batc (None, 12, 12, 128)  512         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 12, 12, 128)  0           batch_normalization_v2_23[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_22 (SpatialDr (None, 12, 12, 128)  0           activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 12, 12, 128)  147584      spatial_dropout2d_22[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_24 (Batc (None, 12, 12, 128)  512         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 12, 12, 128)  0           batch_normalization_v2_24[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_23 (SpatialDr (None, 12, 12, 128)  0           activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 6, 6, 128)    0           spatial_dropout2d_23[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 6, 6, 256)    295168      max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_25 (Batc (None, 6, 6, 256)    1024        conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 6, 6, 256)    0           batch_normalization_v2_25[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_24 (SpatialDr (None, 6, 6, 256)    0           activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTrans (None, 12, 12, 128)  131200      spatial_dropout2d_24[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 12, 12, 256)  0           spatial_dropout2d_23[0][0]       \n",
      "                                                                 conv2d_transpose_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 12, 12, 128)  295040      concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_26 (Batc (None, 12, 12, 128)  512         conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 12, 12, 128)  0           batch_normalization_v2_26[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_25 (SpatialDr (None, 12, 12, 128)  0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 12, 12, 128)  147584      spatial_dropout2d_25[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_27 (Batc (None, 12, 12, 128)  512         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 12, 12, 128)  0           batch_normalization_v2_27[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_26 (SpatialDr (None, 12, 12, 128)  0           activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTrans (None, 24, 24, 64)   32832       spatial_dropout2d_26[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 24, 24, 128)  0           spatial_dropout2d_21[0][0]       \n",
      "                                                                 conv2d_transpose_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 24, 24, 64)   73792       concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_28 (Batc (None, 24, 24, 64)   256         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 24, 24, 64)   0           batch_normalization_v2_28[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_27 (SpatialDr (None, 24, 24, 64)   0           activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 24, 24, 64)   36928       spatial_dropout2d_27[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_29 (Batc (None, 24, 24, 64)   256         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 24, 24, 64)   0           batch_normalization_v2_29[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_28 (SpatialDr (None, 24, 24, 64)   0           activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_6 (Conv2DTrans (None, 48, 48, 32)   8224        spatial_dropout2d_28[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 48, 48, 64)   0           spatial_dropout2d_19[0][0]       \n",
      "                                                                 conv2d_transpose_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 48, 48, 32)   18464       concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_30 (Batc (None, 48, 48, 32)   128         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 48, 48, 32)   0           batch_normalization_v2_30[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_29 (SpatialDr (None, 48, 48, 32)   0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 48, 48, 32)   9248        spatial_dropout2d_29[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_31 (Batc (None, 48, 48, 32)   128         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 48, 48, 32)   0           batch_normalization_v2_31[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_30 (SpatialDr (None, 48, 48, 32)   0           activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_7 (Conv2DTrans (None, 96, 96, 16)   2064        spatial_dropout2d_30[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 96, 96, 32)   0           spatial_dropout2d_17[0][0]       \n",
      "                                                                 conv2d_transpose_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 96, 96, 16)   4624        concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_32 (Batc (None, 96, 96, 16)   64          conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 96, 96, 16)   0           batch_normalization_v2_32[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_31 (SpatialDr (None, 96, 96, 16)   0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 96, 96, 16)   2320        spatial_dropout2d_31[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_33 (Batc (None, 96, 96, 16)   64          conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 96, 96, 16)   0           batch_normalization_v2_33[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 96, 96, 1)    145         activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 96, 96, 1)    0           conv2d_35[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,355,729\n",
      "Trainable params: 1,353,297\n",
      "Non-trainable params: 2,432\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "1881/1881 [==============================] - 73s 39ms/step - loss: 0.3547 - dice_coef: 0.6453 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.4733 - val_dice_coef: 0.5267 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 2/80\n",
      "1881/1881 [==============================] - 50s 27ms/step - loss: 0.0023 - dice_coef: 0.9977 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.1254 - val_dice_coef: 0.8746 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 3/80\n",
      "1881/1881 [==============================] - 51s 27ms/step - loss: 5.0770e-04 - dice_coef: 0.9995 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0496 - val_dice_coef: 0.9504 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 4/80\n",
      "1881/1881 [==============================] - 51s 27ms/step - loss: 1.5493e-04 - dice_coef: 0.9998 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0262 - val_dice_coef: 0.9738 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 5/80\n",
      "1881/1881 [==============================] - 50s 27ms/step - loss: 5.4410e-05 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0109 - val_dice_coef: 0.9891 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 6/80\n",
      "1881/1881 [==============================] - 50s 27ms/step - loss: 1.7891e-05 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0064 - val_dice_coef: 0.9936 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 7/80\n",
      "1881/1881 [==============================] - 51s 27ms/step - loss: 5.7531e-06 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0015 - val_dice_coef: 0.9985 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 8/80\n",
      "1881/1881 [==============================] - 51s 27ms/step - loss: 2.0645e-06 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 6.1041e-04 - val_dice_coef: 0.9994 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 9/80\n",
      "1881/1881 [==============================] - 51s 27ms/step - loss: 9.0164e-07 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 1.5917e-04 - val_dice_coef: 0.9998 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 10/80\n",
      "1881/1881 [==============================] - 50s 27ms/step - loss: 3.5268e-07 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 7.1115e-05 - val_dice_coef: 0.9999 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 11/80\n",
      "1881/1881 [==============================] - 51s 27ms/step - loss: 1.2396e-07 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 3.5292e-05 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 12/80\n",
      "1881/1881 [==============================] - 51s 27ms/step - loss: 5.3933e-08 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 13/80\n",
      "1881/1881 [==============================] - 51s 27ms/step - loss: 2.8646e-08 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 14/80\n",
      "1881/1881 [==============================] - 50s 27ms/step - loss: 9.7598e-09 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 15/80\n",
      "1881/1881 [==============================] - 51s 27ms/step - loss: 4.7532e-09 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 16/80\n",
      "1881/1881 [==============================] - 51s 27ms/step - loss: 2.4083e-09 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 17/80\n",
      "1881/1881 [==============================] - 51s 27ms/step - loss: 1.9013e-09 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 18/80\n",
      "1881/1881 [==============================] - 51s 27ms/step - loss: 5.9573e-09 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 19/80\n",
      "1881/1881 [==============================] - 50s 27ms/step - loss: 1.3943e-09 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 20/80\n",
      "1881/1881 [==============================] - 50s 27ms/step - loss: 4.4363e-10 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 21/80\n",
      "1881/1881 [==============================] - 50s 27ms/step - loss: 4.4363e-10 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 22/80\n",
      "1881/1881 [==============================] - 51s 27ms/step - loss: 6.3375e-11 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 23/80\n",
      "1881/1881 [==============================] - 51s 27ms/step - loss: 0.0000e+00 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 24/80\n",
      "1881/1881 [==============================] - 51s 27ms/step - loss: 3.1688e-10 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 25/80\n",
      "1881/1881 [==============================] - 51s 27ms/step - loss: 0.0000e+00 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 26/80\n",
      "1881/1881 [==============================] - 51s 27ms/step - loss: 2.5350e-10 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 27/80\n",
      "1881/1881 [==============================] - 51s 27ms/step - loss: 3.1688e-10 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 28/80\n",
      "1881/1881 [==============================] - 51s 27ms/step - loss: 1.9013e-10 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 29/80\n",
      "1881/1881 [==============================] - 51s 27ms/step - loss: 6.3375e-11 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 30/80\n",
      "1881/1881 [==============================] - 51s 27ms/step - loss: 1.9013e-10 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 31/80\n",
      "1881/1881 [==============================] - 51s 27ms/step - loss: 2.5350e-10 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 32/80\n",
      "1881/1881 [==============================] - 51s 27ms/step - loss: 2.5350e-10 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 33/80\n",
      "1881/1881 [==============================] - 51s 27ms/step - loss: 6.3375e-11 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 34/80\n",
      "1881/1881 [==============================] - 51s 27ms/step - loss: 2.5350e-10 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 35/80\n",
      "1881/1881 [==============================] - 50s 27ms/step - loss: 1.2675e-10 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 36/80\n",
      "1881/1881 [==============================] - 48s 26ms/step - loss: 6.3375e-11 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 37/80\n",
      "1881/1881 [==============================] - 48s 26ms/step - loss: 1.9013e-10 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 38/80\n",
      "1881/1881 [==============================] - 48s 26ms/step - loss: 0.0000e+00 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 39/80\n",
      "1881/1881 [==============================] - 48s 26ms/step - loss: 0.0000e+00 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 40/80\n",
      "1881/1881 [==============================] - 48s 26ms/step - loss: 1.2675e-10 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 41/80\n",
      "1881/1881 [==============================] - 48s 26ms/step - loss: 0.0000e+00 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 42/80\n",
      "1881/1881 [==============================] - 48s 26ms/step - loss: 0.0000e+00 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 43/80\n",
      "1881/1881 [==============================] - 48s 25ms/step - loss: 0.0000e+00 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 44/80\n",
      "1881/1881 [==============================] - 48s 26ms/step - loss: 1.2675e-10 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 45/80\n",
      "1881/1881 [==============================] - 48s 26ms/step - loss: 0.0000e+00 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 46/80\n",
      "1881/1881 [==============================] - 49s 26ms/step - loss: 1.2675e-10 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 47/80\n",
      "1881/1881 [==============================] - 49s 26ms/step - loss: 2.5350e-10 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 48/80\n",
      "1881/1881 [==============================] - 48s 26ms/step - loss: 0.0000e+00 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 49/80\n",
      "1881/1881 [==============================] - 49s 26ms/step - loss: 1.2675e-10 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 50/80\n",
      "1881/1881 [==============================] - 49s 26ms/step - loss: 1.2675e-10 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 51/80\n",
      "1881/1881 [==============================] - 48s 26ms/step - loss: 0.0000e+00 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 52/80\n",
      "1881/1881 [==============================] - 48s 26ms/step - loss: 1.2675e-10 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 53/80\n",
      "1881/1881 [==============================] - 48s 26ms/step - loss: 1.9013e-10 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 54/80\n",
      "1881/1881 [==============================] - 48s 26ms/step - loss: 1.9013e-10 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 55/80\n",
      "1881/1881 [==============================] - 48s 26ms/step - loss: 0.0000e+00 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 56/80\n",
      "1881/1881 [==============================] - 48s 26ms/step - loss: 0.0000e+00 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 57/80\n",
      "1881/1881 [==============================] - 48s 26ms/step - loss: 0.0000e+00 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 58/80\n",
      "1881/1881 [==============================] - 48s 25ms/step - loss: 0.0000e+00 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 59/80\n",
      "1881/1881 [==============================] - 49s 26ms/step - loss: 0.0000e+00 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 60/80\n",
      "1881/1881 [==============================] - 48s 26ms/step - loss: 6.3375e-11 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 61/80\n",
      "1881/1881 [==============================] - 48s 26ms/step - loss: 0.0000e+00 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 62/80\n",
      "1881/1881 [==============================] - 48s 26ms/step - loss: 0.0000e+00 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 63/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1881/1881 [==============================] - 49s 26ms/step - loss: 6.3375e-11 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 64/80\n",
      "1881/1881 [==============================] - 48s 26ms/step - loss: 6.3375e-11 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 65/80\n",
      "1881/1881 [==============================] - 48s 26ms/step - loss: 6.3375e-11 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 66/80\n",
      " 816/1881 [============>.................] - ETA: 26s - loss: 0.0000e+00 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.metrics import Recall, Precision\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "#Model parameters\n",
    "base = 16\n",
    "image_size = 96\n",
    "img_ch = 1\n",
    "batch_size =4\n",
    "LR = 0.0001\n",
    "SDRate = 0.5\n",
    "batch_normalization = True\n",
    "spatial_dropout = True\n",
    "epochs = 80\n",
    "\n",
    "#Data loader parameters\n",
    "p = 0.2\n",
    "path = '/Lab1/Lab3/MRI/'\n",
    "fold1 = 'Image'\n",
    "fold2 = 'Mask'\n",
    "\n",
    "#Data augmentation parameters\n",
    "rotation_range = 10\n",
    "width_shift = 0.1\n",
    "height_shift_range = 0.1,\n",
    "rescale = 1./255\n",
    "horizontal_flip = True\n",
    "\n",
    "#Load the data\n",
    "train_img, train_mask, test_img, test_mask = get_train_test_data(fold1, fold2, path, p,image_size, image_size)\n",
    "\n",
    "#Data augmentation\n",
    "train_datagen, val_datagen = DataAugmentation(rotation_range,width_shift,height_shift_range,rescale,horizontal_flip)\n",
    "\n",
    "#Build the multi-classification model\n",
    "model = u_net(base,image_size, image_size, img_ch, batch_normalization, SDRate, spatial_dropout)\n",
    "\n",
    "#Compile the model\n",
    "model.compile(optimizer = Adam(lr=LR), loss = dice_coef_loss, metrics =[dice_coef, Recall(), Precision()] )\n",
    "\n",
    "#Fit the data into the model\n",
    "History = model.fit_generator(train_datagen.flow(train_img, train_mask,batch_size = batch_size), validation_data = val_datagen.flow(test_img, test_mask), epochs = epochs, verbose = 1)          \n",
    "\n",
    "#Plot results\n",
    "dice = True\n",
    "recall = True\n",
    "precision = True\n",
    "plotter(History)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##image loader for 3D\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "def path_loader(fold1, fold2, data_path):\n",
    "    #Creating data path\n",
    "    image_data_path = os.path.join(data_path, fold1)   \n",
    "    mask_data_path = os.path.join(data_path, fold2)\n",
    "    images = []\n",
    "    masks = []\n",
    "    #Listing all file names in the path\n",
    "    for root, dirs, files in os.walk(image_data_path):\n",
    "        for name in files:\n",
    "            images.append(os.path.join(image_data_path,name))\n",
    "    for root2, dirs2, files2 in os.walk(mask_data_path):\n",
    "        for name2 in files2:\n",
    "            masks.append(os.path.join(mask_data_path,name2))\n",
    "    return images, masks\n",
    "    \n",
    "\n",
    "# reading and resizing the training images with their corresponding labels\n",
    "def get_train_data_shuffled(images, masks, p):\n",
    "    \n",
    "    c = list(zip(images, masks))\n",
    "\n",
    "    shuffle(c)\n",
    "\n",
    "    images, masks = zip(*c)\n",
    "    \n",
    "    train_x, test_x, train_y, test_y = train_test_split(images,masks,test_size = p)\n",
    "\n",
    "    return train_x, test_x, train_y, test_y \n",
    "\n",
    "def data_loader(fold1, fold2, data_path, p,img_h, img_w, img_d):\n",
    "    \n",
    "    images, masks = path_loader(fold1, fold2, data_path)\n",
    "    train_x, test_x, train_y, test_y = get_train_data_shuffled(images, masks, p)\n",
    "    \n",
    "    train_img = []\n",
    "    train_mask = []\n",
    "    test_img = []\n",
    "    test_mask = []\n",
    "  \n",
    "    for i in range(len(train_x)):\n",
    "        image_name = train_x[i]\n",
    "        img = imread(image_name, as_grey=True)\n",
    "        img = resize(img, (img_h, img_w, img_d), anti_aliasing = True).astype('float32')\n",
    "        train_img.append([np.array(img)]) \n",
    "\n",
    "        if i % 50 == 0:\n",
    "             print('Reading: {0}/{1}  of train images'.format(i, len(train_x)))\n",
    "    for j in range(len(train_y)):\n",
    "        mask_name = train_y[j]\n",
    "        mask = imread(mask_name, as_grey=True)\n",
    "        mask = resize(img, (img_h, img_w, img_d), anti_aliasing = True).astype('float32')\n",
    "        train_mask.append([np.array(mask)])\n",
    "        \n",
    "    for i in range(len(test_x)):\n",
    "        image_name = test_x[i]\n",
    "        img = imread(image_name, as_grey=True)\n",
    "        img = resize(img, (img_h, img_w, img_d), anti_aliasing = True).astype('float32')\n",
    "        test_img.append([np.array(img)]) \n",
    "\n",
    "        if i % 50 == 0:\n",
    "             print('Reading: {0}/{1}  of test images'.format(i, len(test_x)))\n",
    "                \n",
    "    for j in range(len(test_y)):\n",
    "        mask_name = test_y[j]\n",
    "        mask = imread(mask_name, as_grey=True)\n",
    "        mask = resize(img, (img_h, img_w,img_d), anti_aliasing = True).astype('float32')\n",
    "        test_mask.append([np.array(mask)])        \n",
    "    print('finish')\n",
    " \n",
    "    return train_img, train_mask, test_img, test_mask\n",
    "\n",
    "#preprocessing\n",
    "def normalize(image):\n",
    "    MIN_BOUND = -1000.0\n",
    "    MAX_BOUND = 500.0\n",
    "    image = (image - MIN_BOUND) / (MAX_BOUND - MIN_BOUND)\n",
    "    print(image.shape)\n",
    "    image[image>1] = 1.\n",
    "    image[image<0] = 0.\n",
    "    return image\n",
    "\n",
    "# Instantiating images and labels for the model.\n",
    "def get_train_test_data(fold1, fold2, data_path, p,img_h, img_w, img_d):\n",
    "    \n",
    "    train_img, train_mask, test_img, test_mask = data_loader(fold1, fold2, data_path, p,img_h, img_w, img_d)\n",
    "    print(len(train_img))\n",
    "    Train_Img = np.zeros((len(train_img), img_h, img_w, img_d), dtype = np.float32)\n",
    "    Test_Img = np.zeros((len(test_img), img_h, img_w, img_d), dtype = np.float32)\n",
    "\n",
    "    Train_Label = np.zeros((len(train_mask),img_h, img_w, img_d), dtype = np.int32)\n",
    "    Test_Label = np.zeros((len(test_mask),img_h, img_w, img_d), dtype = np.int32)\n",
    "    print('get train')\n",
    "    for i in range(len(train_img)):\n",
    "        print(i)\n",
    "        Train_Img[i] = normalize(train_img[i][0])\n",
    "        Train_Label[i] = normalize(train_mask[i][0])\n",
    "    print(Train.Img.shape)\n",
    "    \n",
    "    Train_Img = np.expand_dims(Train_Img, axis = 4)  \n",
    "    Train_Label = np.expand_dims(Train_Label, axis = 4) \n",
    "\n",
    "    for j in range(len(test_img)):\n",
    "        Test_Img[j] = normalize(test_img[j][0])\n",
    "        Test_Label[j] = normalize(test_mask[j][0])\n",
    "\n",
    "    Test_Img = np.expand_dims(Test_Img, axis = 4)\n",
    "    Test_Label = np.expand_dims(Test_Label, axis = 4)\n",
    "   \n",
    "\n",
    "    return Train_Img, Train_Label, Test_Img, Test_Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.layers import Input, Convolution3D, MaxPooling3D, Conv3DTranspose\n",
    "from tensorflow.keras.layers import Reshape, Activation\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "\n",
    "def u_net3D(img_height, img_width, img_ch, img_d, batchNormalization, k_size=3, Base):\n",
    "    \n",
    "    merge_axis = -1 # Feature maps are concatenated along last axis (for tf backend)\n",
    "    data = Input((img_height, img_width, img_d, img_ch))\n",
    "    \n",
    "    #1ConvBlock\n",
    "    conv1 = Convolution3D(padding='same', filters=Base, kernel_size=k_size)(data)\n",
    "    if batchNormalization:\n",
    "        conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Activation('relu')(conv1)\n",
    "    \n",
    "    conv2 = Convolution3D(padding='same', filters=Base, kernel_size=k_size)(conv1)\n",
    "    if batchNormalization:\n",
    "        conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Activation('relu')(conv2)\n",
    "    \n",
    "    pool1 = MaxPooling3D(pool_size=(2, 2, 2))(conv2)\n",
    "    \n",
    "    #2ConvBlock\n",
    "    conv3 = Convolution3D(padding='same', filters=Base*2, kernel_size=k_size)(pool1)\n",
    "    if batchNormalization:\n",
    "        conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Activation('relu')(conv3)\n",
    "   \n",
    "    conv4 = Convolution3D(padding='same', filters=Base*2, kernel_size=k_size)(conv3)\n",
    "    if batchNormalization:\n",
    "        conv4 = BatchNormalization()(conv4)\n",
    "    conv4 = Activation('relu')(conv4)\n",
    "    \n",
    "    pool2 = MaxPooling3D(pool_size=(2, 2, 2))(conv4)\n",
    "    \n",
    "    #3ConvBlock\n",
    "    conv5 = Convolution3D(padding='same', filters=Base*4, kernel_size=k_size)(pool2)\n",
    "    if batchNormalization:\n",
    "        conv5 = BatchNormalization()(conv5)\n",
    "    conv5 = Activation('relu')(conv5)\n",
    "    \n",
    "    conv6 = Convolution3D(padding='same', filters=Base*4, kernel_size=k_size)(conv5)\n",
    "    if batchNormalization:\n",
    "        conv6 = BatchNormalization()(conv6)\n",
    "    conv6 = Activation('relu')(conv6)\n",
    "    \n",
    "    pool3 = MaxPooling3D(pool_size=(2, 2, 2))(conv6)\n",
    "    \n",
    "    #4ConvBlock\n",
    "    conv7 = Convolution3D(padding='same', filters=Base*8, kernel_size=k_size)(pool3)\n",
    "    if batchNormalization:\n",
    "        conv7 = BatchNormalization()(conv7)\n",
    "    conv7 = Activation('relu')(conv7)\n",
    "    \n",
    "    conv8 = Convolution3D(padding='same', filters=Base*8, kernel_size=k_size)(conv7)\n",
    "    if batchNormalization:\n",
    "        conv8 = BatchNormalization()(conv8)\n",
    "    conv8 = Activation('relu')(conv8)\n",
    "    \n",
    "    pool4 = MaxPooling3D(pool_size=(2, 2, 2))(conv8)\n",
    "    \n",
    "    #Bottleneck\n",
    "    conv9 = Convolution3D(padding='same', filters=Base*16, kernel_size=k_size)(pool4)\n",
    "    if batchNormalization:\n",
    "        conv9 = BatchNormalization()(conv9)\n",
    "    conv9 = Activation('relu')(conv9)\n",
    "    \n",
    "    #Expansion\n",
    "    #1ConvBlock\n",
    "    up1 = Conv3DTranspose(filters= Base*8, kernel_size=(2, 2, 2),padding = 'same')(conv9)\n",
    "    merged1 = concatenate([up1, conv8], axis=merge_axis)\n",
    "    conv10 = Convolution3D(padding='same', filters=Base*8, kernel_size=k_size)(merged1)\n",
    "    if batchNormalization:\n",
    "        conv10 = BatchNormalization()(conv10)\n",
    "    conv10 = Activation('relu')(conv10)\n",
    "    \n",
    "    conv11 = Convolution3D(padding='same', filters=Base*8, kernel_size=k_size)(conv10)\n",
    "    if batchNormalization:\n",
    "        conv11 = BatchNormalization()(conv11)\n",
    "    conv11 = Activation('relu')(conv11)\n",
    "    \n",
    "    #2ConvBlock\n",
    "    up2 = Conv3DTranspose(filters = Base*4, kernel_size=(2, 2, 2),padding = 'same')(conv11)\n",
    "    merged2 = concatenate([up2, conv6], axis=merge_axis)\n",
    "    conv12 = Convolution3D(padding='same', filters=Base*4, kernel_size=k_size)(merged2)\n",
    "    if batchNormalization:\n",
    "        conv12 = BatchNormalization()(conv12)\n",
    "    conv12 = Activation('relu')(conv12)\n",
    "\n",
    "    conv13 = Convolution3D(padding='same', filters=Base*4, kernel_size=k_size)(conv12)\n",
    "    if batchNormalization:\n",
    "        conv13 = BatchNormalization()(conv13)\n",
    "    conv13 = Activation('relu')(conv13)\n",
    "    \n",
    "    #3ConvBlock\n",
    "    up3 = Conv3DTranspose(filters = Base*2, kernel_size=(2, 2, 2),padding='same')(conv13)\n",
    "    merged3 = concatenate([up3, conv4], axis=merge_axis)\n",
    "    conv14 = Convolution3D(padding='same', filters=Base*2, kernel_size=k_size)(merged3)\n",
    "    if batchNormalization:\n",
    "        conv14 = BatchNormalization()(conv14)\n",
    "    conv14 = Activation('relu')(conv14)\n",
    "    \n",
    "    conv15 = Convolution3D(padding='same', filters=64, kernel_size=k_size)(conv14)\n",
    "    if batchNormalization:\n",
    "        conv15 = BatchNormalization()(conv15)\n",
    "    conv15 = Activation('relu')(conv15)\n",
    "\n",
    "    #4ConvBlock\n",
    "    up4 = Conv3DTranspose(filters= Base,kernel_size=(2, 2, 2),padding ='same')(conv15)\n",
    "    merged4 = concatenate([up4, conv2], axis=merge_axis)\n",
    "    conv16 = Convolution3D(padding='same', filters=Base, kernel_size=k_size)(merged4)\n",
    "    if batchNormalization:\n",
    "        conv16 = BatchNormalization()(conv16)\n",
    "    conv16 = Activation('relu')(conv16)\n",
    "    \n",
    "    conv17 = Convolution3D(padding='same', filters=Base, kernel_size=k_size)(conv16)\n",
    "    if batchNormalization:\n",
    "        conv17 = BatchNormalization()(conv17)\n",
    "    conv17 = Activation('relu')(conv17)\n",
    "    \n",
    "    #final layer\n",
    "    conv18 = Convolution3D(padding='same', filters=3, kernel_size=k_size)(merged3)\n",
    "    if batchNormalization:\n",
    "        conv18 = BatchNormalization()(conv18)\n",
    "    output = Reshape([-1, 2])(conv18)\n",
    "    output = Activation('softmax')(output)\n",
    "    output = Reshape(inp_shape[:-1] + (2,))(output)\n",
    "\n",
    "    model = Model(data, output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: 0/6735  of train images\n",
      "Reading: 50/6735  of train images\n",
      "Reading: 100/6735  of train images\n",
      "Reading: 150/6735  of train images\n",
      "Reading: 200/6735  of train images\n",
      "Reading: 250/6735  of train images\n",
      "Reading: 300/6735  of train images\n",
      "Reading: 350/6735  of train images\n",
      "Reading: 400/6735  of train images\n",
      "Reading: 450/6735  of train images\n",
      "Reading: 500/6735  of train images\n",
      "Reading: 550/6735  of train images\n",
      "Reading: 600/6735  of train images\n",
      "Reading: 650/6735  of train images\n",
      "Reading: 700/6735  of train images\n",
      "Reading: 750/6735  of train images\n",
      "Reading: 800/6735  of train images\n",
      "Reading: 850/6735  of train images\n",
      "Reading: 900/6735  of train images\n",
      "Reading: 950/6735  of train images\n",
      "Reading: 1000/6735  of train images\n",
      "Reading: 1050/6735  of train images\n",
      "Reading: 1100/6735  of train images\n",
      "Reading: 1150/6735  of train images\n",
      "Reading: 1200/6735  of train images\n",
      "Reading: 1250/6735  of train images\n",
      "Reading: 1300/6735  of train images\n",
      "Reading: 1350/6735  of train images\n",
      "Reading: 1400/6735  of train images\n",
      "Reading: 1450/6735  of train images\n",
      "Reading: 1500/6735  of train images\n",
      "Reading: 1550/6735  of train images\n",
      "Reading: 1600/6735  of train images\n",
      "Reading: 1650/6735  of train images\n",
      "Reading: 1700/6735  of train images\n",
      "Reading: 1750/6735  of train images\n",
      "Reading: 1800/6735  of train images\n",
      "Reading: 1850/6735  of train images\n",
      "Reading: 1900/6735  of train images\n",
      "Reading: 1950/6735  of train images\n",
      "Reading: 2000/6735  of train images\n",
      "Reading: 2050/6735  of train images\n",
      "Reading: 2100/6735  of train images\n",
      "Reading: 2150/6735  of train images\n",
      "Reading: 2200/6735  of train images\n",
      "Reading: 2250/6735  of train images\n",
      "Reading: 2300/6735  of train images\n",
      "Reading: 2350/6735  of train images\n",
      "Reading: 2400/6735  of train images\n",
      "Reading: 2450/6735  of train images\n",
      "Reading: 2500/6735  of train images\n",
      "Reading: 2550/6735  of train images\n",
      "Reading: 2600/6735  of train images\n",
      "Reading: 2650/6735  of train images\n",
      "Reading: 2700/6735  of train images\n",
      "Reading: 2750/6735  of train images\n",
      "Reading: 2800/6735  of train images\n",
      "Reading: 2850/6735  of train images\n",
      "Reading: 2900/6735  of train images\n",
      "Reading: 2950/6735  of train images\n",
      "Reading: 3000/6735  of train images\n",
      "Reading: 3050/6735  of train images\n",
      "Reading: 3100/6735  of train images\n",
      "Reading: 3150/6735  of train images\n",
      "Reading: 3200/6735  of train images\n",
      "Reading: 3250/6735  of train images\n",
      "Reading: 3300/6735  of train images\n",
      "Reading: 3350/6735  of train images\n",
      "Reading: 3400/6735  of train images\n",
      "Reading: 3450/6735  of train images\n",
      "Reading: 3500/6735  of train images\n",
      "Reading: 3550/6735  of train images\n",
      "Reading: 3600/6735  of train images\n",
      "Reading: 3650/6735  of train images\n",
      "Reading: 3700/6735  of train images\n",
      "Reading: 3750/6735  of train images\n",
      "Reading: 3800/6735  of train images\n",
      "Reading: 3850/6735  of train images\n",
      "Reading: 3900/6735  of train images\n",
      "Reading: 3950/6735  of train images\n",
      "Reading: 4000/6735  of train images\n",
      "Reading: 4050/6735  of train images\n",
      "Reading: 4100/6735  of train images\n",
      "Reading: 4150/6735  of train images\n",
      "Reading: 4200/6735  of train images\n",
      "Reading: 4250/6735  of train images\n",
      "Reading: 4300/6735  of train images\n",
      "Reading: 4350/6735  of train images\n",
      "Reading: 4400/6735  of train images\n",
      "Reading: 4450/6735  of train images\n",
      "Reading: 4500/6735  of train images\n",
      "Reading: 4550/6735  of train images\n",
      "Reading: 4600/6735  of train images\n",
      "Reading: 4650/6735  of train images\n",
      "Reading: 4700/6735  of train images\n",
      "Reading: 4750/6735  of train images\n",
      "Reading: 4800/6735  of train images\n",
      "Reading: 4850/6735  of train images\n",
      "Reading: 4900/6735  of train images\n",
      "Reading: 4950/6735  of train images\n",
      "Reading: 5000/6735  of train images\n",
      "Reading: 5050/6735  of train images\n",
      "Reading: 5100/6735  of train images\n",
      "Reading: 5150/6735  of train images\n",
      "Reading: 5200/6735  of train images\n",
      "Reading: 5250/6735  of train images\n",
      "Reading: 5300/6735  of train images\n",
      "Reading: 5350/6735  of train images\n",
      "Reading: 5400/6735  of train images\n",
      "Reading: 5450/6735  of train images\n",
      "Reading: 5500/6735  of train images\n",
      "Reading: 5550/6735  of train images\n",
      "Reading: 5600/6735  of train images\n",
      "Reading: 5650/6735  of train images\n",
      "Reading: 5700/6735  of train images\n",
      "Reading: 5750/6735  of train images\n",
      "Reading: 5800/6735  of train images\n",
      "Reading: 5850/6735  of train images\n",
      "Reading: 5900/6735  of train images\n",
      "Reading: 5950/6735  of train images\n",
      "Reading: 6000/6735  of train images\n",
      "Reading: 6050/6735  of train images\n",
      "Reading: 6100/6735  of train images\n"
     ]
    }
   ],
   "source": [
    "#training the model\n",
    "\n",
    "image_size = 65\n",
    "img_ch = 1\n",
    "batch_size =8\n",
    "LR = 0.0001\n",
    "epochs = 150\n",
    "p = 0.2\n",
    "path = '/Lab1/Lab3/CT/'\n",
    "fold1 = 'Image'\n",
    "fold2 = 'Mask'\n",
    "base = 32\n",
    "batch_normalization = True\n",
    "\n",
    "train_img, train_mask, test_img, test_mask = get_train_test_data(fold1, fold2, path, p,image_size, image_size, image_size)\n",
    "print('lol')\n",
    "model = u_net3D(image_size, image_size, img_ch, img_d, batch_normalization, base)\n",
    "model.compile(optimizer = Adam(lr=LR), loss = dice_coef_loss, metrics =[dice_coef])\n",
    "\n",
    "History = model.fit(train_img, train_mask, epochs = epochs, batch_size = batch_size, verbose = 1,\n",
    "                        validation_data = (test_img,test_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
