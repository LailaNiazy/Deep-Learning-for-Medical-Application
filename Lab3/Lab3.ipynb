{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.gpu.set_per_process_memory_fraction(0.3)\n",
    "tf.config.gpu.set_per_process_memory_growth(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##image loader\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "def path_loader(fold1, fold2, data_path):\n",
    "    #Creating data path\n",
    "    image_data_path = os.path.join(data_path, fold1)   \n",
    "    mask_data_path = os.path.join(data_path, fold2)\n",
    "    images = []\n",
    "    masks = []\n",
    "    #Listing all file names in the path\n",
    "    for root, dirs, files in os.walk(image_data_path):\n",
    "        for name in files:\n",
    "            images.append(os.path.join(image_data_path,name))\n",
    "    for root2, dirs2, files2 in os.walk(mask_data_path):\n",
    "        for name2 in files2:\n",
    "            masks.append(os.path.join(mask_data_path,name2))\n",
    "    return images, masks\n",
    "    \n",
    "\n",
    "# reading and resizing the training images with their corresponding labels\n",
    "def get_train_data_shuffled(images, masks, p):\n",
    "    \n",
    "    c = list(zip(images, masks))\n",
    "\n",
    "    shuffle(c)\n",
    "\n",
    "    images, masks = zip(*c)\n",
    "    \n",
    "    train_x, test_x, train_y, test_y = train_test_split(images,masks,test_size = p)\n",
    "\n",
    "    return train_x, test_x, train_y, test_y \n",
    "\n",
    "def data_loader(fold1, fold2, data_path, p,img_h, img_w):\n",
    "    \n",
    "    images, masks = path_loader(fold1, fold2, data_path)\n",
    "    train_x, test_x, train_y, test_y = get_train_data_shuffled(images, masks, p)\n",
    "    \n",
    "    train_img = []\n",
    "    train_mask = []\n",
    "    test_img = []\n",
    "    test_mask = []\n",
    "    len(train_x)\n",
    "    for i in range(len(train_x)):\n",
    "        image_name = train_x[i]\n",
    "        img = imread(image_name, as_grey=True)\n",
    "        img = resize(img, (img_h, img_w), anti_aliasing = True).astype('float32')\n",
    "        train_img.append([np.array(img)]) \n",
    "\n",
    "        if i % 50 == 0:\n",
    "             print('Reading: {0}/{1}  of train images'.format(i, len(train_x)))\n",
    "    for j in range(len(train_y)):\n",
    "        mask_name = train_y[j]\n",
    "        mask = imread(mask_name, as_grey=True)\n",
    "        mask = resize(img, (img_h, img_w), anti_aliasing = True).astype('float32')\n",
    "        train_mask.append([np.array(mask)])\n",
    "        \n",
    "    for i in range(len(test_x)):\n",
    "        image_name = test_x[i]\n",
    "        img = imread(image_name, as_grey=True)\n",
    "        img = resize(img, (img_h, img_w), anti_aliasing = True).astype('float32')\n",
    "        test_img.append([np.array(img)]) \n",
    "\n",
    "        if i % 50 == 0:\n",
    "             print('Reading: {0}/{1}  of test images'.format(i, len(test_x)))\n",
    "                \n",
    "    for j in range(len(test_y)):\n",
    "        mask_name = test_y[j]\n",
    "        mask = imread(mask_name, as_grey=True)\n",
    "        mask = resize(img, (img_h, img_w), anti_aliasing = True).astype('float32')\n",
    "        test_mask.append([np.array(mask)])        \n",
    "        \n",
    " \n",
    "    return train_img, train_mask, test_img, test_mask\n",
    "\n",
    "# Instantiating images and labels for the model.\n",
    "def get_train_test_data(fold1, fold2, data_path, p,img_h, img_w):\n",
    "    \n",
    "    train_img, train_mask, test_img, test_mask = data_loader(fold1, fold2, data_path, p,img_h, img_w)\n",
    "\n",
    "    Train_Img = np.zeros((len(train_img), img_h, img_w), dtype = np.float32)\n",
    "    Test_Img = np.zeros((len(test_img), img_h, img_w), dtype = np.float32)\n",
    "\n",
    "    Train_Label = np.zeros((len(train_mask),img_h, img_w), dtype = np.int32)\n",
    "    Test_Label = np.zeros((len(test_mask),img_h, img_w), dtype = np.int32)\n",
    "\n",
    "    for i in range(len(train_img)):\n",
    "        Train_Img[i] = train_img[i][0]\n",
    "        Train_Label[i] = train_mask[i][0]\n",
    "\n",
    "    Train_Img = np.expand_dims(Train_Img, axis = 3)  \n",
    "    Train_Label = np.expand_dims(Train_Label, axis = 3) \n",
    "\n",
    "    for j in range(len(test_img)):\n",
    "        Test_Img[j] = test_img[j][0]\n",
    "        Test_Label[j] = test_mask[j][0]\n",
    "\n",
    "    Test_Img = np.expand_dims(Test_Img, axis = 3)\n",
    "    Test_Label = np.expand_dims(Test_Label, axis = 3)\n",
    "    print(Train_Img.shape)\n",
    "    print(Test_Img.shape)\n",
    "\n",
    "    return Train_Img, Train_Label, Test_Img, Test_Label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Data Augmentation\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img\n",
    "\n",
    "def DataAugmentation(rotation_range,width_shift,height_shift_range,rescale,horizontal_flip):\n",
    "    \n",
    "    #Train data\n",
    "    train_datagen = ImageDataGenerator(rotation_range = rotation_range, width_shift_range = width_shift, height_shift_range=height_shift_range,\n",
    "                                       horizontal_flip = horizontal_flip, rescale = rescale)\n",
    "   \n",
    "    #Val data\n",
    "    val_datagen = ImageDataGenerator(rescale = rescale)\n",
    " \n",
    "    \n",
    "    \n",
    "    return train_datagen, val_datagen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##similarity metrics\n",
    "from sklearn.metrics import recall_score, precision_score\n",
    "from scipy.spatial.distance import dice\n",
    "\n",
    "\n",
    "def similarity(metric,img1,img2):\n",
    "\n",
    "    if metric == 'recall':\n",
    "        m = recall_score(img1,img2)\n",
    "    elif metric == 'precision':\n",
    "        m = precision_score(img1,img2)\n",
    "        \n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "smooth = 1.\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1-dice_coef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##model\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, Input, concatenate, Conv2DTranspose\n",
    "from tensorflow.keras.layers import Dropout, BatchNormalization, SpatialDropout2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "\n",
    "def u_net(Base,img_height, img_width, img_ch, batchNormalization, SDRate, spatial_dropout):\n",
    "    inputs = Input((img_height, img_width, img_ch))\n",
    "    #model = Sequential(img_ch,img_width,img_height, batchNormalization, spatial_dropout, SDRate, dropout, dropoutRate)\n",
    "    ## Contraction\n",
    "    # Conv Block 1\n",
    "    \n",
    "    c1 = Conv2D(filters=Base,\n",
    "                     kernel_size=(3,3), strides=(1,1), padding='same')(inputs)\n",
    "    \n",
    "     #Add batch Normalization\n",
    "    if batchNormalization:\n",
    "        c1 = BatchNormalization(axis=-1)(c1)\n",
    "    \n",
    "    a1 = Activation('relu')(c1)\n",
    "    \n",
    "    #Add spatial Dropout\n",
    "    if spatial_dropout:\n",
    "        a1 = SpatialDropout2D(SDRate)(a1)\n",
    "        \n",
    "    c2 = Conv2D(filters=Base,\n",
    "                     kernel_size=(3,3), strides=(1,1), padding='same')(a1)\n",
    "    \n",
    "     #Add batch Normalization\n",
    "    if batchNormalization:\n",
    "        c2 = BatchNormalization(axis=-1)(c2)\n",
    "    \n",
    "    a2 = Activation('relu')(c2)\n",
    "    \n",
    "    #Add spatial Dropout\n",
    "    if spatial_dropout:\n",
    "        a2 = SpatialDropout2D(SDRate)(a2)\n",
    "        \n",
    "    m1 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(a2)\n",
    "        \n",
    "    # Conv Block 2\n",
    "    c3 = Conv2D(filters=Base*2,\n",
    "                     kernel_size=(3,3), strides=(1,1), padding='same')(m1)\n",
    "    \n",
    "     #Add batch Normalization\n",
    "    if batchNormalization:\n",
    "        c3 = BatchNormalization(axis=-1)(c3)\n",
    "    \n",
    "    a3 = Activation('relu')(c3)\n",
    "    \n",
    "    #Add spatial Dropout\n",
    "    if spatial_dropout:\n",
    "        a3 = SpatialDropout2D(SDRate)(a3)\n",
    "    \n",
    "    c4 = Conv2D(filters=Base*2,\n",
    "                     kernel_size=(3,3), strides=(1,1), padding='same')(a3)\n",
    "    \n",
    "     #Add batch Normalization\n",
    "    if batchNormalization:\n",
    "        c4 = BatchNormalization(axis=-1)(c4)\n",
    "    \n",
    "    a4 = Activation('relu')(c4)\n",
    "    \n",
    "    #Add spatial Dropout\n",
    "    if spatial_dropout:\n",
    "        a4 = SpatialDropout2D(SDRate)(a4)\n",
    "    \n",
    "    m2 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(a4)\n",
    "    \n",
    "    # Conv Block 3\n",
    "    c5 = Conv2D(filters=Base*4, \n",
    "                     kernel_size=(3,3), strides=(1,1), padding='same')(m2)\n",
    "    \n",
    "     #Add batch Normalization\n",
    "    if batchNormalization:\n",
    "        c5 = BatchNormalization(axis=-1)(c5)\n",
    "    \n",
    "    a5 = Activation('relu')(c5)\n",
    "    \n",
    "    #Add spatial Dropout\n",
    "    if spatial_dropout:\n",
    "        a5 = SpatialDropout2D(SDRate)(a5)\n",
    "        \n",
    "    c6 = Conv2D(filters=Base*4,\n",
    "                     kernel_size=(3,3), strides=(1,1), padding='same')(a5)\n",
    "    \n",
    "     #Add batch Normalization\n",
    "    if batchNormalization:\n",
    "          c6 = BatchNormalization(axis=-1)(c6)\n",
    "    \n",
    "    a6 = Activation('relu')(c6)\n",
    "    \n",
    "    #Add spatial Dropout\n",
    "    if spatial_dropout:\n",
    "        a6 = SpatialDropout2D(SDRate)(a6)\n",
    "        \n",
    "    m3 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(a6)\n",
    "    \n",
    "    # Conv Block 4\n",
    "    c7 = Conv2D(filters=Base*8, \n",
    "                     kernel_size=(3,3), strides=(1,1), padding='same')(m3)\n",
    "    \n",
    "     #Add batch Normalization\n",
    "    if batchNormalization:\n",
    "        c7 = BatchNormalization(axis=-1)(c7)\n",
    "    \n",
    "    a7 = Activation('relu')(c7)\n",
    "    \n",
    "    #Add spatial Dropout\n",
    "    if spatial_dropout:\n",
    "        a7 = SpatialDropout2D(SDRate)(a7)\n",
    "        \n",
    "    c8 = Conv2D(filters=Base*8,\n",
    "                     kernel_size=(3,3), strides=(1,1), padding='same')(a7)\n",
    "    \n",
    "     #Add batch Normalization\n",
    "    if batchNormalization:\n",
    "        c8 = BatchNormalization(axis=-1)(c8)\n",
    "    \n",
    "    a8 = Activation('relu')(c8)\n",
    "    \n",
    "    #Add spatial Dropout\n",
    "    if spatial_dropout:\n",
    "        a8 = SpatialDropout2D(SDRate)(a8)\n",
    "        \n",
    "    m4 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(a8)\n",
    "    \n",
    "    ##Bottleneck\n",
    "    # Conv Layer\n",
    "    c9 = Conv2D(filters=Base*16, \n",
    "                     kernel_size=(3,3), strides=(1,1), padding='same')(m4)\n",
    "    \n",
    "     #Add batch Normalization\n",
    "    if batchNormalization:\n",
    "        c9 = BatchNormalization(axis=-1)(c9)\n",
    "    \n",
    "    a9 = Activation('relu')(c9)\n",
    "    \n",
    "    #Add spatial Dropout\n",
    "    if spatial_dropout:\n",
    "        a9 = SpatialDropout2D(SDRate)(a9)\n",
    "        \n",
    "    ##Expansion\n",
    "    #Conv Block 1\n",
    "    c10 = Conv2DTranspose(filters=Base*8,\n",
    "                     kernel_size=(2,2), strides=(2,2), padding='same')(a9)\n",
    "    c10 = concatenate([a8,c10])\n",
    "    \n",
    "    c11 = Conv2D(filters=Base*8,\n",
    "                     kernel_size=(3,3), strides=(1,1), padding='same')(c10)\n",
    "    \n",
    "     #Add batch Normalization\n",
    "    if batchNormalization:\n",
    "        c11 = BatchNormalization(axis=-1)(c11)\n",
    "    \n",
    "    a10 = Activation('relu')(c11)\n",
    "    \n",
    "    #Add spatial Dropout\n",
    "    if spatial_dropout:\n",
    "        a10 = SpatialDropout2D(SDRate)(a10)\n",
    "        \n",
    "    c12 = Conv2D(filters=Base*8, \n",
    "                     kernel_size=(3,3), strides=(1,1), padding='same')(a10)\n",
    "    \n",
    "     #Add batch Normalization\n",
    "    if batchNormalization:\n",
    "        c12 = BatchNormalization(axis=-1)(c12)\n",
    "    \n",
    "    a11 = Activation('relu')(c12)\n",
    "    \n",
    "    #Add spatial Dropout\n",
    "    if spatial_dropout:\n",
    "        a11 = SpatialDropout2D(SDRate)(a11)\n",
    "        \n",
    "    \n",
    "    #Conv Block 2\n",
    "    c13 = Conv2DTranspose(filters=Base*4,\n",
    "                     kernel_size=(2,2), strides=(2,2), padding='same')(a11)\n",
    "    c13 = concatenate([a6,c13])\n",
    "    \n",
    "    c14 = Conv2D(filters=Base*4,\n",
    "                     kernel_size=(3,3), strides=(1,1), padding='same')(c13)\n",
    "    \n",
    "     #Add batch Normalization\n",
    "    if batchNormalization:\n",
    "        c14 = BatchNormalization(axis=-1)(c14)\n",
    "    \n",
    "    a12 = Activation('relu')(c14)\n",
    "    \n",
    "    #Add spatial Dropout\n",
    "    if spatial_dropout:\n",
    "        a12 = SpatialDropout2D(SDRate)(a12)\n",
    "        \n",
    "    c15 = Conv2D(filters=Base*4, \n",
    "                     kernel_size=(3,3), strides=(1,1), padding='same')(a12)\n",
    "    \n",
    "     #Add batch Normalization\n",
    "    if batchNormalization:\n",
    "        c15 = BatchNormalization(axis=-1)(c15)\n",
    "    \n",
    "    a13 = Activation('relu')(c15)\n",
    "    \n",
    "    #Add spatial Dropout\n",
    "    if spatial_dropout:\n",
    "        a13 = SpatialDropout2D(SDRate)(a13)\n",
    "        \n",
    "    \n",
    "    #Conv Block 3\n",
    "    c16 = Conv2DTranspose(filters=Base*2,\n",
    "                     kernel_size=(2,2), strides=(2,2), padding='same')(a13)\n",
    "    c16 = concatenate([a4,c16])\n",
    "    \n",
    "    c17 = Conv2D(filters=Base*2, \n",
    "                     kernel_size=(3,3), strides=(1,1), padding='same')(c16)\n",
    "    \n",
    "     #Add batch Normalization\n",
    "    if batchNormalization:\n",
    "        c17 = BatchNormalization(axis=-1)(c17)\n",
    "    \n",
    "    a14 = Activation('relu')(c17)\n",
    "    \n",
    "    #Add spatial Dropout\n",
    "    if spatial_dropout:\n",
    "        a14 = SpatialDropout2D(SDRate)(a14)\n",
    "        \n",
    "    c18 = Conv2D(filters=Base*2,\n",
    "                     kernel_size=(3,3), strides=(1,1), padding='same')(a14)\n",
    "    \n",
    "     #Add batch Normalization\n",
    "    if batchNormalization:\n",
    "        c18 = BatchNormalization(axis=-1)(c18)\n",
    "    \n",
    "    a15 = Activation('relu')(c18)\n",
    "    \n",
    "    #Add spatial Dropout\n",
    "    if spatial_dropout:\n",
    "        a15 = SpatialDropout2D(SDRate)(a15)\n",
    "        \n",
    "    \n",
    "    #Conv Block 4\n",
    "    c19 = Conv2DTranspose(filters=Base,\n",
    "                     kernel_size=(2,2), strides=(2,2), padding='same')(a15)\n",
    "    c19 = concatenate([a2,c19])\n",
    "    \n",
    "    c20 = Conv2D(filters=Base,\n",
    "                     kernel_size=(3,3), strides=(1,1), padding='same')(c19)\n",
    "    \n",
    "     #Add batch Normalization\n",
    "    if batchNormalization:\n",
    "        c20 = BatchNormalization(axis=-1)(c20)\n",
    "    \n",
    "    a16 = Activation('relu')(c20)\n",
    "    \n",
    "    #Add spatial Dropout\n",
    "    if spatial_dropout:\n",
    "        a16 = SpatialDropout2D(SDRate)(a16)\n",
    "        \n",
    "    c21 = Conv2D(filters=Base,\n",
    "                     kernel_size=(3,3), strides=(1,1), padding='same')(a16)\n",
    "    \n",
    "     #Add batch Normalization\n",
    "    if batchNormalization:\n",
    "        c21 = BatchNormalization(axis=-1)(c21)\n",
    "    \n",
    "    a17 = Activation('relu')(c21)\n",
    "    \n",
    "    #final layer\n",
    "    c22 = Conv2D(1, kernel_size=(3,3), strides=(1,1), padding='same')(a17)\n",
    "    a18 = Activation('sigmoid')(c22)\n",
    "    \n",
    "    model = Model(inputs,a18)\n",
    "    \n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##model\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, Input, concatenate, Conv2DTranspose\n",
    "from tensorflow.keras.layers import Dropout, BatchNormalization, SpatialDropout2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "\n",
    "def u_net_3labels(Base,img_height, img_width, img_ch, batchNormalization, SDRate, spatial_dropout):\n",
    "    print(\"building model\")\n",
    "    inputs = Input((img_height, img_width, img_ch))\n",
    "    #model = Sequential(img_ch,img_width,img_height, batchNormalization, spatial_dropout, SDRate, dropout, dropoutRate)\n",
    "    ## Contraction\n",
    "    # Conv Block 1\n",
    "    \n",
    "    c1 = Conv2D(filters=Base,\n",
    "                     kernel_size=(3,3), strides=(1,1), padding='same')(inputs)\n",
    "    \n",
    "     #Add batch Normalization\n",
    "    if batchNormalization:\n",
    "        c1 = BatchNormalization(axis=-1)(c1)\n",
    "    \n",
    "    a1 = Activation('relu')(c1)\n",
    "    \n",
    "    #Add spatial Dropout\n",
    "    if spatial_dropout:\n",
    "        a1 = SpatialDropout2D(SDRate)(a1)\n",
    "        \n",
    "    c2 = Conv2D(filters=Base,\n",
    "                     kernel_size=(3,3), strides=(1,1), padding='same')(a1)\n",
    "    \n",
    "     #Add batch Normalization\n",
    "    if batchNormalization:\n",
    "        c2 = BatchNormalization(axis=-1)(c2)\n",
    "    \n",
    "    a2 = Activation('relu')(c2)\n",
    "    \n",
    "    #Add spatial Dropout\n",
    "    if spatial_dropout:\n",
    "        a2 = SpatialDropout2D(SDRate)(a2)\n",
    "        \n",
    "    m1 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(a2)\n",
    "        \n",
    "    # Conv Block 2\n",
    "    c3 = Conv2D(filters=Base*2,\n",
    "                     kernel_size=(3,3), strides=(1,1), padding='same')(m1)\n",
    "    \n",
    "     #Add batch Normalization\n",
    "    if batchNormalization:\n",
    "        c3 = BatchNormalization(axis=-1)(c3)\n",
    "    \n",
    "    a3 = Activation('relu')(c3)\n",
    "    \n",
    "    #Add spatial Dropout\n",
    "    if spatial_dropout:\n",
    "        a3 = SpatialDropout2D(SDRate)(a3)\n",
    "    \n",
    "    c4 = Conv2D(filters=Base*2,\n",
    "                     kernel_size=(3,3), strides=(1,1), padding='same')(a3)\n",
    "    \n",
    "     #Add batch Normalization\n",
    "    if batchNormalization:\n",
    "        c4 = BatchNormalization(axis=-1)(c4)\n",
    "    \n",
    "    a4 = Activation('relu')(c4)\n",
    "    \n",
    "    #Add spatial Dropout\n",
    "    if spatial_dropout:\n",
    "        a4 = SpatialDropout2D(SDRate)(a4)\n",
    "    \n",
    "    m2 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(a4)\n",
    "    \n",
    "    # Conv Block 3\n",
    "    c5 = Conv2D(filters=Base*4, \n",
    "                     kernel_size=(3,3), strides=(1,1), padding='same')(m2)\n",
    "    \n",
    "     #Add batch Normalization\n",
    "    if batchNormalization:\n",
    "        c5 = BatchNormalization(axis=-1)(c5)\n",
    "    \n",
    "    a5 = Activation('relu')(c5)\n",
    "    \n",
    "    #Add spatial Dropout\n",
    "    if spatial_dropout:\n",
    "        a5 = SpatialDropout2D(SDRate)(a5)\n",
    "        \n",
    "    c6 = Conv2D(filters=Base*4,\n",
    "                     kernel_size=(3,3), strides=(1,1), padding='same')(a5)\n",
    "    \n",
    "     #Add batch Normalization\n",
    "    if batchNormalization:\n",
    "          c6 = BatchNormalization(axis=-1)(c6)\n",
    "    \n",
    "    a6 = Activation('relu')(c6)\n",
    "    \n",
    "    #Add spatial Dropout\n",
    "    if spatial_dropout:\n",
    "        a6 = SpatialDropout2D(SDRate)(a6)\n",
    "        \n",
    "    m3 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(a6)\n",
    "    \n",
    "    # Conv Block 4\n",
    "    c7 = Conv2D(filters=Base*8, \n",
    "                     kernel_size=(3,3), strides=(1,1), padding='same')(m3)\n",
    "    \n",
    "     #Add batch Normalization\n",
    "    if batchNormalization:\n",
    "        c7 = BatchNormalization(axis=-1)(c7)\n",
    "    \n",
    "    a7 = Activation('relu')(c7)\n",
    "    \n",
    "    #Add spatial Dropout\n",
    "    if spatial_dropout:\n",
    "        a7 = SpatialDropout2D(SDRate)(a7)\n",
    "        \n",
    "    c8 = Conv2D(filters=Base*8,\n",
    "                     kernel_size=(3,3), strides=(1,1), padding='same')(a7)\n",
    "    \n",
    "     #Add batch Normalization\n",
    "    if batchNormalization:\n",
    "        c8 = BatchNormalization(axis=-1)(c8)\n",
    "    \n",
    "    a8 = Activation('relu')(c8)\n",
    "    \n",
    "    #Add spatial Dropout\n",
    "    if spatial_dropout:\n",
    "        a8 = SpatialDropout2D(SDRate)(a8)\n",
    "        \n",
    "    m4 = MaxPooling2D(pool_size=(2, 2), strides=(2, 2))(a8)\n",
    "    \n",
    "    ##Bottleneck\n",
    "    # Conv Layer\n",
    "    c9 = Conv2D(filters=Base*16, \n",
    "                     kernel_size=(3,3), strides=(1,1), padding='same')(m4)\n",
    "    \n",
    "     #Add batch Normalization\n",
    "    if batchNormalization:\n",
    "        c9 = BatchNormalization(axis=-1)(c9)\n",
    "    \n",
    "    a9 = Activation('relu')(c9)\n",
    "    \n",
    "    #Add spatial Dropout\n",
    "    if spatial_dropout:\n",
    "        a9 = SpatialDropout2D(SDRate)(a9)\n",
    "        \n",
    "    ##Expansion\n",
    "    #Conv Block 1\n",
    "    c10 = Conv2DTranspose(filters=Base*8,\n",
    "                     kernel_size=(2,2), strides=(2,2), padding='same')(a9)\n",
    "    c10 = concatenate([a8,c10])\n",
    "    \n",
    "    c11 = Conv2D(filters=Base*8,\n",
    "                     kernel_size=(3,3), strides=(1,1), padding='same')(c10)\n",
    "    \n",
    "     #Add batch Normalization\n",
    "    if batchNormalization:\n",
    "        c11 = BatchNormalization(axis=-1)(c11)\n",
    "    \n",
    "    a10 = Activation('relu')(c11)\n",
    "    \n",
    "    #Add spatial Dropout\n",
    "    if spatial_dropout:\n",
    "        a10 = SpatialDropout2D(SDRate)(a10)\n",
    "        \n",
    "    c12 = Conv2D(filters=Base*8, \n",
    "                     kernel_size=(3,3), strides=(1,1), padding='same')(a10)\n",
    "    \n",
    "     #Add batch Normalization\n",
    "    if batchNormalization:\n",
    "        c12 = BatchNormalization(axis=-1)(c12)\n",
    "    \n",
    "    a11 = Activation('relu')(c12)\n",
    "    \n",
    "    #Add spatial Dropout\n",
    "    if spatial_dropout:\n",
    "        a11 = SpatialDropout2D(SDRate)(a11)\n",
    "        \n",
    "    \n",
    "    #Conv Block 2\n",
    "    c13 = Conv2DTranspose(filters=Base*4,\n",
    "                     kernel_size=(2,2), strides=(2,2), padding='same')(a11)\n",
    "    c13 = concatenate([a6,c13])\n",
    "    \n",
    "    c14 = Conv2D(filters=Base*4,\n",
    "                     kernel_size=(3,3), strides=(1,1), padding='same')(c13)\n",
    "    \n",
    "     #Add batch Normalization\n",
    "    if batchNormalization:\n",
    "        c14 = BatchNormalization(axis=-1)(c14)\n",
    "    \n",
    "    a12 = Activation('relu')(c14)\n",
    "    \n",
    "    #Add spatial Dropout\n",
    "    if spatial_dropout:\n",
    "        a12 = SpatialDropout2D(SDRate)(a12)\n",
    "        \n",
    "    c15 = Conv2D(filters=Base*4, \n",
    "                     kernel_size=(3,3), strides=(1,1), padding='same')(a12)\n",
    "    \n",
    "     #Add batch Normalization\n",
    "    if batchNormalization:\n",
    "        c15 = BatchNormalization(axis=-1)(c15)\n",
    "    \n",
    "    a13 = Activation('relu')(c15)\n",
    "    \n",
    "    #Add spatial Dropout\n",
    "    if spatial_dropout:\n",
    "        a13 = SpatialDropout2D(SDRate)(a13)\n",
    "        \n",
    "    \n",
    "    #Conv Block 3\n",
    "    c16 = Conv2DTranspose(filters=Base*2,\n",
    "                     kernel_size=(2,2), strides=(2,2), padding='same')(a13)\n",
    "    c16 = concatenate([a4,c16])\n",
    "    \n",
    "    c17 = Conv2D(filters=Base*2, \n",
    "                     kernel_size=(3,3), strides=(1,1), padding='same')(c16)\n",
    "    \n",
    "     #Add batch Normalization\n",
    "    if batchNormalization:\n",
    "        c17 = BatchNormalization(axis=-1)(c17)\n",
    "    \n",
    "    a14 = Activation('relu')(c17)\n",
    "    \n",
    "    #Add spatial Dropout\n",
    "    if spatial_dropout:\n",
    "        a14 = SpatialDropout2D(SDRate)(a14)\n",
    "        \n",
    "    c18 = Conv2D(filters=Base*2,\n",
    "                     kernel_size=(3,3), strides=(1,1), padding='same')(a14)\n",
    "    \n",
    "     #Add batch Normalization\n",
    "    if batchNormalization:\n",
    "        c18 = BatchNormalization(axis=-1)(c18)\n",
    "    \n",
    "    a15 = Activation('relu')(c18)\n",
    "    \n",
    "    #Add spatial Dropout\n",
    "    if spatial_dropout:\n",
    "        a15 = SpatialDropout2D(SDRate)(a15)\n",
    "        \n",
    "    \n",
    "    #Conv Block 4\n",
    "    c19 = Conv2DTranspose(filters=Base,\n",
    "                     kernel_size=(2,2), strides=(2,2), padding='same')(a15)\n",
    "    c19 = concatenate([a2,c19])\n",
    "    \n",
    "    c20 = Conv2D(filters=Base,\n",
    "                     kernel_size=(3,3), strides=(1,1), padding='same')(c19)\n",
    "    \n",
    "     #Add batch Normalization\n",
    "    if batchNormalization:\n",
    "        c20 = BatchNormalization(axis=-1)(c20)\n",
    "    \n",
    "    a16 = Activation('relu')(c20)\n",
    "    \n",
    "    #Add spatial Dropout\n",
    "    if spatial_dropout:\n",
    "        a16 = SpatialDropout2D(SDRate)(a16)\n",
    "        \n",
    "    c21 = Conv2D(filters=Base,\n",
    "                     kernel_size=(3,3), strides=(1,1), padding='same')(a16)\n",
    "    \n",
    "     #Add batch Normalization\n",
    "    if batchNormalization:\n",
    "        c21 = BatchNormalization(axis=-1)(c21)\n",
    "    \n",
    "    a17 = Activation('relu')(c21)\n",
    "    \n",
    "    #final layer\n",
    "    c22 = Conv2D(3, kernel_size=(3,3), strides=(1,1), padding='same')(a17)\n",
    "    a18 = Activation('softmax')(c22)\n",
    "    \n",
    "    model = Model(inputs,a18)\n",
    "    \n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##plotter\n",
    "import matplotlib.pyplot as plt\n",
    "def plotter(History):\n",
    "    #Training vs Validation Learning loss \n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.title(\"Learning curve\")\n",
    "    plt.plot(History.history[\"loss\"], label=\"loss\")\n",
    "    plt.plot(History.history[\"val_loss\"], label=\"val_loss\")\n",
    "    plt.plot( np.argmin(History.history[\"val_loss\"]),\n",
    "             np.min(History.history[\"val_loss\"]),\n",
    "             marker=\"x\", color=\"r\", label=\"best model\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss Value\")\n",
    "    plt.legend(); \n",
    "    \n",
    "    #Train and test accuracy plot\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.title(\"Dice Score Curve\")\n",
    "    plt.plot(History.history[\"dice_coef\"], label=\"dice_coef\")\n",
    "    plt.plot(History.history[\"val_dice_coef\"], label=\"val_dice_coef\")\n",
    "    #plt.plot(History.history[\"categorical_accuracy\"], label=\"categorical_accuracy\")\n",
    "    #plt.plot(History.history[\"val_categorical_accuracy\"], label=\"val_categorical_accuracy\")\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Dice Coef')\n",
    "    plt.legend(); \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1a/b & 2 & 3 & 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: 0/187  of train images\n",
      "Reading: 50/187  of train images\n",
      "Reading: 100/187  of train images\n",
      "Reading: 150/187  of train images\n",
      "Reading: 0/47  of test images\n",
      "(187, 256, 256, 1)\n",
      "(47, 256, 256, 1)\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 256, 256, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 256, 256, 16) 160         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 256, 256, 16) 0           conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_16 (SpatialDr (None, 256, 256, 16) 0           activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 256, 256, 16) 2320        spatial_dropout2d_16[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 256, 256, 16) 0           conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_17 (SpatialDr (None, 256, 256, 16) 0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 128, 128, 16) 0           spatial_dropout2d_17[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 128, 128, 32) 4640        max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 128, 128, 32) 0           conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_18 (SpatialDr (None, 128, 128, 32) 0           activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 128, 128, 32) 9248        spatial_dropout2d_18[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 128, 128, 32) 0           conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_19 (SpatialDr (None, 128, 128, 32) 0           activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 64, 64, 32)   0           spatial_dropout2d_19[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 64, 64, 64)   18496       max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 64, 64, 64)   0           conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_20 (SpatialDr (None, 64, 64, 64)   0           activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 64, 64, 64)   36928       spatial_dropout2d_20[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 64, 64, 64)   0           conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_21 (SpatialDr (None, 64, 64, 64)   0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 32, 32, 64)   0           spatial_dropout2d_21[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 32, 32, 128)  73856       max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 32, 32, 128)  0           conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_22 (SpatialDr (None, 32, 32, 128)  0           activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 32, 32, 128)  147584      spatial_dropout2d_22[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 32, 32, 128)  0           conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_23 (SpatialDr (None, 32, 32, 128)  0           activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 16, 16, 128)  0           spatial_dropout2d_23[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 16, 16, 256)  295168      max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 16, 16, 256)  0           conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_24 (SpatialDr (None, 16, 16, 256)  0           activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTrans (None, 32, 32, 128)  131200      spatial_dropout2d_24[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 32, 32, 256)  0           spatial_dropout2d_23[0][0]       \n",
      "                                                                 conv2d_transpose_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 32, 32, 128)  295040      concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 32, 32, 128)  0           conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_25 (SpatialDr (None, 32, 32, 128)  0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 32, 32, 128)  147584      spatial_dropout2d_25[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 32, 32, 128)  0           conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_26 (SpatialDr (None, 32, 32, 128)  0           activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTrans (None, 64, 64, 64)   32832       spatial_dropout2d_26[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 64, 64, 128)  0           spatial_dropout2d_21[0][0]       \n",
      "                                                                 conv2d_transpose_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 64, 64, 64)   73792       concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 64, 64, 64)   0           conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_27 (SpatialDr (None, 64, 64, 64)   0           activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 64, 64, 64)   36928       spatial_dropout2d_27[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 64, 64, 64)   0           conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_28 (SpatialDr (None, 64, 64, 64)   0           activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_6 (Conv2DTrans (None, 128, 128, 32) 8224        spatial_dropout2d_28[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 128, 128, 64) 0           spatial_dropout2d_19[0][0]       \n",
      "                                                                 conv2d_transpose_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 128, 128, 32) 18464       concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 128, 128, 32) 0           conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_29 (SpatialDr (None, 128, 128, 32) 0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 128, 128, 32) 9248        spatial_dropout2d_29[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 128, 128, 32) 0           conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_30 (SpatialDr (None, 128, 128, 32) 0           activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_7 (Conv2DTrans (None, 256, 256, 16) 2064        spatial_dropout2d_30[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 256, 256, 32) 0           spatial_dropout2d_17[0][0]       \n",
      "                                                                 conv2d_transpose_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 256, 256, 16) 4624        concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 256, 256, 16) 0           conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_31 (SpatialDr (None, 256, 256, 16) 0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 256, 256, 16) 2320        spatial_dropout2d_31[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 256, 256, 16) 0           conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 256, 256, 1)  145         activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 256, 256, 1)  0           conv2d_35[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,350,865\n",
      "Trainable params: 1,350,865\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "lol\n",
      "Train on 187 samples, validate on 47 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "187/187 [==============================] - 3s 13ms/sample - loss: -0.0196 - dice_coef: 0.0196 - val_loss: -0.0207 - val_dice_coef: 0.0207\n",
      "Epoch 2/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.0203 - dice_coef: 0.0204 - val_loss: -0.0212 - val_dice_coef: 0.0212\n",
      "Epoch 3/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.0214 - dice_coef: 0.0214 - val_loss: -0.0226 - val_dice_coef: 0.0226\n",
      "Epoch 4/150\n",
      "187/187 [==============================] - 1s 5ms/sample - loss: -0.0239 - dice_coef: 0.0240 - val_loss: -0.0271 - val_dice_coef: 0.0271\n",
      "Epoch 5/150\n",
      "187/187 [==============================] - 1s 5ms/sample - loss: -0.0259 - dice_coef: 0.0259 - val_loss: -0.0282 - val_dice_coef: 0.0282\n",
      "Epoch 6/150\n",
      "187/187 [==============================] - 1s 5ms/sample - loss: -0.0279 - dice_coef: 0.0280 - val_loss: -0.0339 - val_dice_coef: 0.0339\n",
      "Epoch 7/150\n",
      "187/187 [==============================] - 1s 5ms/sample - loss: -0.0323 - dice_coef: 0.0323 - val_loss: -0.0379 - val_dice_coef: 0.0379\n",
      "Epoch 8/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.0349 - dice_coef: 0.0349 - val_loss: -0.0399 - val_dice_coef: 0.0399\n",
      "Epoch 9/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.0365 - dice_coef: 0.0365 - val_loss: -0.0407 - val_dice_coef: 0.0407\n",
      "Epoch 10/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.0377 - dice_coef: 0.0377 - val_loss: -0.0424 - val_dice_coef: 0.0424\n",
      "Epoch 11/150\n",
      "187/187 [==============================] - 1s 5ms/sample - loss: -0.0405 - dice_coef: 0.0406 - val_loss: -0.0470 - val_dice_coef: 0.0470\n",
      "Epoch 12/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.0551 - dice_coef: 0.0554 - val_loss: -0.3010 - val_dice_coef: 0.3011\n",
      "Epoch 13/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.2862 - dice_coef: 0.2894 - val_loss: -0.4575 - val_dice_coef: 0.4583\n",
      "Epoch 14/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.1976 - dice_coef: 0.2065 - val_loss: -0.4487 - val_dice_coef: 0.4494\n",
      "Epoch 15/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.4087 - dice_coef: 0.4130 - val_loss: -0.3331 - val_dice_coef: 0.3335\n",
      "Epoch 16/150\n",
      "187/187 [==============================] - 1s 5ms/sample - loss: -0.4936 - dice_coef: 0.5002 - val_loss: -0.3998 - val_dice_coef: 0.3990\n",
      "Epoch 17/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.5653 - dice_coef: 0.5689 - val_loss: -0.3171 - val_dice_coef: 0.3180\n",
      "Epoch 18/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.6659 - dice_coef: 0.6688 - val_loss: -0.4288 - val_dice_coef: 0.4280\n",
      "Epoch 19/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.6276 - dice_coef: 0.6297 - val_loss: -0.2381 - val_dice_coef: 0.2382\n",
      "Epoch 20/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.6775 - dice_coef: 0.6802 - val_loss: -0.1848 - val_dice_coef: 0.1858\n",
      "Epoch 21/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.6270 - dice_coef: 0.6299 - val_loss: -0.1768 - val_dice_coef: 0.1772\n",
      "Epoch 22/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.6803 - dice_coef: 0.6845 - val_loss: -0.2045 - val_dice_coef: 0.2041\n",
      "Epoch 23/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.7293 - dice_coef: 0.7325 - val_loss: -0.1863 - val_dice_coef: 0.1865\n",
      "Epoch 24/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.6964 - dice_coef: 0.6991 - val_loss: -0.1444 - val_dice_coef: 0.1448\n",
      "Epoch 25/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.7699 - dice_coef: 0.7716 - val_loss: -0.1756 - val_dice_coef: 0.1752\n",
      "Epoch 26/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.8223 - dice_coef: 0.8234 - val_loss: -0.1264 - val_dice_coef: 0.1264\n",
      "Epoch 27/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.8600 - dice_coef: 0.8610 - val_loss: -0.1306 - val_dice_coef: 0.1305\n",
      "Epoch 28/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.7411 - dice_coef: 0.7458 - val_loss: -0.1678 - val_dice_coef: 0.1676\n",
      "Epoch 29/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.8425 - dice_coef: 0.8430 - val_loss: -0.1709 - val_dice_coef: 0.1709\n",
      "Epoch 30/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.8048 - dice_coef: 0.8073 - val_loss: -0.1508 - val_dice_coef: 0.1509\n",
      "Epoch 31/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.8067 - dice_coef: 0.8088 - val_loss: -0.1713 - val_dice_coef: 0.1712\n",
      "Epoch 32/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.8161 - dice_coef: 0.8188 - val_loss: -0.1513 - val_dice_coef: 0.1513\n",
      "Epoch 33/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9030 - dice_coef: 0.9032 - val_loss: -0.1754 - val_dice_coef: 0.1754\n",
      "Epoch 34/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.8971 - dice_coef: 0.8971 - val_loss: -0.2026 - val_dice_coef: 0.2026\n",
      "Epoch 35/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.8879 - dice_coef: 0.8898 - val_loss: -0.1761 - val_dice_coef: 0.1761\n",
      "Epoch 36/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.7942 - dice_coef: 0.7967 - val_loss: -0.1839 - val_dice_coef: 0.1839\n",
      "Epoch 37/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.8828 - dice_coef: 0.8840 - val_loss: -0.1730 - val_dice_coef: 0.1730\n",
      "Epoch 38/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.8941 - dice_coef: 0.8956 - val_loss: -0.1721 - val_dice_coef: 0.1721\n",
      "Epoch 39/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.8675 - dice_coef: 0.8466 - val_loss: -0.1705 - val_dice_coef: 0.1705\n",
      "Epoch 40/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9007 - dice_coef: 0.9020 - val_loss: -0.1844 - val_dice_coef: 0.1843\n",
      "Epoch 41/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.8700 - dice_coef: 0.8720 - val_loss: -0.1887 - val_dice_coef: 0.1887\n",
      "Epoch 42/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.8971 - dice_coef: 0.8983 - val_loss: -0.1872 - val_dice_coef: 0.1872\n",
      "Epoch 43/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.8757 - dice_coef: 0.8743 - val_loss: -0.1738 - val_dice_coef: 0.1739\n",
      "Epoch 44/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9389 - dice_coef: 0.9393 - val_loss: -0.2051 - val_dice_coef: 0.2051\n",
      "Epoch 45/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.8738 - dice_coef: 0.8751 - val_loss: -0.1783 - val_dice_coef: 0.1783\n",
      "Epoch 46/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9022 - dice_coef: 0.9039 - val_loss: -0.1894 - val_dice_coef: 0.1893\n",
      "Epoch 47/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9150 - dice_coef: 0.9155 - val_loss: -0.1862 - val_dice_coef: 0.1863\n",
      "Epoch 48/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.8994 - dice_coef: 0.9008 - val_loss: -0.1804 - val_dice_coef: 0.1804\n",
      "Epoch 49/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.8889 - dice_coef: 0.8897 - val_loss: -0.1916 - val_dice_coef: 0.1916\n",
      "Epoch 50/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9440 - dice_coef: 0.9443 - val_loss: -0.2016 - val_dice_coef: 0.2016\n",
      "Epoch 51/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9195 - dice_coef: 0.9181 - val_loss: -0.2060 - val_dice_coef: 0.2060\n",
      "Epoch 52/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9484 - dice_coef: 0.9485 - val_loss: -0.1987 - val_dice_coef: 0.1986\n",
      "Epoch 53/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9226 - dice_coef: 0.9235 - val_loss: -0.1980 - val_dice_coef: 0.1980\n",
      "Epoch 54/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9210 - dice_coef: 0.9216 - val_loss: -0.1980 - val_dice_coef: 0.1980\n",
      "Epoch 55/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.8859 - dice_coef: 0.8881 - val_loss: -0.1991 - val_dice_coef: 0.1991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9193 - dice_coef: 0.9205 - val_loss: -0.1863 - val_dice_coef: 0.1863\n",
      "Epoch 57/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9502 - dice_coef: 0.9506 - val_loss: -0.1998 - val_dice_coef: 0.1997\n",
      "Epoch 58/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9470 - dice_coef: 0.9474 - val_loss: -0.1867 - val_dice_coef: 0.1867\n",
      "Epoch 59/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9495 - dice_coef: 0.9495 - val_loss: -0.2026 - val_dice_coef: 0.2026\n",
      "Epoch 60/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.8314 - dice_coef: 0.8344 - val_loss: -0.2043 - val_dice_coef: 0.2043\n",
      "Epoch 61/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9455 - dice_coef: 0.9451 - val_loss: -0.1877 - val_dice_coef: 0.1877\n",
      "Epoch 62/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9558 - dice_coef: 0.9557 - val_loss: -0.1924 - val_dice_coef: 0.1924\n",
      "Epoch 63/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9499 - dice_coef: 0.9502 - val_loss: -0.2004 - val_dice_coef: 0.2004\n",
      "Epoch 64/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9201 - dice_coef: 0.9212 - val_loss: -0.1910 - val_dice_coef: 0.1910\n",
      "Epoch 65/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9591 - dice_coef: 0.9591 - val_loss: -0.1935 - val_dice_coef: 0.1935\n",
      "Epoch 66/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.8785 - dice_coef: 0.8807 - val_loss: -0.1935 - val_dice_coef: 0.1935\n",
      "Epoch 67/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9284 - dice_coef: 0.9293 - val_loss: -0.1927 - val_dice_coef: 0.1927\n",
      "Epoch 68/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9164 - dice_coef: 0.9173 - val_loss: -0.1910 - val_dice_coef: 0.1910\n",
      "Epoch 69/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.8938 - dice_coef: 0.8958 - val_loss: -0.1902 - val_dice_coef: 0.1902\n",
      "Epoch 70/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9118 - dice_coef: 0.9130 - val_loss: -0.2005 - val_dice_coef: 0.2005\n",
      "Epoch 71/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9556 - dice_coef: 0.9559 - val_loss: -0.1900 - val_dice_coef: 0.1900\n",
      "Epoch 72/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9218 - dice_coef: 0.9230 - val_loss: -0.1941 - val_dice_coef: 0.1941\n",
      "Epoch 73/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9025 - dice_coef: 0.9042 - val_loss: -0.2008 - val_dice_coef: 0.2008\n",
      "Epoch 74/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9642 - dice_coef: 0.9641 - val_loss: -0.2097 - val_dice_coef: 0.2097\n",
      "Epoch 75/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9035 - dice_coef: 0.9052 - val_loss: -0.2047 - val_dice_coef: 0.2047\n",
      "Epoch 76/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9414 - dice_coef: 0.9419 - val_loss: -0.2031 - val_dice_coef: 0.2031\n",
      "Epoch 77/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9323 - dice_coef: 0.9333 - val_loss: -0.1944 - val_dice_coef: 0.1944\n",
      "Epoch 78/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9056 - dice_coef: 0.9073 - val_loss: -0.2078 - val_dice_coef: 0.2078\n",
      "Epoch 79/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9668 - dice_coef: 0.9666 - val_loss: -0.2078 - val_dice_coef: 0.2078\n",
      "Epoch 80/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9592 - dice_coef: 0.9596 - val_loss: -0.1903 - val_dice_coef: 0.1903\n",
      "Epoch 81/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9345 - dice_coef: 0.9355 - val_loss: -0.1919 - val_dice_coef: 0.1919\n",
      "Epoch 82/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9372 - dice_coef: 0.9383 - val_loss: -0.2066 - val_dice_coef: 0.2066\n",
      "Epoch 83/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9053 - dice_coef: 0.9071 - val_loss: -0.2031 - val_dice_coef: 0.2030\n",
      "Epoch 84/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9614 - dice_coef: 0.9619 - val_loss: -0.1939 - val_dice_coef: 0.1939\n",
      "Epoch 85/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9580 - dice_coef: 0.9584 - val_loss: -0.2006 - val_dice_coef: 0.2006\n",
      "Epoch 86/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9588 - dice_coef: 0.9592 - val_loss: -0.2040 - val_dice_coef: 0.2039\n",
      "Epoch 87/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9113 - dice_coef: 0.9128 - val_loss: -0.2037 - val_dice_coef: 0.2037\n",
      "Epoch 88/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9705 - dice_coef: 0.9708 - val_loss: -0.1964 - val_dice_coef: 0.1964\n",
      "Epoch 89/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9681 - dice_coef: 0.9683 - val_loss: -0.1890 - val_dice_coef: 0.1890\n",
      "Epoch 90/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9386 - dice_coef: 0.9396 - val_loss: -0.1947 - val_dice_coef: 0.1946\n",
      "Epoch 91/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9719 - dice_coef: 0.9719 - val_loss: -0.1969 - val_dice_coef: 0.1969\n",
      "Epoch 92/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9415 - dice_coef: 0.9423 - val_loss: -0.1980 - val_dice_coef: 0.1980\n",
      "Epoch 93/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9728 - dice_coef: 0.9731 - val_loss: -0.1946 - val_dice_coef: 0.1946\n",
      "Epoch 94/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9133 - dice_coef: 0.9150 - val_loss: -0.1974 - val_dice_coef: 0.1974\n",
      "Epoch 95/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.8543 - dice_coef: 0.8573 - val_loss: -0.1978 - val_dice_coef: 0.1978\n",
      "Epoch 96/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9717 - dice_coef: 0.9718 - val_loss: -0.1910 - val_dice_coef: 0.1910\n",
      "Epoch 97/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9176 - dice_coef: 0.9192 - val_loss: -0.1937 - val_dice_coef: 0.1937\n",
      "Epoch 98/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9722 - dice_coef: 0.9725 - val_loss: -0.1965 - val_dice_coef: 0.1965\n",
      "Epoch 99/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9423 - dice_coef: 0.9427 - val_loss: -0.1898 - val_dice_coef: 0.1898\n",
      "Epoch 100/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.8265 - dice_coef: 0.8306 - val_loss: -0.1937 - val_dice_coef: 0.1937\n",
      "Epoch 101/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9147 - dice_coef: 0.9163 - val_loss: -0.1974 - val_dice_coef: 0.1974\n",
      "Epoch 102/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9739 - dice_coef: 0.9741 - val_loss: -0.1936 - val_dice_coef: 0.1936\n",
      "Epoch 103/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9742 - dice_coef: 0.9745 - val_loss: -0.1941 - val_dice_coef: 0.1941\n",
      "Epoch 104/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9706 - dice_coef: 0.9703 - val_loss: -0.1897 - val_dice_coef: 0.1897\n",
      "Epoch 105/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9755 - dice_coef: 0.9754 - val_loss: -0.1901 - val_dice_coef: 0.1901\n",
      "Epoch 106/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9696 - dice_coef: 0.9695 - val_loss: -0.1910 - val_dice_coef: 0.1910\n",
      "Epoch 107/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9379 - dice_coef: 0.9380 - val_loss: -0.1909 - val_dice_coef: 0.1909\n",
      "Epoch 108/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9157 - dice_coef: 0.9174 - val_loss: -0.1906 - val_dice_coef: 0.1906\n",
      "Epoch 109/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9711 - dice_coef: 0.9713 - val_loss: -0.1909 - val_dice_coef: 0.1909\n",
      "Epoch 110/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9746 - dice_coef: 0.9744 - val_loss: -0.1910 - val_dice_coef: 0.1910\n",
      "Epoch 111/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9767 - dice_coef: 0.9767 - val_loss: -0.1924 - val_dice_coef: 0.1924\n",
      "Epoch 112/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9645 - dice_coef: 0.9645 - val_loss: -0.1910 - val_dice_coef: 0.1910\n",
      "Epoch 113/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.8875 - dice_coef: 0.8897 - val_loss: -0.1908 - val_dice_coef: 0.1908\n",
      "Epoch 114/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.8551 - dice_coef: 0.8586 - val_loss: -0.1928 - val_dice_coef: 0.1928\n",
      "Epoch 115/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.8872 - dice_coef: 0.8897 - val_loss: -0.1924 - val_dice_coef: 0.1924\n",
      "Epoch 116/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9759 - dice_coef: 0.9759 - val_loss: -0.1927 - val_dice_coef: 0.1927\n",
      "Epoch 117/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9775 - dice_coef: 0.9775 - val_loss: -0.1940 - val_dice_coef: 0.1940\n",
      "Epoch 118/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9475 - dice_coef: 0.9483 - val_loss: -0.1929 - val_dice_coef: 0.1929\n",
      "Epoch 119/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9803 - dice_coef: 0.9793 - val_loss: -0.1923 - val_dice_coef: 0.1923\n",
      "Epoch 120/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9770 - dice_coef: 0.9769 - val_loss: -0.1923 - val_dice_coef: 0.1923\n",
      "Epoch 121/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9485 - dice_coef: 0.9489 - val_loss: -0.1924 - val_dice_coef: 0.1924\n",
      "Epoch 122/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9762 - dice_coef: 0.9764 - val_loss: -0.1919 - val_dice_coef: 0.1919\n",
      "Epoch 123/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9485 - dice_coef: 0.9494 - val_loss: -0.1926 - val_dice_coef: 0.1926\n",
      "Epoch 124/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9172 - dice_coef: 0.9190 - val_loss: -0.1900 - val_dice_coef: 0.1900\n",
      "Epoch 125/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9590 - dice_coef: 0.9599 - val_loss: -0.1916 - val_dice_coef: 0.1916\n",
      "Epoch 126/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9492 - dice_coef: 0.9503 - val_loss: -0.1933 - val_dice_coef: 0.1933\n",
      "Epoch 127/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9807 - dice_coef: 0.9809 - val_loss: -0.1911 - val_dice_coef: 0.1911\n",
      "Epoch 128/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9797 - dice_coef: 0.9796 - val_loss: -0.1936 - val_dice_coef: 0.1936\n",
      "Epoch 129/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9504 - dice_coef: 0.9464 - val_loss: -0.1922 - val_dice_coef: 0.1922\n",
      "Epoch 130/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9745 - dice_coef: 0.9748 - val_loss: -0.1903 - val_dice_coef: 0.1903\n",
      "Epoch 131/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9493 - dice_coef: 0.9504 - val_loss: -0.1919 - val_dice_coef: 0.1919\n",
      "Epoch 132/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9511 - dice_coef: 0.9521 - val_loss: -0.1927 - val_dice_coef: 0.1927\n",
      "Epoch 133/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9538 - dice_coef: 0.9547 - val_loss: -0.1911 - val_dice_coef: 0.1910\n",
      "Epoch 134/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9828 - dice_coef: 0.9830 - val_loss: -0.1927 - val_dice_coef: 0.1927\n",
      "Epoch 135/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9511 - dice_coef: 0.9519 - val_loss: -0.1913 - val_dice_coef: 0.1913\n",
      "Epoch 136/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9708 - dice_coef: 0.9713 - val_loss: -0.1905 - val_dice_coef: 0.1905\n",
      "Epoch 137/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9458 - dice_coef: 0.9470 - val_loss: -0.1912 - val_dice_coef: 0.1912\n",
      "Epoch 138/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.8911 - dice_coef: 0.8935 - val_loss: -0.1900 - val_dice_coef: 0.1900\n",
      "Epoch 139/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9840 - dice_coef: 0.9841 - val_loss: -0.1918 - val_dice_coef: 0.1918\n",
      "Epoch 140/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9562 - dice_coef: 0.9564 - val_loss: -0.1922 - val_dice_coef: 0.1922\n",
      "Epoch 141/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9540 - dice_coef: 0.9549 - val_loss: -0.1908 - val_dice_coef: 0.1908\n",
      "Epoch 142/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9822 - dice_coef: 0.9823 - val_loss: -0.1902 - val_dice_coef: 0.1902\n",
      "Epoch 143/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9535 - dice_coef: 0.9540 - val_loss: -0.1906 - val_dice_coef: 0.1906\n",
      "Epoch 144/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9810 - dice_coef: 0.9809 - val_loss: -0.1900 - val_dice_coef: 0.1900\n",
      "Epoch 145/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9835 - dice_coef: 0.9834 - val_loss: -0.1914 - val_dice_coef: 0.1914\n",
      "Epoch 146/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9798 - dice_coef: 0.9798 - val_loss: -0.1908 - val_dice_coef: 0.1908\n",
      "Epoch 147/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9496 - dice_coef: 0.9504 - val_loss: -0.1918 - val_dice_coef: 0.1919\n",
      "Epoch 148/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9536 - dice_coef: 0.9494 - val_loss: -0.1896 - val_dice_coef: 0.1896\n",
      "Epoch 149/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9727 - dice_coef: 0.9730 - val_loss: -0.1897 - val_dice_coef: 0.1897\n",
      "Epoch 150/150\n",
      "187/187 [==============================] - 1s 6ms/sample - loss: -0.9211 - dice_coef: 0.9226 - val_loss: -0.1899 - val_dice_coef: 0.1899\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from scipy.spatial.distance import dice\n",
    "image_size = 256\n",
    "img_ch = 1\n",
    "batch_size = 8\n",
    "LR = 0.0001\n",
    "SDRate = 0.5\n",
    "spatial_dropout = True\n",
    "epochs = 150\n",
    "p = 0.2 #percentage of training and test data\n",
    "path = '/Lab1/Lab3/X_ray/'\n",
    "fold1 = 'Image'\n",
    "fold2 = 'Mask'\n",
    "task = '2b' \n",
    "\n",
    "if task == '4':\n",
    "    \n",
    "    base  = 32\n",
    "    batch_normalization = True\n",
    "    rotation_range = 10\n",
    "    width_shift = 0.1\n",
    "    height_shift = 0.1\n",
    "    rescale = 0.2\n",
    "    horizontal_flip = True\n",
    "    \n",
    "    train_datagen, val_datagen = DataAugmentation(rotation_range,width_shift,height_shift,rescale,horizontal_flip)\n",
    "    train_img, train_mask, test_img, test_mask = get_train_test_data(fold1, fold2, path, p,image_size, image_size)\n",
    "    model = u_net(base,image_size, image_size, img_ch, batch_normalization, SDRate, spatial_dropout)\n",
    "    model.compile(optimizer = Adam(lr=LR), loss = dice_coef_loss, metrics =[dice_coef])\n",
    "    #Fit the data into the model\n",
    "    History = model.fit_generator(train_datagen.flow(train_img, train_mask,batch_size = batch_size), validation_data = val_datagen.flow(test_img, test_mask), epochs = epochs, verbose = 1)        \n",
    "\n",
    "else:\n",
    "    if task == '1a':\n",
    "        base = 16\n",
    "        batch_normalization = True\n",
    "        train_img, train_mask, test_img, test_mask = get_train_test_data(fold1, fold2, path, p, image_size, image_size)\n",
    "        model = u_net(base,image_size, image_size, img_ch, batch_normalization, SDRate, spatial_dropout)\n",
    "        model.compile(optimizer = Adam(lr=LR), loss = 'binary_crossentropy', metrics =[dice_coef])\n",
    "\n",
    "    elif task == '1b':\n",
    "        base = 16\n",
    "        batch_normalization = True\n",
    "        train_img, train_mask, test_img, test_mask = get_train_test_data(fold1, fold2, path, p,image_size, image_size)\n",
    "        model = u_net(base,image_size, image_size, img_ch, batch_normalization, SDRate, spatial_dropout)\n",
    "        model.compile(optimizer = Adam(lr=LR), loss = dice_coef_loss, metrics =[dice_coef])\n",
    "\n",
    "    elif task == '2a':\n",
    "        base = 16\n",
    "        batch_normalization = False\n",
    "        train_img, train_mask, test_img, test_mask = get_train_test_data(fold1, fold2, path, p,image_size, image_size)\n",
    "        model = u_net(base,image_size, image_size, img_ch, batch_normalization, SDRate, spatial_dropout)\n",
    "        model.compile(optimizer = Adam(lr=LR), loss = 'binary_crossentropy', metrics =[dice_coef])\n",
    "\n",
    "    elif task == '2b':\n",
    "        base = 16\n",
    "        batch_normalization = False\n",
    "        train_img, train_mask, test_img, test_mask = get_train_test_data(fold1, fold2, path, p,image_size, image_size)\n",
    "        model = u_net(base,image_size, image_size, img_ch, batch_normalization, SDRate, spatial_dropout)\n",
    "        model.compile(optimizer = Adam(lr=LR), loss = dice_coef_loss, metrics =[dice_coef])\n",
    "\n",
    "    elif task == '3':\n",
    "        base = 32\n",
    "        batch_normalization = True\n",
    "        train_img, train_mask, test_img, test_mask = get_train_test_data(fold1, fold2, path, p,image_size, image_size)\n",
    "        model = u_net(base,image_size, image_size, img_ch, batch_normalization, SDRate, spatial_dropout)\n",
    "        model.compile(optimizer = Adam(lr=LR), loss = dice_coef_loss, metrics =[dice_coef])\n",
    "    \n",
    "    print('lol')\n",
    "    History = model.fit(train_img, train_mask, epochs = epochs, batch_size = batch_size, verbose = 1,\n",
    "                        validation_data = (test_img,test_mask))\n",
    "    \n",
    "\n",
    "plotter(History)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls '/Lab1/Lab3/CT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7ff2024e48d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = plt.imread('/Lab1/Lab3/CT/Mask/Im127_128_Mask.png')\n",
    "plt.figure()\n",
    "plt.imshow(image)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/skimage/io/_io.py:48: UserWarning: `as_grey` has been deprecated in favor of `as_gray`\n",
      "  warn('`as_grey` has been deprecated in favor of `as_gray`')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: 0/6735  of train images\n",
      "Reading: 50/6735  of train images\n",
      "Reading: 100/6735  of train images\n",
      "Reading: 150/6735  of train images\n",
      "Reading: 200/6735  of train images\n",
      "Reading: 250/6735  of train images\n",
      "Reading: 300/6735  of train images\n",
      "Reading: 350/6735  of train images\n",
      "Reading: 400/6735  of train images\n",
      "Reading: 450/6735  of train images\n",
      "Reading: 500/6735  of train images\n",
      "Reading: 550/6735  of train images\n",
      "Reading: 600/6735  of train images\n",
      "Reading: 650/6735  of train images\n",
      "Reading: 700/6735  of train images\n",
      "Reading: 750/6735  of train images\n",
      "Reading: 800/6735  of train images\n",
      "Reading: 850/6735  of train images\n",
      "Reading: 900/6735  of train images\n",
      "Reading: 950/6735  of train images\n",
      "Reading: 1000/6735  of train images\n",
      "Reading: 1050/6735  of train images\n",
      "Reading: 1100/6735  of train images\n",
      "Reading: 1150/6735  of train images\n",
      "Reading: 1200/6735  of train images\n",
      "Reading: 1250/6735  of train images\n",
      "Reading: 1300/6735  of train images\n",
      "Reading: 1350/6735  of train images\n",
      "Reading: 1400/6735  of train images\n",
      "Reading: 1450/6735  of train images\n",
      "Reading: 1500/6735  of train images\n",
      "Reading: 1550/6735  of train images\n",
      "Reading: 1600/6735  of train images\n",
      "Reading: 1650/6735  of train images\n",
      "Reading: 1700/6735  of train images\n",
      "Reading: 1750/6735  of train images\n",
      "Reading: 1800/6735  of train images\n",
      "Reading: 1850/6735  of train images\n",
      "Reading: 1900/6735  of train images\n",
      "Reading: 1950/6735  of train images\n",
      "Reading: 2000/6735  of train images\n",
      "Reading: 2050/6735  of train images\n",
      "Reading: 2100/6735  of train images\n",
      "Reading: 2150/6735  of train images\n",
      "Reading: 2200/6735  of train images\n",
      "Reading: 2250/6735  of train images\n",
      "Reading: 2300/6735  of train images\n",
      "Reading: 2350/6735  of train images\n",
      "Reading: 2400/6735  of train images\n",
      "Reading: 2450/6735  of train images\n",
      "Reading: 2500/6735  of train images\n",
      "Reading: 2550/6735  of train images\n",
      "Reading: 2600/6735  of train images\n",
      "Reading: 2650/6735  of train images\n",
      "Reading: 2700/6735  of train images\n",
      "Reading: 2750/6735  of train images\n",
      "Reading: 2800/6735  of train images\n",
      "Reading: 2850/6735  of train images\n",
      "Reading: 2900/6735  of train images\n",
      "Reading: 2950/6735  of train images\n",
      "Reading: 3000/6735  of train images\n",
      "Reading: 3050/6735  of train images\n",
      "Reading: 3100/6735  of train images\n",
      "Reading: 3150/6735  of train images\n",
      "Reading: 3200/6735  of train images\n",
      "Reading: 3250/6735  of train images\n",
      "Reading: 3300/6735  of train images\n",
      "Reading: 3350/6735  of train images\n",
      "Reading: 3400/6735  of train images\n",
      "Reading: 3450/6735  of train images\n",
      "Reading: 3500/6735  of train images\n",
      "Reading: 3550/6735  of train images\n",
      "Reading: 3600/6735  of train images\n",
      "Reading: 3650/6735  of train images\n",
      "Reading: 3700/6735  of train images\n",
      "Reading: 3750/6735  of train images\n",
      "Reading: 3800/6735  of train images\n",
      "Reading: 3850/6735  of train images\n",
      "Reading: 3900/6735  of train images\n",
      "Reading: 3950/6735  of train images\n",
      "Reading: 4000/6735  of train images\n",
      "Reading: 4050/6735  of train images\n",
      "Reading: 4100/6735  of train images\n",
      "Reading: 4150/6735  of train images\n",
      "Reading: 4200/6735  of train images\n",
      "Reading: 4250/6735  of train images\n",
      "Reading: 4300/6735  of train images\n",
      "Reading: 4350/6735  of train images\n",
      "Reading: 4400/6735  of train images\n",
      "Reading: 4450/6735  of train images\n",
      "Reading: 4500/6735  of train images\n",
      "Reading: 4550/6735  of train images\n",
      "Reading: 4600/6735  of train images\n",
      "Reading: 4650/6735  of train images\n",
      "Reading: 4700/6735  of train images\n",
      "Reading: 4750/6735  of train images\n",
      "Reading: 4800/6735  of train images\n",
      "Reading: 4850/6735  of train images\n",
      "Reading: 4900/6735  of train images\n",
      "Reading: 4950/6735  of train images\n",
      "Reading: 5000/6735  of train images\n",
      "Reading: 5050/6735  of train images\n",
      "Reading: 5100/6735  of train images\n",
      "Reading: 5150/6735  of train images\n",
      "Reading: 5200/6735  of train images\n",
      "Reading: 5250/6735  of train images\n",
      "Reading: 5300/6735  of train images\n",
      "Reading: 5350/6735  of train images\n",
      "Reading: 5400/6735  of train images\n",
      "Reading: 5450/6735  of train images\n",
      "Reading: 5500/6735  of train images\n",
      "Reading: 5550/6735  of train images\n",
      "Reading: 5600/6735  of train images\n",
      "Reading: 5650/6735  of train images\n",
      "Reading: 5700/6735  of train images\n",
      "Reading: 5750/6735  of train images\n",
      "Reading: 5800/6735  of train images\n",
      "Reading: 5850/6735  of train images\n",
      "Reading: 5900/6735  of train images\n",
      "Reading: 5950/6735  of train images\n",
      "Reading: 6000/6735  of train images\n",
      "Reading: 6050/6735  of train images\n",
      "Reading: 6100/6735  of train images\n",
      "Reading: 6150/6735  of train images\n",
      "Reading: 6200/6735  of train images\n",
      "Reading: 6250/6735  of train images\n",
      "Reading: 6300/6735  of train images\n",
      "Reading: 6350/6735  of train images\n",
      "Reading: 6400/6735  of train images\n",
      "Reading: 6450/6735  of train images\n",
      "Reading: 6500/6735  of train images\n",
      "Reading: 6550/6735  of train images\n",
      "Reading: 6600/6735  of train images\n",
      "Reading: 6650/6735  of train images\n",
      "Reading: 6700/6735  of train images\n",
      "Reading: 0/1684  of test images\n",
      "Reading: 50/1684  of test images\n",
      "Reading: 100/1684  of test images\n",
      "Reading: 150/1684  of test images\n",
      "Reading: 200/1684  of test images\n",
      "Reading: 250/1684  of test images\n",
      "Reading: 300/1684  of test images\n",
      "Reading: 350/1684  of test images\n",
      "Reading: 400/1684  of test images\n",
      "Reading: 450/1684  of test images\n",
      "Reading: 500/1684  of test images\n",
      "Reading: 550/1684  of test images\n",
      "Reading: 600/1684  of test images\n",
      "Reading: 650/1684  of test images\n",
      "Reading: 700/1684  of test images\n",
      "Reading: 750/1684  of test images\n",
      "Reading: 800/1684  of test images\n",
      "Reading: 850/1684  of test images\n",
      "Reading: 900/1684  of test images\n",
      "Reading: 950/1684  of test images\n",
      "Reading: 1000/1684  of test images\n",
      "Reading: 1050/1684  of test images\n",
      "Reading: 1100/1684  of test images\n",
      "Reading: 1150/1684  of test images\n",
      "Reading: 1200/1684  of test images\n",
      "Reading: 1250/1684  of test images\n",
      "Reading: 1300/1684  of test images\n",
      "Reading: 1350/1684  of test images\n",
      "Reading: 1400/1684  of test images\n",
      "Reading: 1450/1684  of test images\n",
      "Reading: 1500/1684  of test images\n",
      "Reading: 1550/1684  of test images\n",
      "Reading: 1600/1684  of test images\n",
      "Reading: 1650/1684  of test images\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "base = 16\n",
    "image_size = 256\n",
    "img_ch = 1\n",
    "batch_size =8\n",
    "LR = 0.0001\n",
    "SDRate = 0.5\n",
    "batch_normalization = True\n",
    "spatial_dropout = True\n",
    "metric = 'dice'\n",
    "epochs = 150\n",
    "p = 0.2\n",
    "path = '/Lab1/Lab3/CT/'\n",
    "fold1 = 'Image'\n",
    "fold2 = 'Mask'\n",
    "\n",
    "train_img, train_mask, test_img, test_mask = get_train_test_data(fold1, fold2, path, p,image_size, image_size)\n",
    "model = u_net(base,image_size, image_size, img_ch, batch_normalization, SDRate, spatial_dropout)\n",
    "\n",
    "model.compile(optimizer = Adam(lr=LR), loss = dice_coef_loss, metrics =[dice_coef] )\n",
    "History = model.fit(train_img, train_mask, epochs = epochs, batch_size = batch_size, verbose = 1,\n",
    "                    validation_data = (test_img,test_mask))\n",
    "\n",
    "plotter(History)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.metrics import Recall, Precision\n",
    "\n",
    "#Model parameters\n",
    "base = 16\n",
    "image_size = 256\n",
    "img_ch = 1\n",
    "batch_size =8\n",
    "LR = 0.0001\n",
    "SDRate = 0.5\n",
    "batch_normalization = True\n",
    "spatial_dropout = True\n",
    "metric = 'dice'\n",
    "epochs = 150\n",
    "\n",
    "#Data loader parameters\n",
    "p = 0.2\n",
    "path = '/Lab1/Lab3/CT/'\n",
    "fold1 = 'Image'\n",
    "fold2 = 'Mask'\n",
    "\n",
    "#Data augmentation parameters\n",
    "rotation_range = 10\n",
    "width_shift = 0.1\n",
    "height_shift_range = 0.1,\n",
    "rescale = 1./255\n",
    "horizontal_flip = True\n",
    "\n",
    "#Load the data\n",
    "train_img, train_mask, test_img, test_mask = get_train_test_data(fold1, fold2, path, p,image_size, image_size)\n",
    "\n",
    "#Data augmentation\n",
    "train_datagen, val_datagen = DataAugmentation(rotation_range,width_shift,height_shift_range,rescale,horizontal_flip)\n",
    "\n",
    "\n",
    "#Build the model\n",
    "model = u_net(base,image_size, image_size, img_ch, batch_normalization, SDRate, spatial_dropout)\n",
    "\n",
    "#Compile the model\n",
    "model.compile(optimizer = Adam(lr=LR), loss = dice_coef_loss, metrics =[dice_coef, Recall(), Precision()] )\n",
    "\n",
    "#Fit the data into the model\n",
    "History = model.fit_generator(train_datagen.flow(train_img, train_mask,batch_size = batch_size), validation_data = val_datagen.flow(test_img, test_mask), epochs = epochs, verbose = 1)        \n",
    "\n",
    "#Plot results\n",
    "#Training vs Validation Learning loss \n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.title(\"Learning curve\")\n",
    "plt.plot(History.history[\"loss\"], label=\"loss\")\n",
    "plt.plot(History.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot( np.argmin(History.history[\"val_loss\"]),\n",
    "         np.min(History.history[\"val_loss\"]),\n",
    "         marker=\"x\", color=\"r\", label=\"best model\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss Value\")\n",
    "plt.legend(); \n",
    "\n",
    "#Train and test accuracy plot\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.title(\"Dice Score Curve\")\n",
    "plt.plot(History.history[\"dice_coef\"], label=\"dice_coef\")\n",
    "plt.plot(History.history[\"val_dice_coef\"], label=\"val_dice_coef\")\n",
    "plt.plot(History.history[\"val_recall\"], label=\"val_recall\")\n",
    "plt.plot(History.history[\"val_precision\"], label=\"val_precision\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Dice Coef')\n",
    "plt.legend(); \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: 0/6735  of train images\n",
      "Reading: 50/6735  of train images\n",
      "Reading: 100/6735  of train images\n",
      "Reading: 150/6735  of train images\n",
      "Reading: 200/6735  of train images\n",
      "Reading: 250/6735  of train images\n",
      "Reading: 300/6735  of train images\n",
      "Reading: 350/6735  of train images\n",
      "Reading: 400/6735  of train images\n",
      "Reading: 450/6735  of train images\n",
      "Reading: 500/6735  of train images\n",
      "Reading: 550/6735  of train images\n",
      "Reading: 600/6735  of train images\n",
      "Reading: 650/6735  of train images\n",
      "Reading: 700/6735  of train images\n",
      "Reading: 750/6735  of train images\n",
      "Reading: 800/6735  of train images\n",
      "Reading: 850/6735  of train images\n",
      "Reading: 900/6735  of train images\n",
      "Reading: 950/6735  of train images\n",
      "Reading: 1000/6735  of train images\n",
      "Reading: 1050/6735  of train images\n",
      "Reading: 1100/6735  of train images\n",
      "Reading: 1150/6735  of train images\n",
      "Reading: 1200/6735  of train images\n",
      "Reading: 1250/6735  of train images\n",
      "Reading: 1300/6735  of train images\n",
      "Reading: 1350/6735  of train images\n",
      "Reading: 1400/6735  of train images\n",
      "Reading: 1450/6735  of train images\n",
      "Reading: 1500/6735  of train images\n",
      "Reading: 1550/6735  of train images\n",
      "Reading: 1600/6735  of train images\n",
      "Reading: 1650/6735  of train images\n",
      "Reading: 1700/6735  of train images\n",
      "Reading: 1750/6735  of train images\n",
      "Reading: 1800/6735  of train images\n",
      "Reading: 1850/6735  of train images\n",
      "Reading: 1900/6735  of train images\n",
      "Reading: 1950/6735  of train images\n",
      "Reading: 2000/6735  of train images\n",
      "Reading: 2050/6735  of train images\n",
      "Reading: 2100/6735  of train images\n",
      "Reading: 2150/6735  of train images\n",
      "Reading: 2200/6735  of train images\n",
      "Reading: 2250/6735  of train images\n",
      "Reading: 2300/6735  of train images\n",
      "Reading: 2350/6735  of train images\n",
      "Reading: 2400/6735  of train images\n",
      "Reading: 2450/6735  of train images\n",
      "Reading: 2500/6735  of train images\n",
      "Reading: 2550/6735  of train images\n",
      "Reading: 2600/6735  of train images\n",
      "Reading: 2650/6735  of train images\n",
      "Reading: 2700/6735  of train images\n",
      "Reading: 2750/6735  of train images\n",
      "Reading: 2800/6735  of train images\n",
      "Reading: 2850/6735  of train images\n",
      "Reading: 2900/6735  of train images\n",
      "Reading: 2950/6735  of train images\n",
      "Reading: 3000/6735  of train images\n",
      "Reading: 3050/6735  of train images\n",
      "Reading: 3100/6735  of train images\n",
      "Reading: 3150/6735  of train images\n",
      "Reading: 3200/6735  of train images\n",
      "Reading: 3250/6735  of train images\n",
      "Reading: 3300/6735  of train images\n",
      "Reading: 3350/6735  of train images\n",
      "Reading: 3400/6735  of train images\n",
      "Reading: 3450/6735  of train images\n",
      "Reading: 3500/6735  of train images\n",
      "Reading: 3550/6735  of train images\n",
      "Reading: 3600/6735  of train images\n",
      "Reading: 3650/6735  of train images\n",
      "Reading: 3700/6735  of train images\n",
      "Reading: 3750/6735  of train images\n",
      "Reading: 3800/6735  of train images\n",
      "Reading: 3850/6735  of train images\n",
      "Reading: 3900/6735  of train images\n",
      "Reading: 3950/6735  of train images\n",
      "Reading: 4000/6735  of train images\n",
      "Reading: 4050/6735  of train images\n",
      "Reading: 4100/6735  of train images\n",
      "Reading: 4150/6735  of train images\n",
      "Reading: 4200/6735  of train images\n",
      "Reading: 4250/6735  of train images\n",
      "Reading: 4300/6735  of train images\n",
      "Reading: 4350/6735  of train images\n",
      "Reading: 4400/6735  of train images\n",
      "Reading: 4450/6735  of train images\n",
      "Reading: 4500/6735  of train images\n",
      "Reading: 4550/6735  of train images\n",
      "Reading: 4600/6735  of train images\n",
      "Reading: 4650/6735  of train images\n",
      "Reading: 4700/6735  of train images\n",
      "Reading: 4750/6735  of train images\n",
      "Reading: 4800/6735  of train images\n",
      "Reading: 4850/6735  of train images\n",
      "Reading: 4900/6735  of train images\n",
      "Reading: 4950/6735  of train images\n",
      "Reading: 5000/6735  of train images\n",
      "Reading: 5050/6735  of train images\n",
      "Reading: 5100/6735  of train images\n",
      "Reading: 5150/6735  of train images\n",
      "Reading: 5200/6735  of train images\n",
      "Reading: 5250/6735  of train images\n",
      "Reading: 5300/6735  of train images\n",
      "Reading: 5350/6735  of train images\n",
      "Reading: 5400/6735  of train images\n",
      "Reading: 5450/6735  of train images\n",
      "Reading: 5500/6735  of train images\n",
      "Reading: 5550/6735  of train images\n",
      "Reading: 5600/6735  of train images\n",
      "Reading: 5650/6735  of train images\n",
      "Reading: 5700/6735  of train images\n",
      "Reading: 5750/6735  of train images\n",
      "Reading: 5800/6735  of train images\n",
      "Reading: 5850/6735  of train images\n",
      "Reading: 5900/6735  of train images\n",
      "Reading: 5950/6735  of train images\n",
      "Reading: 6000/6735  of train images\n",
      "Reading: 6050/6735  of train images\n",
      "Reading: 6100/6735  of train images\n",
      "Reading: 6150/6735  of train images\n",
      "Reading: 6200/6735  of train images\n",
      "Reading: 6250/6735  of train images\n",
      "Reading: 6300/6735  of train images\n",
      "Reading: 6350/6735  of train images\n",
      "Reading: 6400/6735  of train images\n",
      "Reading: 6450/6735  of train images\n",
      "Reading: 6500/6735  of train images\n",
      "Reading: 6550/6735  of train images\n",
      "Reading: 6600/6735  of train images\n",
      "Reading: 6650/6735  of train images\n",
      "Reading: 6700/6735  of train images\n",
      "Reading: 0/1684  of test images\n",
      "Reading: 50/1684  of test images\n",
      "Reading: 100/1684  of test images\n",
      "Reading: 150/1684  of test images\n",
      "Reading: 200/1684  of test images\n",
      "Reading: 250/1684  of test images\n",
      "Reading: 300/1684  of test images\n",
      "Reading: 350/1684  of test images\n",
      "Reading: 400/1684  of test images\n",
      "Reading: 450/1684  of test images\n",
      "Reading: 500/1684  of test images\n",
      "Reading: 550/1684  of test images\n",
      "Reading: 600/1684  of test images\n",
      "Reading: 650/1684  of test images\n",
      "Reading: 700/1684  of test images\n",
      "Reading: 750/1684  of test images\n",
      "Reading: 800/1684  of test images\n",
      "Reading: 850/1684  of test images\n",
      "Reading: 900/1684  of test images\n",
      "Reading: 950/1684  of test images\n",
      "Reading: 1000/1684  of test images\n",
      "Reading: 1050/1684  of test images\n",
      "Reading: 1100/1684  of test images\n",
      "Reading: 1150/1684  of test images\n",
      "Reading: 1200/1684  of test images\n",
      "Reading: 1250/1684  of test images\n",
      "Reading: 1300/1684  of test images\n",
      "Reading: 1350/1684  of test images\n",
      "Reading: 1400/1684  of test images\n",
      "Reading: 1450/1684  of test images\n",
      "Reading: 1500/1684  of test images\n",
      "Reading: 1550/1684  of test images\n",
      "Reading: 1600/1684  of test images\n",
      "Reading: 1650/1684  of test images\n",
      "(6735, 256, 256, 1)\n",
      "(1684, 256, 256, 1)\n",
      "building model\n",
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            [(None, 256, 256, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_108 (Conv2D)             (None, 256, 256, 16) 160         input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_85 (Batc (None, 256, 256, 16) 64          conv2d_108[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_108 (Activation)     (None, 256, 256, 16) 0           batch_normalization_v2_85[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_96 (SpatialDr (None, 256, 256, 16) 0           activation_108[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_109 (Conv2D)             (None, 256, 256, 16) 2320        spatial_dropout2d_96[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_86 (Batc (None, 256, 256, 16) 64          conv2d_109[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_109 (Activation)     (None, 256, 256, 16) 0           batch_normalization_v2_86[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_97 (SpatialDr (None, 256, 256, 16) 0           activation_109[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_24 (MaxPooling2D) (None, 128, 128, 16) 0           spatial_dropout2d_97[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_110 (Conv2D)             (None, 128, 128, 32) 4640        max_pooling2d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_87 (Batc (None, 128, 128, 32) 128         conv2d_110[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_110 (Activation)     (None, 128, 128, 32) 0           batch_normalization_v2_87[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_98 (SpatialDr (None, 128, 128, 32) 0           activation_110[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_111 (Conv2D)             (None, 128, 128, 32) 9248        spatial_dropout2d_98[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_88 (Batc (None, 128, 128, 32) 128         conv2d_111[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_111 (Activation)     (None, 128, 128, 32) 0           batch_normalization_v2_88[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_99 (SpatialDr (None, 128, 128, 32) 0           activation_111[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_25 (MaxPooling2D) (None, 64, 64, 32)   0           spatial_dropout2d_99[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_112 (Conv2D)             (None, 64, 64, 64)   18496       max_pooling2d_25[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_89 (Batc (None, 64, 64, 64)   256         conv2d_112[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_112 (Activation)     (None, 64, 64, 64)   0           batch_normalization_v2_89[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_100 (SpatialD (None, 64, 64, 64)   0           activation_112[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_113 (Conv2D)             (None, 64, 64, 64)   36928       spatial_dropout2d_100[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_90 (Batc (None, 64, 64, 64)   256         conv2d_113[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_113 (Activation)     (None, 64, 64, 64)   0           batch_normalization_v2_90[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_101 (SpatialD (None, 64, 64, 64)   0           activation_113[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_26 (MaxPooling2D) (None, 32, 32, 64)   0           spatial_dropout2d_101[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_114 (Conv2D)             (None, 32, 32, 128)  73856       max_pooling2d_26[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_91 (Batc (None, 32, 32, 128)  512         conv2d_114[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_114 (Activation)     (None, 32, 32, 128)  0           batch_normalization_v2_91[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_102 (SpatialD (None, 32, 32, 128)  0           activation_114[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_115 (Conv2D)             (None, 32, 32, 128)  147584      spatial_dropout2d_102[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_92 (Batc (None, 32, 32, 128)  512         conv2d_115[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_115 (Activation)     (None, 32, 32, 128)  0           batch_normalization_v2_92[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_103 (SpatialD (None, 32, 32, 128)  0           activation_115[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_27 (MaxPooling2D) (None, 16, 16, 128)  0           spatial_dropout2d_103[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_116 (Conv2D)             (None, 16, 16, 256)  295168      max_pooling2d_27[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_93 (Batc (None, 16, 16, 256)  1024        conv2d_116[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_116 (Activation)     (None, 16, 16, 256)  0           batch_normalization_v2_93[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_104 (SpatialD (None, 16, 16, 256)  0           activation_116[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_24 (Conv2DTran (None, 32, 32, 128)  131200      spatial_dropout2d_104[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_24 (Concatenate)    (None, 32, 32, 256)  0           spatial_dropout2d_103[0][0]      \n",
      "                                                                 conv2d_transpose_24[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_117 (Conv2D)             (None, 32, 32, 128)  295040      concatenate_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_94 (Batc (None, 32, 32, 128)  512         conv2d_117[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_117 (Activation)     (None, 32, 32, 128)  0           batch_normalization_v2_94[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_105 (SpatialD (None, 32, 32, 128)  0           activation_117[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_118 (Conv2D)             (None, 32, 32, 128)  147584      spatial_dropout2d_105[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_95 (Batc (None, 32, 32, 128)  512         conv2d_118[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_118 (Activation)     (None, 32, 32, 128)  0           batch_normalization_v2_95[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_106 (SpatialD (None, 32, 32, 128)  0           activation_118[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_25 (Conv2DTran (None, 64, 64, 64)   32832       spatial_dropout2d_106[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_25 (Concatenate)    (None, 64, 64, 128)  0           spatial_dropout2d_101[0][0]      \n",
      "                                                                 conv2d_transpose_25[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_119 (Conv2D)             (None, 64, 64, 64)   73792       concatenate_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_96 (Batc (None, 64, 64, 64)   256         conv2d_119[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_119 (Activation)     (None, 64, 64, 64)   0           batch_normalization_v2_96[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_107 (SpatialD (None, 64, 64, 64)   0           activation_119[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_120 (Conv2D)             (None, 64, 64, 64)   36928       spatial_dropout2d_107[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_97 (Batc (None, 64, 64, 64)   256         conv2d_120[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_120 (Activation)     (None, 64, 64, 64)   0           batch_normalization_v2_97[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_108 (SpatialD (None, 64, 64, 64)   0           activation_120[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_26 (Conv2DTran (None, 128, 128, 32) 8224        spatial_dropout2d_108[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_26 (Concatenate)    (None, 128, 128, 64) 0           spatial_dropout2d_99[0][0]       \n",
      "                                                                 conv2d_transpose_26[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_121 (Conv2D)             (None, 128, 128, 32) 18464       concatenate_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_98 (Batc (None, 128, 128, 32) 128         conv2d_121[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_121 (Activation)     (None, 128, 128, 32) 0           batch_normalization_v2_98[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_109 (SpatialD (None, 128, 128, 32) 0           activation_121[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_122 (Conv2D)             (None, 128, 128, 32) 9248        spatial_dropout2d_109[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_99 (Batc (None, 128, 128, 32) 128         conv2d_122[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_122 (Activation)     (None, 128, 128, 32) 0           batch_normalization_v2_99[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_110 (SpatialD (None, 128, 128, 32) 0           activation_122[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_27 (Conv2DTran (None, 256, 256, 16) 2064        spatial_dropout2d_110[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_27 (Concatenate)    (None, 256, 256, 32) 0           spatial_dropout2d_97[0][0]       \n",
      "                                                                 conv2d_transpose_27[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_123 (Conv2D)             (None, 256, 256, 16) 4624        concatenate_27[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_100 (Bat (None, 256, 256, 16) 64          conv2d_123[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_123 (Activation)     (None, 256, 256, 16) 0           batch_normalization_v2_100[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_111 (SpatialD (None, 256, 256, 16) 0           activation_123[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_124 (Conv2D)             (None, 256, 256, 16) 2320        spatial_dropout2d_111[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_101 (Bat (None, 256, 256, 16) 64          conv2d_124[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_124 (Activation)     (None, 256, 256, 16) 0           batch_normalization_v2_101[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_125 (Conv2D)             (None, 256, 256, 3)  435         activation_124[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_125 (Activation)     (None, 256, 256, 3)  0           conv2d_125[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 1,356,019\n",
      "Trainable params: 1,353,587\n",
      "Non-trainable params: 2,432\n",
      "__________________________________________________________________________________________________\n",
      "here\n",
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "842/842 [==============================] - 106s 126ms/step - loss: -0.8692 - dice_coef: 0.8692 - recall_1: 0.9280 - precision_1: 0.9743 - val_loss: -0.9629 - val_dice_coef: 0.9629 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 2/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9927 - dice_coef: 0.9927 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -0.9874 - val_dice_coef: 0.9874 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 3/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9975 - dice_coef: 0.9975 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -0.9933 - val_dice_coef: 0.9933 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 4/150\n",
      "842/842 [==============================] - 56s 67ms/step - loss: -0.9987 - dice_coef: 0.9987 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -0.9963 - val_dice_coef: 0.9963 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 5/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9993 - dice_coef: 0.9993 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -0.9977 - val_dice_coef: 0.9977 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 6/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9995 - dice_coef: 0.9995 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -0.9984 - val_dice_coef: 0.9984 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 7/150\n",
      "842/842 [==============================] - 56s 67ms/step - loss: -0.9996 - dice_coef: 0.9996 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -0.9990 - val_dice_coef: 0.9990 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 8/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9997 - dice_coef: 0.9997 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -0.9993 - val_dice_coef: 0.9993 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 9/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -0.9995 - val_dice_coef: 0.9995 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 10/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -0.9997 - val_dice_coef: 0.9997 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 11/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -0.9998 - val_dice_coef: 0.9998 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 12/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -0.9999 - val_dice_coef: 0.9999 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 13/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -0.9999 - val_dice_coef: 0.9999 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 14/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -0.9999 - val_dice_coef: 0.9999 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 15/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 16/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 17/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 18/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 19/150\n",
      "842/842 [==============================] - 55s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 20/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 21/150\n",
      "842/842 [==============================] - 55s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 22/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 23/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 24/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 25/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 26/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 27/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 28/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 29/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 30/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 31/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 32/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 33/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 34/150\n",
      "842/842 [==============================] - 56s 67ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 35/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 36/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 37/150\n",
      "842/842 [==============================] - 56s 67ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 38/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 39/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 40/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 41/150\n",
      "842/842 [==============================] - 55s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 42/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 43/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 44/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 45/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 46/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 47/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 48/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 49/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 50/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 51/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 52/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 53/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 54/150\n",
      "842/842 [==============================] - 56s 67ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 55/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 56/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 57/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 58/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 59/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 60/150\n",
      "842/842 [==============================] - 56s 67ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 61/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 62/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 63/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 64/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 65/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 66/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 67/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 68/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 69/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 70/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 71/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 72/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 73/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 74/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 75/150\n",
      "842/842 [==============================] - 56s 67ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 76/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 77/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 78/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 79/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 80/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 81/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 82/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 83/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 84/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 85/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 86/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 87/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 88/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 89/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 90/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 91/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 92/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 93/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 94/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 95/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 96/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 97/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 98/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 99/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 100/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 101/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 102/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 103/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 104/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 105/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 106/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 107/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 108/150\n",
      "842/842 [==============================] - 56s 67ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 109/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 110/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 111/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 112/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 113/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 114/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 115/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 116/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 117/150\n",
      "842/842 [==============================] - 55s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 118/150\n",
      "842/842 [==============================] - 55s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 119/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 120/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 121/150\n",
      "842/842 [==============================] - 55s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 122/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 123/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 124/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 125/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 126/150\n",
      "842/842 [==============================] - 55s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 127/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 128/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 129/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 130/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 131/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 132/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 133/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 134/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 135/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 136/150\n",
      "842/842 [==============================] - 55s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 137/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 138/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 139/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 140/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 141/150\n",
      "842/842 [==============================] - 56s 67ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 142/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 143/150\n",
      "842/842 [==============================] - 55s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 144/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 145/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 146/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 147/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 148/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 149/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n",
      "Epoch 150/150\n",
      "842/842 [==============================] - 56s 66ms/step - loss: -0.9998 - dice_coef: 0.9998 - recall_1: 0.9998 - precision_1: 0.9998 - val_loss: -1.0000 - val_dice_coef: 1.0000 - val_recall_1: 1.0000 - val_precision_1: 1.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.metrics import Recall, Precision\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "#Model parameters\n",
    "base = 16\n",
    "image_size = 256\n",
    "img_ch = 1\n",
    "batch_size =8\n",
    "LR = 0.0001\n",
    "SDRate = 0.5\n",
    "batch_normalization = True\n",
    "spatial_dropout = True\n",
    "epochs = 150\n",
    "\n",
    "#Data loader parameters\n",
    "p = 0.2\n",
    "path = '/Lab1/Lab3/CT/'\n",
    "fold1 = 'Image'\n",
    "fold2 = 'Mask'\n",
    "\n",
    "#Data augmentation parameters\n",
    "rotation_range = 10\n",
    "width_shift = 0.1\n",
    "height_shift_range = 0.1,\n",
    "rescale = 1./255\n",
    "horizontal_flip = True\n",
    "\n",
    "#Load the data\n",
    "train_img, train_mask, test_img, test_mask = get_train_test_data(fold1, fold2, path, p,image_size, image_size)\n",
    "\n",
    "#To one-hot-encoding\n",
    "train_mask = to_categorical(train_mask, num_classes=3)\n",
    "test_mask = to_categorical(test_mask, num_classes=3)\n",
    "\n",
    "#Data augmentation\n",
    "train_datagen, val_datagen = DataAugmentation(rotation_range,width_shift,height_shift_range,rescale,horizontal_flip)\n",
    "\n",
    "#Build the multi-classification model\n",
    "model = u_net_3labels(base,image_size, image_size, img_ch, batch_normalization, SDRate, spatial_dropout)\n",
    "\n",
    "#Compile the model\n",
    "model.compile(optimizer = Adam(lr=LR), loss = dice_coef_loss, metrics =[dice_coef, Recall(), Precision()] )\n",
    "\n",
    "#Fit the data into the model\n",
    "History = model.fit_generator(train_datagen.flow(train_img, train_mask,batch_size = batch_size), validation_data = val_datagen.flow(test_img, test_mask), epochs = epochs, verbose = 1)          \n",
    "\n",
    "#Plot results\n",
    "dice = True\n",
    "recall = True\n",
    "precision = True\n",
    "plotter(History)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: 0/7522  of train images\n",
      "Reading: 50/7522  of train images\n",
      "Reading: 100/7522  of train images\n",
      "Reading: 150/7522  of train images\n",
      "Reading: 200/7522  of train images\n",
      "Reading: 250/7522  of train images\n",
      "Reading: 300/7522  of train images\n",
      "Reading: 350/7522  of train images\n",
      "Reading: 400/7522  of train images\n",
      "Reading: 450/7522  of train images\n",
      "Reading: 500/7522  of train images\n",
      "Reading: 550/7522  of train images\n",
      "Reading: 600/7522  of train images\n",
      "Reading: 650/7522  of train images\n",
      "Reading: 700/7522  of train images\n",
      "Reading: 750/7522  of train images\n",
      "Reading: 800/7522  of train images\n",
      "Reading: 850/7522  of train images\n",
      "Reading: 900/7522  of train images\n",
      "Reading: 950/7522  of train images\n",
      "Reading: 1000/7522  of train images\n",
      "Reading: 1050/7522  of train images\n",
      "Reading: 1100/7522  of train images\n",
      "Reading: 1150/7522  of train images\n",
      "Reading: 1200/7522  of train images\n",
      "Reading: 1250/7522  of train images\n",
      "Reading: 1300/7522  of train images\n",
      "Reading: 1350/7522  of train images\n",
      "Reading: 1400/7522  of train images\n",
      "Reading: 1450/7522  of train images\n",
      "Reading: 1500/7522  of train images\n",
      "Reading: 1550/7522  of train images\n",
      "Reading: 1600/7522  of train images\n",
      "Reading: 1650/7522  of train images\n",
      "Reading: 1700/7522  of train images\n",
      "Reading: 1750/7522  of train images\n",
      "Reading: 1800/7522  of train images\n",
      "Reading: 1850/7522  of train images\n",
      "Reading: 1900/7522  of train images\n",
      "Reading: 1950/7522  of train images\n",
      "Reading: 2000/7522  of train images\n",
      "Reading: 2050/7522  of train images\n",
      "Reading: 2100/7522  of train images\n",
      "Reading: 2150/7522  of train images\n",
      "Reading: 2200/7522  of train images\n",
      "Reading: 2250/7522  of train images\n",
      "Reading: 2300/7522  of train images\n",
      "Reading: 2350/7522  of train images\n",
      "Reading: 2400/7522  of train images\n",
      "Reading: 2450/7522  of train images\n",
      "Reading: 2500/7522  of train images\n",
      "Reading: 2550/7522  of train images\n",
      "Reading: 2600/7522  of train images\n",
      "Reading: 2650/7522  of train images\n",
      "Reading: 2700/7522  of train images\n",
      "Reading: 2750/7522  of train images\n",
      "Reading: 2800/7522  of train images\n",
      "Reading: 2850/7522  of train images\n",
      "Reading: 2900/7522  of train images\n",
      "Reading: 2950/7522  of train images\n",
      "Reading: 3000/7522  of train images\n",
      "Reading: 3050/7522  of train images\n",
      "Reading: 3100/7522  of train images\n",
      "Reading: 3150/7522  of train images\n",
      "Reading: 3200/7522  of train images\n",
      "Reading: 3250/7522  of train images\n",
      "Reading: 3300/7522  of train images\n",
      "Reading: 3350/7522  of train images\n",
      "Reading: 3400/7522  of train images\n",
      "Reading: 3450/7522  of train images\n",
      "Reading: 3500/7522  of train images\n",
      "Reading: 3550/7522  of train images\n",
      "Reading: 3600/7522  of train images\n",
      "Reading: 3650/7522  of train images\n",
      "Reading: 3700/7522  of train images\n",
      "Reading: 3750/7522  of train images\n",
      "Reading: 3800/7522  of train images\n",
      "Reading: 3850/7522  of train images\n",
      "Reading: 3900/7522  of train images\n",
      "Reading: 3950/7522  of train images\n",
      "Reading: 4000/7522  of train images\n",
      "Reading: 4050/7522  of train images\n",
      "Reading: 4100/7522  of train images\n",
      "Reading: 4150/7522  of train images\n",
      "Reading: 4200/7522  of train images\n",
      "Reading: 4250/7522  of train images\n",
      "Reading: 4300/7522  of train images\n",
      "Reading: 4350/7522  of train images\n",
      "Reading: 4400/7522  of train images\n",
      "Reading: 4450/7522  of train images\n",
      "Reading: 4500/7522  of train images\n",
      "Reading: 4550/7522  of train images\n",
      "Reading: 4600/7522  of train images\n",
      "Reading: 4650/7522  of train images\n",
      "Reading: 4700/7522  of train images\n",
      "Reading: 4750/7522  of train images\n",
      "Reading: 4800/7522  of train images\n",
      "Reading: 4850/7522  of train images\n",
      "Reading: 4900/7522  of train images\n",
      "Reading: 4950/7522  of train images\n",
      "Reading: 5000/7522  of train images\n",
      "Reading: 5050/7522  of train images\n",
      "Reading: 5100/7522  of train images\n",
      "Reading: 5150/7522  of train images\n",
      "Reading: 5200/7522  of train images\n",
      "Reading: 5250/7522  of train images\n",
      "Reading: 5300/7522  of train images\n",
      "Reading: 5350/7522  of train images\n",
      "Reading: 5400/7522  of train images\n",
      "Reading: 5450/7522  of train images\n",
      "Reading: 5500/7522  of train images\n",
      "Reading: 5550/7522  of train images\n",
      "Reading: 5600/7522  of train images\n",
      "Reading: 5650/7522  of train images\n",
      "Reading: 5700/7522  of train images\n",
      "Reading: 5750/7522  of train images\n",
      "Reading: 5800/7522  of train images\n",
      "Reading: 5850/7522  of train images\n",
      "Reading: 5900/7522  of train images\n",
      "Reading: 5950/7522  of train images\n",
      "Reading: 6000/7522  of train images\n",
      "Reading: 6050/7522  of train images\n",
      "Reading: 6100/7522  of train images\n",
      "Reading: 6150/7522  of train images\n",
      "Reading: 6200/7522  of train images\n",
      "Reading: 6250/7522  of train images\n",
      "Reading: 6300/7522  of train images\n",
      "Reading: 6350/7522  of train images\n",
      "Reading: 6400/7522  of train images\n",
      "Reading: 6450/7522  of train images\n",
      "Reading: 6500/7522  of train images\n",
      "Reading: 6550/7522  of train images\n",
      "Reading: 6600/7522  of train images\n",
      "Reading: 6650/7522  of train images\n",
      "Reading: 6700/7522  of train images\n",
      "Reading: 6750/7522  of train images\n",
      "Reading: 6800/7522  of train images\n",
      "Reading: 6850/7522  of train images\n",
      "Reading: 6900/7522  of train images\n",
      "Reading: 6950/7522  of train images\n",
      "Reading: 7000/7522  of train images\n",
      "Reading: 7050/7522  of train images\n",
      "Reading: 7100/7522  of train images\n",
      "Reading: 7150/7522  of train images\n",
      "Reading: 7200/7522  of train images\n",
      "Reading: 7250/7522  of train images\n",
      "Reading: 7300/7522  of train images\n",
      "Reading: 7350/7522  of train images\n",
      "Reading: 7400/7522  of train images\n",
      "Reading: 7450/7522  of train images\n",
      "Reading: 7500/7522  of train images\n",
      "Reading: 0/1881  of test images\n",
      "Reading: 50/1881  of test images\n",
      "Reading: 100/1881  of test images\n",
      "Reading: 150/1881  of test images\n",
      "Reading: 200/1881  of test images\n",
      "Reading: 250/1881  of test images\n",
      "Reading: 300/1881  of test images\n",
      "Reading: 350/1881  of test images\n",
      "Reading: 400/1881  of test images\n",
      "Reading: 450/1881  of test images\n",
      "Reading: 500/1881  of test images\n",
      "Reading: 550/1881  of test images\n",
      "Reading: 600/1881  of test images\n",
      "Reading: 650/1881  of test images\n",
      "Reading: 700/1881  of test images\n",
      "Reading: 750/1881  of test images\n",
      "Reading: 800/1881  of test images\n",
      "Reading: 850/1881  of test images\n",
      "Reading: 900/1881  of test images\n",
      "Reading: 950/1881  of test images\n",
      "Reading: 1000/1881  of test images\n",
      "Reading: 1050/1881  of test images\n",
      "Reading: 1100/1881  of test images\n",
      "Reading: 1150/1881  of test images\n",
      "Reading: 1200/1881  of test images\n",
      "Reading: 1250/1881  of test images\n",
      "Reading: 1300/1881  of test images\n",
      "Reading: 1350/1881  of test images\n",
      "Reading: 1400/1881  of test images\n",
      "Reading: 1450/1881  of test images\n",
      "Reading: 1500/1881  of test images\n",
      "Reading: 1550/1881  of test images\n",
      "Reading: 1600/1881  of test images\n",
      "Reading: 1650/1881  of test images\n",
      "Reading: 1700/1881  of test images\n",
      "Reading: 1750/1881  of test images\n",
      "Reading: 1800/1881  of test images\n",
      "Reading: 1850/1881  of test images\n",
      "(7522, 96, 96, 1)\n",
      "(1881, 96, 96, 1)\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 96, 96, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 96, 96, 16)   160         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_17 (Batc (None, 96, 96, 16)   64          conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 96, 96, 16)   0           batch_normalization_v2_17[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_16 (SpatialDr (None, 96, 96, 16)   0           activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 96, 96, 16)   2320        spatial_dropout2d_16[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_18 (Batc (None, 96, 96, 16)   64          conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 96, 96, 16)   0           batch_normalization_v2_18[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_17 (SpatialDr (None, 96, 96, 16)   0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 48, 48, 16)   0           spatial_dropout2d_17[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 48, 48, 32)   4640        max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_19 (Batc (None, 48, 48, 32)   128         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 48, 48, 32)   0           batch_normalization_v2_19[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_18 (SpatialDr (None, 48, 48, 32)   0           activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 48, 48, 32)   9248        spatial_dropout2d_18[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_20 (Batc (None, 48, 48, 32)   128         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 48, 48, 32)   0           batch_normalization_v2_20[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_19 (SpatialDr (None, 48, 48, 32)   0           activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 24, 24, 32)   0           spatial_dropout2d_19[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 24, 24, 64)   18496       max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_21 (Batc (None, 24, 24, 64)   256         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 24, 24, 64)   0           batch_normalization_v2_21[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_20 (SpatialDr (None, 24, 24, 64)   0           activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 24, 24, 64)   36928       spatial_dropout2d_20[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_22 (Batc (None, 24, 24, 64)   256         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 24, 24, 64)   0           batch_normalization_v2_22[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_21 (SpatialDr (None, 24, 24, 64)   0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 12, 12, 64)   0           spatial_dropout2d_21[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 12, 12, 128)  73856       max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_23 (Batc (None, 12, 12, 128)  512         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 12, 12, 128)  0           batch_normalization_v2_23[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_22 (SpatialDr (None, 12, 12, 128)  0           activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 12, 12, 128)  147584      spatial_dropout2d_22[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_24 (Batc (None, 12, 12, 128)  512         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 12, 12, 128)  0           batch_normalization_v2_24[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_23 (SpatialDr (None, 12, 12, 128)  0           activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 6, 6, 128)    0           spatial_dropout2d_23[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 6, 6, 256)    295168      max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_25 (Batc (None, 6, 6, 256)    1024        conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 6, 6, 256)    0           batch_normalization_v2_25[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_24 (SpatialDr (None, 6, 6, 256)    0           activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTrans (None, 12, 12, 128)  131200      spatial_dropout2d_24[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 12, 12, 256)  0           spatial_dropout2d_23[0][0]       \n",
      "                                                                 conv2d_transpose_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 12, 12, 128)  295040      concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_26 (Batc (None, 12, 12, 128)  512         conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 12, 12, 128)  0           batch_normalization_v2_26[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_25 (SpatialDr (None, 12, 12, 128)  0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 12, 12, 128)  147584      spatial_dropout2d_25[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_27 (Batc (None, 12, 12, 128)  512         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 12, 12, 128)  0           batch_normalization_v2_27[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_26 (SpatialDr (None, 12, 12, 128)  0           activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTrans (None, 24, 24, 64)   32832       spatial_dropout2d_26[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 24, 24, 128)  0           spatial_dropout2d_21[0][0]       \n",
      "                                                                 conv2d_transpose_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 24, 24, 64)   73792       concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_28 (Batc (None, 24, 24, 64)   256         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 24, 24, 64)   0           batch_normalization_v2_28[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_27 (SpatialDr (None, 24, 24, 64)   0           activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 24, 24, 64)   36928       spatial_dropout2d_27[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_29 (Batc (None, 24, 24, 64)   256         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 24, 24, 64)   0           batch_normalization_v2_29[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_28 (SpatialDr (None, 24, 24, 64)   0           activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_6 (Conv2DTrans (None, 48, 48, 32)   8224        spatial_dropout2d_28[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 48, 48, 64)   0           spatial_dropout2d_19[0][0]       \n",
      "                                                                 conv2d_transpose_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 48, 48, 32)   18464       concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_30 (Batc (None, 48, 48, 32)   128         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 48, 48, 32)   0           batch_normalization_v2_30[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_29 (SpatialDr (None, 48, 48, 32)   0           activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 48, 48, 32)   9248        spatial_dropout2d_29[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_31 (Batc (None, 48, 48, 32)   128         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 48, 48, 32)   0           batch_normalization_v2_31[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_30 (SpatialDr (None, 48, 48, 32)   0           activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_7 (Conv2DTrans (None, 96, 96, 16)   2064        spatial_dropout2d_30[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 96, 96, 32)   0           spatial_dropout2d_17[0][0]       \n",
      "                                                                 conv2d_transpose_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 96, 96, 16)   4624        concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_32 (Batc (None, 96, 96, 16)   64          conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 96, 96, 16)   0           batch_normalization_v2_32[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout2d_31 (SpatialDr (None, 96, 96, 16)   0           activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 96, 96, 16)   2320        spatial_dropout2d_31[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_v2_33 (Batc (None, 96, 96, 16)   64          conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 96, 96, 16)   0           batch_normalization_v2_33[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 96, 96, 1)    145         activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 96, 96, 1)    0           conv2d_35[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,355,729\n",
      "Trainable params: 1,353,297\n",
      "Non-trainable params: 2,432\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "1881/1881 [==============================] - 73s 39ms/step - loss: 0.3547 - dice_coef: 0.6453 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.4733 - val_dice_coef: 0.5267 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 2/80\n",
      "1881/1881 [==============================] - 50s 27ms/step - loss: 0.0023 - dice_coef: 0.9977 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.1254 - val_dice_coef: 0.8746 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 3/80\n",
      "1881/1881 [==============================] - 51s 27ms/step - loss: 5.0770e-04 - dice_coef: 0.9995 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0496 - val_dice_coef: 0.9504 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 4/80\n",
      "1881/1881 [==============================] - 51s 27ms/step - loss: 1.5493e-04 - dice_coef: 0.9998 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0262 - val_dice_coef: 0.9738 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 5/80\n",
      "1881/1881 [==============================] - 50s 27ms/step - loss: 5.4410e-05 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0109 - val_dice_coef: 0.9891 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 6/80\n",
      "1881/1881 [==============================] - 50s 27ms/step - loss: 1.7891e-05 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0064 - val_dice_coef: 0.9936 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 7/80\n",
      "1881/1881 [==============================] - 51s 27ms/step - loss: 5.7531e-06 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0015 - val_dice_coef: 0.9985 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 8/80\n",
      "1881/1881 [==============================] - 51s 27ms/step - loss: 2.0645e-06 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 6.1041e-04 - val_dice_coef: 0.9994 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 9/80\n",
      "1881/1881 [==============================] - 51s 27ms/step - loss: 9.0164e-07 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 1.5917e-04 - val_dice_coef: 0.9998 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 10/80\n",
      "1881/1881 [==============================] - 50s 27ms/step - loss: 3.5268e-07 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 7.1115e-05 - val_dice_coef: 0.9999 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 11/80\n",
      "1881/1881 [==============================] - 51s 27ms/step - loss: 1.2396e-07 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 3.5292e-05 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 12/80\n",
      "1881/1881 [==============================] - 51s 27ms/step - loss: 5.3933e-08 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 13/80\n",
      "1881/1881 [==============================] - 51s 27ms/step - loss: 2.8646e-08 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 14/80\n",
      "1881/1881 [==============================] - 50s 27ms/step - loss: 9.7598e-09 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 15/80\n",
      "1881/1881 [==============================] - 51s 27ms/step - loss: 4.7532e-09 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 16/80\n",
      "1881/1881 [==============================] - 51s 27ms/step - loss: 2.4083e-09 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 17/80\n",
      "1881/1881 [==============================] - 51s 27ms/step - loss: 1.9013e-09 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 18/80\n",
      "1881/1881 [==============================] - 51s 27ms/step - loss: 5.9573e-09 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 19/80\n",
      "1881/1881 [==============================] - 50s 27ms/step - loss: 1.3943e-09 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 20/80\n",
      "1881/1881 [==============================] - 50s 27ms/step - loss: 4.4363e-10 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 21/80\n",
      "1881/1881 [==============================] - 50s 27ms/step - loss: 4.4363e-10 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 22/80\n",
      "1881/1881 [==============================] - 51s 27ms/step - loss: 6.3375e-11 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 23/80\n",
      "1881/1881 [==============================] - 51s 27ms/step - loss: 0.0000e+00 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 24/80\n",
      "1881/1881 [==============================] - 51s 27ms/step - loss: 3.1688e-10 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 25/80\n",
      "1881/1881 [==============================] - 51s 27ms/step - loss: 0.0000e+00 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 26/80\n",
      "1881/1881 [==============================] - 51s 27ms/step - loss: 2.5350e-10 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 27/80\n",
      "1881/1881 [==============================] - 51s 27ms/step - loss: 3.1688e-10 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 28/80\n",
      "1881/1881 [==============================] - 51s 27ms/step - loss: 1.9013e-10 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 29/80\n",
      "1881/1881 [==============================] - 51s 27ms/step - loss: 6.3375e-11 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 30/80\n",
      "1881/1881 [==============================] - 51s 27ms/step - loss: 1.9013e-10 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 31/80\n",
      "1881/1881 [==============================] - 51s 27ms/step - loss: 2.5350e-10 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 32/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1881/1881 [==============================] - 51s 27ms/step - loss: 2.5350e-10 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 33/80\n",
      "1881/1881 [==============================] - 51s 27ms/step - loss: 6.3375e-11 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 34/80\n",
      "1881/1881 [==============================] - 51s 27ms/step - loss: 2.5350e-10 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 35/80\n",
      "1881/1881 [==============================] - 50s 27ms/step - loss: 1.2675e-10 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 36/80\n",
      "1881/1881 [==============================] - 48s 26ms/step - loss: 6.3375e-11 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 37/80\n",
      "1881/1881 [==============================] - 48s 26ms/step - loss: 1.9013e-10 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 38/80\n",
      "1881/1881 [==============================] - 48s 26ms/step - loss: 0.0000e+00 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 39/80\n",
      "1881/1881 [==============================] - 48s 26ms/step - loss: 0.0000e+00 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 40/80\n",
      "1881/1881 [==============================] - 48s 26ms/step - loss: 1.2675e-10 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 41/80\n",
      "1881/1881 [==============================] - 48s 26ms/step - loss: 0.0000e+00 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 42/80\n",
      "1881/1881 [==============================] - 48s 26ms/step - loss: 0.0000e+00 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 43/80\n",
      "1881/1881 [==============================] - 48s 25ms/step - loss: 0.0000e+00 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 44/80\n",
      "1881/1881 [==============================] - 48s 26ms/step - loss: 1.2675e-10 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 45/80\n",
      "1881/1881 [==============================] - 48s 26ms/step - loss: 0.0000e+00 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 46/80\n",
      "1881/1881 [==============================] - 49s 26ms/step - loss: 1.2675e-10 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 47/80\n",
      "1881/1881 [==============================] - 49s 26ms/step - loss: 2.5350e-10 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 48/80\n",
      "1881/1881 [==============================] - 48s 26ms/step - loss: 0.0000e+00 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 49/80\n",
      "1881/1881 [==============================] - 49s 26ms/step - loss: 1.2675e-10 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 50/80\n",
      "1881/1881 [==============================] - 49s 26ms/step - loss: 1.2675e-10 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 51/80\n",
      "1881/1881 [==============================] - 48s 26ms/step - loss: 0.0000e+00 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 52/80\n",
      "1881/1881 [==============================] - 48s 26ms/step - loss: 1.2675e-10 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 53/80\n",
      "1881/1881 [==============================] - 48s 26ms/step - loss: 1.9013e-10 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 54/80\n",
      "1881/1881 [==============================] - 48s 26ms/step - loss: 1.9013e-10 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 55/80\n",
      "1881/1881 [==============================] - 48s 26ms/step - loss: 0.0000e+00 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 56/80\n",
      "1881/1881 [==============================] - 48s 26ms/step - loss: 0.0000e+00 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 57/80\n",
      "1881/1881 [==============================] - 48s 26ms/step - loss: 0.0000e+00 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 58/80\n",
      "1881/1881 [==============================] - 48s 25ms/step - loss: 0.0000e+00 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 59/80\n",
      "1881/1881 [==============================] - 49s 26ms/step - loss: 0.0000e+00 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 60/80\n",
      "1881/1881 [==============================] - 48s 26ms/step - loss: 6.3375e-11 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 61/80\n",
      "1881/1881 [==============================] - 48s 26ms/step - loss: 0.0000e+00 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 62/80\n",
      "1881/1881 [==============================] - 48s 26ms/step - loss: 0.0000e+00 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 63/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1881/1881 [==============================] - 49s 26ms/step - loss: 6.3375e-11 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 64/80\n",
      "1881/1881 [==============================] - 48s 26ms/step - loss: 6.3375e-11 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 65/80\n",
      "1881/1881 [==============================] - 48s 26ms/step - loss: 6.3375e-11 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00 - val_loss: 0.0000e+00 - val_dice_coef: 1.0000 - val_recall_1: 0.0000e+00 - val_precision_1: 0.0000e+00\n",
      "Epoch 66/80\n",
      " 816/1881 [============>.................] - ETA: 26s - loss: 0.0000e+00 - dice_coef: 1.0000 - recall_1: 0.0000e+00 - precision_1: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.metrics import Recall, Precision\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "#Model parameters\n",
    "base = 16\n",
    "image_size = 96\n",
    "img_ch = 1\n",
    "batch_size =4\n",
    "LR = 0.0001\n",
    "SDRate = 0.5\n",
    "batch_normalization = True\n",
    "spatial_dropout = True\n",
    "epochs = 80\n",
    "\n",
    "#Data loader parameters\n",
    "p = 0.2\n",
    "path = '/Lab1/Lab3/MRI/'\n",
    "fold1 = 'Image'\n",
    "fold2 = 'Mask'\n",
    "\n",
    "#Data augmentation parameters\n",
    "rotation_range = 10\n",
    "width_shift = 0.1\n",
    "height_shift_range = 0.1,\n",
    "rescale = 1./255\n",
    "horizontal_flip = True\n",
    "\n",
    "#Load the data\n",
    "train_img, train_mask, test_img, test_mask = get_train_test_data(fold1, fold2, path, p,image_size, image_size)\n",
    "\n",
    "#Data augmentation\n",
    "train_datagen, val_datagen = DataAugmentation(rotation_range,width_shift,height_shift_range,rescale,horizontal_flip)\n",
    "\n",
    "#Build the multi-classification model\n",
    "model = u_net(base,image_size, image_size, img_ch, batch_normalization, SDRate, spatial_dropout)\n",
    "\n",
    "#Compile the model\n",
    "model.compile(optimizer = Adam(lr=LR), loss = dice_coef_loss, metrics =[dice_coef, Recall(), Precision()] )\n",
    "\n",
    "#Fit the data into the model\n",
    "History = model.fit_generator(train_datagen.flow(train_img, train_mask,batch_size = batch_size), validation_data = val_datagen.flow(test_img, test_mask), epochs = epochs, verbose = 1)          \n",
    "\n",
    "#Plot results\n",
    "dice = True\n",
    "recall = True\n",
    "precision = True\n",
    "plotter(History)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##image loader for 3D\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "def path_loader(fold1, fold2, data_path):\n",
    "    #Creating data path\n",
    "    image_data_path = os.path.join(data_path, fold1)   \n",
    "    mask_data_path = os.path.join(data_path, fold2)\n",
    "    images = []\n",
    "    masks = []\n",
    "    #Listing all file names in the path\n",
    "    for root, dirs, files in os.walk(image_data_path):\n",
    "        for name in files:\n",
    "            images.append(os.path.join(image_data_path,name))\n",
    "    for root2, dirs2, files2 in os.walk(mask_data_path):\n",
    "        for name2 in files2:\n",
    "            masks.append(os.path.join(mask_data_path,name2))\n",
    "    return images, masks\n",
    "    \n",
    "\n",
    "# reading and resizing the training images with their corresponding labels\n",
    "def get_train_data_shuffled(images, masks, p):\n",
    "    \n",
    "    c = list(zip(images, masks))\n",
    "\n",
    "    shuffle(c)\n",
    "\n",
    "    images, masks = zip(*c)\n",
    "    \n",
    "    train_x, test_x, train_y, test_y = train_test_split(images,masks,test_size = p)\n",
    "\n",
    "    return train_x, test_x, train_y, test_y \n",
    "\n",
    "def data_loader(fold1, fold2, data_path, p,img_h, img_w, img_d):\n",
    "    \n",
    "    images, masks = path_loader(fold1, fold2, data_path)\n",
    "    train_x, test_x, train_y, test_y = get_train_data_shuffled(images, masks, p)\n",
    "    \n",
    "    train_img = []\n",
    "    train_mask = []\n",
    "    test_img = []\n",
    "    test_mask = []\n",
    "  \n",
    "    for i in range(len(train_x)):\n",
    "        image_name = train_x[i]\n",
    "        img = imread(image_name, as_grey=True)\n",
    "        img = resize(img, (img_h, img_w, img_d), anti_aliasing = True).astype('float32')\n",
    "        train_img.append([np.array(img)]) \n",
    "\n",
    "        if i % 50 == 0:\n",
    "             print('Reading: {0}/{1}  of train images'.format(i, len(train_x)))\n",
    "    for j in range(len(train_y)):\n",
    "        mask_name = train_y[j]\n",
    "        mask = imread(mask_name, as_grey=True)\n",
    "        mask = resize(img, (img_h, img_w, img_d), anti_aliasing = True).astype('float32')\n",
    "        train_mask.append([np.array(mask)])\n",
    "        \n",
    "    for i in range(len(test_x)):\n",
    "        image_name = test_x[i]\n",
    "        img = imread(image_name, as_grey=True)\n",
    "        img = resize(img, (img_h, img_w, img_d), anti_aliasing = True).astype('float32')\n",
    "        test_img.append([np.array(img)]) \n",
    "\n",
    "        if i % 50 == 0:\n",
    "             print('Reading: {0}/{1}  of test images'.format(i, len(test_x)))\n",
    "                \n",
    "    for j in range(len(test_y)):\n",
    "        mask_name = test_y[j]\n",
    "        mask = imread(mask_name, as_grey=True)\n",
    "        mask = resize(img, (img_h, img_w,img_d), anti_aliasing = True).astype('float32')\n",
    "        test_mask.append([np.array(mask)])        \n",
    "    print('finish')\n",
    " \n",
    "    return train_img, train_mask, test_img, test_mask\n",
    "\n",
    "#preprocessing\n",
    "def normalize(image):\n",
    "    MIN_BOUND = -1000.0\n",
    "    MAX_BOUND = 500.0\n",
    "    image = (image - MIN_BOUND) / (MAX_BOUND - MIN_BOUND)\n",
    "    print(image.shape)\n",
    "    image[image>1] = 1.\n",
    "    image[image<0] = 0.\n",
    "    return image\n",
    "\n",
    "# Instantiating images and labels for the model.\n",
    "def get_train_test_data(fold1, fold2, data_path, p,img_h, img_w, img_d):\n",
    "    \n",
    "    train_img, train_mask, test_img, test_mask = data_loader(fold1, fold2, data_path, p,img_h, img_w, img_d)\n",
    "    print(len(train_img))\n",
    "    Train_Img = np.zeros((len(train_img), img_h, img_w, img_d), dtype = np.float32)\n",
    "    Test_Img = np.zeros((len(test_img), img_h, img_w, img_d), dtype = np.float32)\n",
    "\n",
    "    Train_Label = np.zeros((len(train_mask),img_h, img_w, img_d), dtype = np.int32)\n",
    "    Test_Label = np.zeros((len(test_mask),img_h, img_w, img_d), dtype = np.int32)\n",
    "    print('get train')\n",
    "    for i in range(len(train_img)):\n",
    "        print(i)\n",
    "        Train_Img[i] = normalize(train_img[i][0])\n",
    "        Train_Label[i] = normalize(train_mask[i][0])\n",
    "    print(Train.Img.shape)\n",
    "    \n",
    "    Train_Img = np.expand_dims(Train_Img, axis = 4)  \n",
    "    Train_Label = np.expand_dims(Train_Label, axis = 4) \n",
    "\n",
    "    for j in range(len(test_img)):\n",
    "        Test_Img[j] = normalize(test_img[j][0])\n",
    "        Test_Label[j] = normalize(test_mask[j][0])\n",
    "\n",
    "    Test_Img = np.expand_dims(Test_Img, axis = 4)\n",
    "    Test_Label = np.expand_dims(Test_Label, axis = 4)\n",
    "   \n",
    "\n",
    "    return Train_Img, Train_Label, Test_Img, Test_Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.layers import Input, Convolution3D, MaxPooling3D, Conv3DTranspose\n",
    "from tensorflow.keras.layers import Reshape, Activation\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "\n",
    "def u_net3D(img_height, img_width, img_ch, img_d, batchNormalization, k_size=3):\n",
    "    \n",
    "    merge_axis = -1 # Feature maps are concatenated along last axis (for tf backend)\n",
    "    data = Input((img_height, img_width, img_d, img_ch))\n",
    "    \n",
    "    #1ConvBlock\n",
    "    conv1 = Convolution3D(padding='same', filters=Base, kernel_size=k_size)(data)\n",
    "    if batchNormalization:\n",
    "        conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Activation('relu')(conv1)\n",
    "    \n",
    "    conv2 = Convolution3D(padding='same', filters=Base, kernel_size=k_size)(conv1)\n",
    "    if batchNormalization:\n",
    "        conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Activation('relu')(conv2)\n",
    "    \n",
    "    pool1 = MaxPooling3D(pool_size=(2, 2, 2))(conv2)\n",
    "    \n",
    "    #2ConvBlock\n",
    "    conv3 = Convolution3D(padding='same', filters=Base*2, kernel_size=k_size)(pool1)\n",
    "    if batchNormalization:\n",
    "        conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Activation('relu')(conv3)\n",
    "   \n",
    "    conv4 = Convolution3D(padding='same', filters=Base*2, kernel_size=k_size)(conv3)\n",
    "    if batchNormalization:\n",
    "        conv4 = BatchNormalization()(conv4)\n",
    "    conv4 = Activation('relu')(conv4)\n",
    "    \n",
    "    pool2 = MaxPooling3D(pool_size=(2, 2, 2))(conv4)\n",
    "    \n",
    "    #3ConvBlock\n",
    "    conv5 = Convolution3D(padding='same', filters=Base*4, kernel_size=k_size)(pool2)\n",
    "    if batchNormalization:\n",
    "        conv5 = BatchNormalization()(conv5)\n",
    "    conv5 = Activation('relu')(conv5)\n",
    "    \n",
    "    conv6 = Convolution3D(padding='same', filters=Base*4, kernel_size=k_size)(conv5)\n",
    "    if batchNormalization:\n",
    "        conv6 = BatchNormalization()(conv6)\n",
    "    conv6 = Activation('relu')(conv6)\n",
    "    \n",
    "    pool3 = MaxPooling3D(pool_size=(2, 2, 2))(conv6)\n",
    "    \n",
    "    #4ConvBlock\n",
    "    conv7 = Convolution3D(padding='same', filters=Base*8, kernel_size=k_size)(pool3)\n",
    "    if batchNormalization:\n",
    "        conv7 = BatchNormalization()(conv7)\n",
    "    conv7 = Activation('relu')(conv7)\n",
    "    \n",
    "    conv8 = Convolution3D(padding='same', filters=Base*8, kernel_size=k_size)(conv7)\n",
    "    if batchNormalization:\n",
    "        conv8 = BatchNormalization()(conv8)\n",
    "    conv8 = Activation('relu')(conv8)\n",
    "    \n",
    "    pool4 = MaxPooling3D(pool_size=(2, 2, 2))(conv8)\n",
    "    \n",
    "    #Bottleneck\n",
    "    conv9 = Convolution3D(padding='same', filters=Base*16, kernel_size=k_size)(pool4)\n",
    "    if batchNormalization:\n",
    "        conv9 = BatchNormalization()(conv9)\n",
    "    conv9 = Activation('relu')(conv9)\n",
    "    \n",
    "    #Expansion\n",
    "    #1ConvBlock\n",
    "    up1 = Conv3DTranspose(filters= Base*8, kernel_size=(2, 2, 2),padding = 'same')(conv9)\n",
    "    merged1 = concatenate([up1, conv8], axis=merge_axis)\n",
    "    conv10 = Convolution3D(padding='same', filters=Base*8, kernel_size=k_size)(merged1)\n",
    "    if batchNormalization:\n",
    "        conv10 = BatchNormalization()(conv10)\n",
    "    conv10 = Activation('relu')(conv10)\n",
    "    \n",
    "    conv11 = Convolution3D(padding='same', filters=Base*8, kernel_size=k_size)(conv10)\n",
    "    if batchNormalization:\n",
    "        conv11 = BatchNormalization()(conv11)\n",
    "    conv11 = Activation('relu')(conv11)\n",
    "    \n",
    "    #2ConvBlock\n",
    "    up2 = Conv3DTranspose(filters = Base*4, kernel_size=(2, 2, 2),padding = 'same')(conv11)\n",
    "    merged2 = concatenate([up2, conv6], axis=merge_axis)\n",
    "    conv12 = Convolution3D(padding='same', filters=Base*4, kernel_size=k_size)(merged2)\n",
    "    if batchNormalization:\n",
    "        conv12 = BatchNormalization()(conv12)\n",
    "    conv12 = Activation('relu')(conv12)\n",
    "\n",
    "    conv13 = Convolution3D(padding='same', filters=Base*4, kernel_size=k_size)(conv12)\n",
    "    if batchNormalization:\n",
    "        conv13 = BatchNormalization()(conv13)\n",
    "    conv13 = Activation('relu')(conv13)\n",
    "    \n",
    "    #3ConvBlock\n",
    "    up3 = Conv3DTranspose(filters = Base*2, kernel_size=(2, 2, 2),padding='same')(conv13)\n",
    "    merged3 = concatenate([up3, conv4], axis=merge_axis)\n",
    "    conv14 = Convolution3D(padding='same', filters=Base*2, kernel_size=k_size)(merged3)\n",
    "    if batchNormalization:\n",
    "        conv14 = BatchNormalization()(conv14)\n",
    "    conv14 = Activation('relu')(conv14)\n",
    "    \n",
    "    conv15 = Convolution3D(padding='same', filters=64, kernel_size=k_size)(conv14)\n",
    "    if batchNormalization:\n",
    "        conv15 = BatchNormalization()(conv15)\n",
    "    conv15 = Activation('relu')(conv15)\n",
    "\n",
    "    #4ConvBlock\n",
    "    up4 = Conv3DTranspose(filters= Base,kernel_size=(2, 2, 2),padding ='same')(conv15)\n",
    "    merged4 = concatenate([up4, conv2], axis=merge_axis)\n",
    "    conv16 = Convolution3D(padding='same', filters=Base, kernel_size=k_size)(merged4)\n",
    "    if batchNormalization:\n",
    "        conv16 = BatchNormalization()(conv16)\n",
    "    conv16 = Activation('relu')(conv16)\n",
    "    \n",
    "    conv17 = Convolution3D(padding='same', filters=Base, kernel_size=k_size)(conv16)\n",
    "    if batchNormalization:\n",
    "        conv17 = BatchNormalization()(conv17)\n",
    "    conv17 = Activation('relu')(conv17)\n",
    "    \n",
    "    #final layer\n",
    "    conv18 = Convolution3D(padding='same', filters=3, kernel_size=k_size)(merged3)\n",
    "    if batchNormalization:\n",
    "        conv18 = BatchNormalization()(conv18)\n",
    "    output = Reshape([-1, 2])(conv18)\n",
    "    output = Activation('softmax')(output)\n",
    "    output = Reshape(inp_shape[:-1] + (2,))(output)\n",
    "\n",
    "    model = Model(data, output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: 0/6735  of train images\n",
      "Reading: 50/6735  of train images\n",
      "Reading: 100/6735  of train images\n",
      "Reading: 150/6735  of train images\n",
      "Reading: 200/6735  of train images\n",
      "Reading: 250/6735  of train images\n",
      "Reading: 300/6735  of train images\n",
      "Reading: 350/6735  of train images\n",
      "Reading: 400/6735  of train images\n",
      "Reading: 450/6735  of train images\n",
      "Reading: 500/6735  of train images\n",
      "Reading: 550/6735  of train images\n",
      "Reading: 600/6735  of train images\n",
      "Reading: 650/6735  of train images\n",
      "Reading: 700/6735  of train images\n",
      "Reading: 750/6735  of train images\n",
      "Reading: 800/6735  of train images\n",
      "Reading: 850/6735  of train images\n",
      "Reading: 900/6735  of train images\n",
      "Reading: 950/6735  of train images\n",
      "Reading: 1000/6735  of train images\n",
      "Reading: 1050/6735  of train images\n",
      "Reading: 1100/6735  of train images\n",
      "Reading: 1150/6735  of train images\n",
      "Reading: 1200/6735  of train images\n",
      "Reading: 1250/6735  of train images\n",
      "Reading: 1300/6735  of train images\n",
      "Reading: 1350/6735  of train images\n",
      "Reading: 1400/6735  of train images\n",
      "Reading: 1450/6735  of train images\n",
      "Reading: 1500/6735  of train images\n",
      "Reading: 1550/6735  of train images\n",
      "Reading: 1600/6735  of train images\n",
      "Reading: 1650/6735  of train images\n",
      "Reading: 1700/6735  of train images\n",
      "Reading: 1750/6735  of train images\n",
      "Reading: 1800/6735  of train images\n",
      "Reading: 1850/6735  of train images\n",
      "Reading: 1900/6735  of train images\n",
      "Reading: 1950/6735  of train images\n",
      "Reading: 2000/6735  of train images\n",
      "Reading: 2050/6735  of train images\n",
      "Reading: 2100/6735  of train images\n",
      "Reading: 2150/6735  of train images\n",
      "Reading: 2200/6735  of train images\n",
      "Reading: 2250/6735  of train images\n",
      "Reading: 2300/6735  of train images\n",
      "Reading: 2350/6735  of train images\n",
      "Reading: 2400/6735  of train images\n",
      "Reading: 2450/6735  of train images\n",
      "Reading: 2500/6735  of train images\n",
      "Reading: 2550/6735  of train images\n",
      "Reading: 2600/6735  of train images\n",
      "Reading: 2650/6735  of train images\n",
      "Reading: 2700/6735  of train images\n",
      "Reading: 2750/6735  of train images\n",
      "Reading: 2800/6735  of train images\n",
      "Reading: 2850/6735  of train images\n",
      "Reading: 2900/6735  of train images\n",
      "Reading: 2950/6735  of train images\n",
      "Reading: 3000/6735  of train images\n",
      "Reading: 3050/6735  of train images\n",
      "Reading: 3100/6735  of train images\n",
      "Reading: 3150/6735  of train images\n",
      "Reading: 3200/6735  of train images\n",
      "Reading: 3250/6735  of train images\n",
      "Reading: 3300/6735  of train images\n",
      "Reading: 3350/6735  of train images\n",
      "Reading: 3400/6735  of train images\n",
      "Reading: 3450/6735  of train images\n",
      "Reading: 3500/6735  of train images\n",
      "Reading: 3550/6735  of train images\n",
      "Reading: 3600/6735  of train images\n",
      "Reading: 3650/6735  of train images\n",
      "Reading: 3700/6735  of train images\n",
      "Reading: 3750/6735  of train images\n",
      "Reading: 3800/6735  of train images\n",
      "Reading: 3850/6735  of train images\n",
      "Reading: 3900/6735  of train images\n",
      "Reading: 3950/6735  of train images\n",
      "Reading: 4000/6735  of train images\n",
      "Reading: 4050/6735  of train images\n",
      "Reading: 4100/6735  of train images\n",
      "Reading: 4150/6735  of train images\n",
      "Reading: 4200/6735  of train images\n",
      "Reading: 4250/6735  of train images\n",
      "Reading: 4300/6735  of train images\n",
      "Reading: 4350/6735  of train images\n",
      "Reading: 4400/6735  of train images\n",
      "Reading: 4450/6735  of train images\n",
      "Reading: 4500/6735  of train images\n",
      "Reading: 4550/6735  of train images\n",
      "Reading: 4600/6735  of train images\n",
      "Reading: 4650/6735  of train images\n",
      "Reading: 4700/6735  of train images\n",
      "Reading: 4750/6735  of train images\n",
      "Reading: 4800/6735  of train images\n",
      "Reading: 4850/6735  of train images\n",
      "Reading: 4900/6735  of train images\n",
      "Reading: 4950/6735  of train images\n",
      "Reading: 5000/6735  of train images\n",
      "Reading: 5050/6735  of train images\n",
      "Reading: 5100/6735  of train images\n",
      "Reading: 5150/6735  of train images\n",
      "Reading: 5200/6735  of train images\n",
      "Reading: 5250/6735  of train images\n",
      "Reading: 5300/6735  of train images\n",
      "Reading: 5350/6735  of train images\n",
      "Reading: 5400/6735  of train images\n",
      "Reading: 5450/6735  of train images\n",
      "Reading: 5500/6735  of train images\n",
      "Reading: 5550/6735  of train images\n",
      "Reading: 5600/6735  of train images\n",
      "Reading: 5650/6735  of train images\n",
      "Reading: 5700/6735  of train images\n",
      "Reading: 5750/6735  of train images\n",
      "Reading: 5800/6735  of train images\n",
      "Reading: 5850/6735  of train images\n",
      "Reading: 5900/6735  of train images\n",
      "Reading: 5950/6735  of train images\n",
      "Reading: 6000/6735  of train images\n",
      "Reading: 6050/6735  of train images\n",
      "Reading: 6100/6735  of train images\n"
     ]
    }
   ],
   "source": [
    "#training the model\n",
    "\n",
    "image_size = 65\n",
    "img_ch = 1\n",
    "batch_size =8\n",
    "LR = 0.0001\n",
    "SDRate = 0.5\n",
    "spatial_dropout = True\n",
    "metric = 'dice'\n",
    "epochs = 150\n",
    "p = 0.2\n",
    "path = '/Lab1/Lab3/CT/'\n",
    "fold1 = 'Image'\n",
    "fold2 = 'Mask'\n",
    "base = 32\n",
    "batch_normalization = True\n",
    "\n",
    "train_img, train_mask, test_img, test_mask = get_train_test_data(fold1, fold2, path, p,image_size, image_size, image_size)\n",
    "print('lol')\n",
    "model = u_net3D(base,image_size, image_size, img_d, img_ch, batch_normalization, SDRate, spatial_dropout)\n",
    "model.compile(optimizer = Adam(lr=LR), loss = dice_coef_loss, metrics =[dice_coef])\n",
    "\n",
    "History = model.fit(train_img, train_mask, epochs = epochs, batch_size = batch_size, verbose = 1,\n",
    "                        validation_data = (test_img,test_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
